{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 21\n",
    "\n",
    "# How certain is the model; parametric bootstrapping\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "Previously, we have investigated the nonparametric bootstrap resampling method. We used bootstrap resampling to compute a point estimate and confidence interval for simple univariate statistics.\n",
    "\n",
    "The bootstrap method can also be applied to regression models. Computing the bootstrap distribution of the regression model parameters provides insight into variability of these parameters. It is useful to know how much random variation there is in regression coefficients simply because of a small change in training data values.   \n",
    "\n",
    "To proceed, we need to define our terminology:  \n",
    "\n",
    "- **Parametric bootstrap:** Linear regression is an example of a **parametric model**. Simply, a parametric model is a model with parameters which must be estimated. A parametric bootstrap process is used to find point estimates of the model parameters and the associated uncertainty or confidence intervals. For each bootstrap resample created, a new estimate of the model parameters is computed. For example, a regression model estimates parameters that minimize the sum of squared errors. In the previous chapter, we showed that this least squares model assumes a Normal likelihood model for the residuals, making application of a bootstrap algorithm parametric.      \n",
    "- **Nonparametric bootstrap:** Some statistical estimates have no particular model associated with them. Examples include the mean, variance and bootstrap confidence intervals. These values can be estimated using the bootstrap method without specifying a parametric model. The nonparametric bootstrap makes no assumptions about a likelihood model, and therefore has no parameters.  \n",
    "\n",
    "Which bootstrap model should you use? The answer is not straight forward. You must consider the following:\n",
    "1. Generally, the parametric bootstrap gives tighter confidence intervals, if the model assumptions are reasonably correct. \n",
    "2. If the model assumptions are violated, the parametric bootstrap will give poor results and a nonparametric bootstrap is preferred. \n",
    "\n",
    "As with most statistics, it is possible to bootstrap the parameters of most any regression model. However, since bootstrap resampling uses a large number of subsamples, it can be computationally intensive. For large-scale problems it is necessary to use other resampling methods like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Let's try an example. We will work with a simple regression model similar to the one used in Chapter 19. \n",
    "\n",
    "As a first step, execute the code in the cell below to import the required packages and generate a synthetic data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot these data and examine the result by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')\n",
    "plt.ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Bootstrap of Regression\n",
    "\n",
    "We will now apply the parametric bootstrap algorithm to a simple parametric model, a regression model. The work is done in a `for loop`, run once for each bootstrap sample with the following steps:   \n",
    "1. Bernoulli sample the rows of the data frame with replacement with same number of rows as the original frame.    \n",
    "2. Compute the OLS model using the Bernoulli sample.   \n",
    "3. Save the model parameters in the data frame to be returned.   \n",
    "Execute the code below and example the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_regression(df, n_boots, n_params=2, formula='y ~ x'):\n",
    "    ## array to hold the bootstrap samples of the parameters\n",
    "    boot_samples = np.zeros((n_boots,n_params))\n",
    "    n_samples = df.shape[0]\n",
    "    ## Loop over the number of resamples\n",
    "    for i in range(n_boots):\n",
    "        ## Create a bootstrap sample of the data frame\n",
    "        boot_sample = df.sample(n=n_samples, replace=True)\n",
    "        ## Compute the OLS model\n",
    "        boot_model = sm.ols(formula=formula, data=boot_sample).fit()\n",
    "        ## Save the model parameters in the array\n",
    "        boot_samples[i,:] = boot_model._results.params\n",
    "    return boot_samples\n",
    "\n",
    "param_boots = resample_regression(sim_data,2000)\n",
    "param_boots[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the first few bootstrap estimates of the intercept and slope coefficients. Notice the resampling leads to considerable variation in these values.       \n",
    "\n",
    "Let's compute and display the bootstrap distributions of the slope and intercept parameters. Execute the code in the cell below to examine the distribution of the intercept, along with the confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CI(values, p=0.05):   \n",
    "    mean = np.mean(values)\n",
    "    p = 100 * p / 2.0\n",
    "    UCI = np.percentile(values, 100 - p)\n",
    "    LCI = np.percentile(values, p)\n",
    "    print(f'Mean = {mean}')\n",
    "    print(f'Upper confidence interval = {UCI}')\n",
    "    print(f'Lower confidence interval = {LCI}')\n",
    "    return(mean, UCI, LCI)\n",
    "\n",
    "def plot_boot_params(params, parameter='intercept'):\n",
    "    mean, UCI, LCI = compute_CI(params)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ## Plot a histogram\n",
    "    ax[0].hist(params, bins=20)\n",
    "    ax[0].axvline(mean, color='red', linewidth=1)\n",
    "    ax[0].axvline(UCI, color='red', linewidth=1, linestyle='--')\n",
    "    ax[0].axvline(LCI, color='red', linewidth=1, linestyle='--')\n",
    "     \n",
    "    ax[0].set_title('Histogram of parameter\\n' + parameter)\n",
    "    ax[0].set_xlabel('Model parameter values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(params, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of parameter\\n' + parameter)\n",
    "    plt.show()\n",
    "\n",
    "plot_boot_params(param_boots[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to display a plot of the slope parameter, along with the confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boot_params(param_boots[:,1], parameter='slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above you can see the variability and confidence intervals of the parameter estimates. Consider the answers to the following questions:  \n",
    "1. Are the bootstrap distribution values approximately Normal?\n",
    "2. Does one coefficient have greater variability than the other, and if so, which one?  \n",
    "3. Are the known parameter values of the synthetic data within the confidence intervals? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot bootstrap realizations of the regression line. This will give you a feel for the variability of the regression solutions. Execute the code in the cell below and examine the result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boot_regression(boot_params, df, plot_x_col='x', plot_y_col='y', cols='x', xlabel='x', ylabel='y', n_lines = 100):  \n",
    "    ## Find the mean slope and intercept and plot the line \n",
    "    means = np.mean(boot_params, axis=0)\n",
    "    ## randomly select n_lines to plot\n",
    "    sample_indx = nr.choice(range(boot_params.shape[0]),n_lines)\n",
    "    ## Plot the observations\n",
    "    fig,ax = plt.subplots(figsize=(7,7))\n",
    "    ax = sns.scatterplot(x=plot_x_col, y=plot_y_col, data=df, color = 'magenta', ax=ax)\n",
    "    ## Loop over the number of bootstrap regression lines to be displayed\n",
    "    for indx in sample_indx:     \n",
    "        df['predicted'] = boot_params[indx,0] + np.multiply(df[cols], boot_params[indx,1:])\n",
    "        sns.lineplot(x=plot_x_col, y='predicted', data=df, color='Blue', size = 1.0, alpha=0.1, ax=ax)\n",
    "    ## Plot the mean regression line\n",
    "    df['predicted'] = means[0] + np.multiply(df[cols] ,means[1:])\n",
    "    sns.lineplot(x=plot_x_col, y='predicted', data=df, color='red', ax=ax)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "plot_boot_regression(param_boots, sim_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the following:   \n",
    "1. The heavy line is the mean parameter regression line.   \n",
    "2. The light blue lines show a selected number of the bootstrap regression lines. Notice the variability in these possible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example      \n",
    "\n",
    "To further illustrate the foregoing concepts you will now work though another example using the auto price data.  \n",
    "\n",
    "> **Exercise 21-1:** As a first step you will construct a parametric OLS regression model for the price of automobiles. The code in the cell below loads and prepares the auto price dataset. To compute and display the bootstrap distributions of the model parameters do the following:    \n",
    "> 1. Use the `resample_regression` function to compute 2000 bootstrap realizations of a regression model using formula `'log_price ~ curb_weight_centered*engine_size_centered'`. This model includes the interaction term between the two independent variables.   \n",
    "> 2. For each set of model parameters call the `plot_boot_params`. The first argument is the column of your bootstrap parameter Numpy array. The second, `parameter`, argument set to the parameter name in `['intercept', 'curb_weight_centered','engine_size_centered', 'curb_weight_centered:engine_size_centered']`.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the dataset\n",
    "auto_price = pd.read_csv('../data//AutoPricesClean.csv')\n",
    "auto_price['log_price'] = np.log(auto_price.loc[:,'price'])\n",
    "for col,new_col in zip(['curb_weight','engine_size'],['curb_weight_centered','engine_size_centered']):\n",
    "    auto_price[new_col] = auto_price.loc[:,col] - np.mean(auto_price.loc[:,col])\n",
    "\n",
    "np.random.seed(2244)    \n",
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now answer the following questions:     \n",
    "> 1. Are the bootstrap distributions of all the variables Normally distributed or is the noticeable skew?  If skew is present, describe the skew for each parameter.    \n",
    "> 2. Examine the 95% confidence intervals for each of the model parameters. Based on these CIs are all the parameters significant and if not, which parameters are not significant and why?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.           \n",
    "> 2.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 21-2:** In the foregoing exercise, you have identified which regression model parameters are significant. Now, repeat the analysis you performed in Exercise 21-1, but using a model formula including only the significant parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:    \n",
    "> 1. Are all the parameters of your model significant now and why?    \n",
    "> 2. Are the bootstrap distributions of the parameters reasonably Normal and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.             \n",
    "> 2.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2020, 2022, 2023, 2024 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
