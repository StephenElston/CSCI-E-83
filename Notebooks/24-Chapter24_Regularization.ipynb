{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24\n",
    "# Taming Model Behavior with Regularization   \n",
    "\n",
    "\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "In modern computation statistics and machine learning, we are often faced with a large number of independent variables, or features, which can be colinear or simply uninformative. Given the scale we need algorithms to reduce the number of independent variables to just the informative ones. Methods like stepwise selection are know to fail at scale as a result of the multiple hypothesis testing problem. Solution of this problem leads us to regularization and sparse models.\n",
    "\n",
    "As a general rule, an overfit model has learned the training data too well. The overfitting likely involved learning noise present in the training data. This noise can arise from uninformative or weakly informative variables or features being used in the model. A related problem arises from using colinear independent variables or features. The colinarity confounds model training, amplifying random variation in the model fitting. Regardless of the source, the random noise causes the fitted model to exhibit high levels of random variation, or high variance.   When a new data case is presented to such a model it may produce unexpected results since the random noise will be different.    \n",
    "\n",
    "To prevent overfitting, we seek to find **sparse models**. A sparse model uses just the most informative variables required to produce accurate outputs. Term sparse is used since uninformative variables are excluded from the model. Use of sparse models follows the principle of Occam's razor. In simple terms, Occam's razor is a scientific principle that the simplest of competing theories is preferred. A sparse model then is the simplest model that explains the behavior of the data.         \n",
    "\n",
    "So, what is one to do to prevent overfitting of machine learning models? The most widely used set of tools for preventing overfitting are known as **regularization methods**. Regularization methods take a number of forms, but all have the same goal, to prevent overfitting of machine learning models. The sparse models resulting from the application of regularization methods will generalize better and be useful in production. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade-off\n",
    "\n",
    "Regularization is not free, however. While regularization reduces the **variance** in the model results, it introduces **bias**. Whereas, an overfit model exhibits low bias the variance is high. The high variance leads to unpredictable results when the model is exposed to new data cases. On the other hand, the stronger the regularization of a model the lower the variance, but the greater the bias. This all means that when applying regularization you will need to contend with the **bias-variance trade-off**. \n",
    "\n",
    "To better understand the bias variance trade-off consider the following examples of extreme model cases:\n",
    "\n",
    "- If the prediction for all cases is just the mean (or median), variance is minimized. The estimate for all cases is the same, so the bias of the estimates is zero. However, there is likely considerable variance in these estimates. \n",
    "- On the other hand, consider what happens when the data are fit with a kNN model with k=1. The training data will fit this model perfectly, since there is one model coefficient per training data point. The variance will be low. However, the model will have considerable bias when applied to test data. \n",
    "\n",
    "In either case, these extreme models will not generalize well and will exhibit large errors on any independent test data. Any practical model must come to terms with the trade-off between bias and variance to make accurate predictions. \n",
    "\n",
    "To better understand this trade-off you can consider the example of the mean square error, which can be decomposed into its components. The mean square error can be written as:\n",
    "\n",
    "$$\\Delta y^2 = E \\Big[ \\big(Y - \\hat{f}(X) \\big)^2 \\Big] = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat{f}(x_i) \\big)^2 $$\n",
    "\n",
    "Where,\n",
    "$Y = $ the label vector.  \n",
    "$X = $ the feature matrix.   \n",
    "$\\hat{f}(x) = $ the trained model.   \n",
    "\n",
    "Expanding the representation of the mean square error:\n",
    "\n",
    "$$\\Delta y^2 = \\big( E[ \\hat{f}(X)] - \\hat{f}(X) \\big)^2 + E \\big[ ( \\hat{f}(X) - E[ \\hat{f}(X)])^2 \\big] + \\sigma^2\\\\\n",
    "\\Delta y^2 = Bias^2 + Variance + Irreducible\\ Error$$\n",
    "\n",
    "The forgoing looks a bit intimidating. How can we interpret this relationship?     \n",
    "\n",
    "- The first term, $\\Big( E[ \\hat{f}(X)] - \\hat{f}(X) \\Big)^2$, is the expected value of the difference between the model output and the expected model output or the **bias** of the model. For a **unbiased model**, $E \\Big[ \\hat{f}(X)] - \\hat{f}(X) \\Big] = 0$. For example: an OLS model with $residuals \\sim N(0, \\sigma^2)$ is unbiased.        \n",
    "- The second term, $E \\Big[ \\big( \\hat{f}(X) - E[ \\hat{f}(X)] \\big)^2 \\Big]$, is the expected squared difference between the model output and the expected model output, or the **variance** of the model. For a low variance model, $E \\Big[ \\big( \\hat{f}(X) - E[ \\hat{f}(X)] \\big)^2 \\Big] \\rightarrow 0$. A low variance model generalizes since variance is low for each prediction, $\\hat{f}(X)$.     \n",
    "- Finally, $\\sigma^2$ is inherent or irreducable error in data. We cannot do anything to improve this random variation.        \n",
    "\n",
    "The relationship between bias and variance is illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/BiasVariance.png\" alt=\"Drawing\" style=\"width:600px; height:400px\"/>\n",
    "<center> Trade-off between bias and variance for machine learning model <center>      \n",
    "    \n",
    "\n",
    "Study this relationship. Notice that as regularization reduces variance, bias increases. The irreducible error will remain unchanged. Regularization parameters are chosen to minimize $\\Delta x$. In many cases, this will prove challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split the Dataset\n",
    "\n",
    "With the above bit of theory in mind, it is time to try an example. In this example you will compute and compare linear regression models using different levels and types of regularization. \n",
    "\n",
    "Execute the code in the cell below to load the packages required for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf  \n",
    "import scipy.stats as ss\n",
    "import patsy \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import sklearn.metrics as sklm\n",
    "from patsy import dmatrices\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the following to load and prepare the data set:    \n",
    "1. Load the data set.   \n",
    "2. Scale the numeric columns except the columns used as labels for the examples. \n",
    "3. Split the 195 cases into 100 training cases and 95 test cases.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data frame   \n",
    "auto_data = pd.read_csv('../data/AutoPricesClean.csv')\n",
    "\n",
    "## Remove unwanted columns and scale numeric columns \n",
    "auto_data.drop(auto_data.columns[:3], axis=1, inplace=True)\n",
    "numeric_columns = [col for col_type,col in zip(auto_data.iloc[:,:-3].dtypes,auto_data.iloc[:,:-3].columns) if col_type in ['int64','float64']]\n",
    "auto_data.loc[:,numeric_columns] = StandardScaler().fit_transform(auto_data.loc[:,numeric_columns])\n",
    "\n",
    "## Create a mask and use it to split the data into a train and test set   \n",
    "nr.seed(6665)\n",
    "mask = nr.choice(auto_data.index, size = 100, replace=False)\n",
    "auto_data_train = auto_data.iloc[mask,:].reset_index()\n",
    "auto_data_test = auto_data.drop(mask, axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = nr.choice(auto_data.index, size = 100, replace=False)\n",
    "auto_data_train = auto_data.iloc[mask,:].reset_index()\n",
    "auto_data_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first linear regression model\n",
    "\n",
    "First you will create a model of city MPG using both categorical and real-valued features or independent variables and no regularization. In the terminology used before this model has high variance and low bias. In other words, this model is over-fit and provides a baseline for comparison with regularized models. \n",
    "\n",
    "### Dependency structure \n",
    "\n",
    "To get a feel for the relationship between the independent variables used in the model, we will compute and display a correlation matrix. A few notes about the code:    \n",
    "1. We compute correlation using the design matrix. This approach is required, since the categorical variables must be one-hot encoded.    \n",
    "2. The intercept term is not included in the design matrix. The intercept term is represented by a column of all 1a which has 0 variance, that therefore has undefined correlation coefficients.      \n",
    "\n",
    "Execute the code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model formula with no intercept - column of all 1s with 0 variance \n",
    "formula = 'city_mpg ~ -1 + C(fuel_type) + C(aspiration) + C(drive_wheels) + horsepower + compression_ratio + curb_weight + engine_size'\n",
    "\n",
    "## Compute correlation matrix \n",
    "y, design_matrix = patsy.dmatrices(formula, data=auto_data)\n",
    "corr_mat = np.corrcoef(np.transpose(design_matrix))\n",
    "\n",
    "## Display correlation matrix  \n",
    "c_names = list(design_matrix.design_info.column_name_indexes)\n",
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "sns.heatmap(corr_mat, xticklabels=c_names, yticklabels=c_names, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several important observations we can make about the relationship between these variables.    \n",
    "1. There are several strong positive correlations. Not surprisingly, horsepower and engine size are highly correlated. Diesel fuel type is highly correlated with high compression, owing to the nature of diesel engines. And as a further example, curb weight is correlated with engine size.     \n",
    "2. There are also variable pairs with strong negative correlation. Some of these are simply a result of coding, such as gas fuel type and compression ratio, or front and real drive wheels. These relationships are expected from the one-hot encoding of the design matrix. Another example is front wheel drive and engine size, since cars of this era with large engines tended to have real wheel drive.    \n",
    "\n",
    "In summary, we can say that these variables do not meet the iid requirement of a linear model, because of the high correlation. Any model fit with these variables will be quite over-fit. As we progress with applying regularization methods it will help to keep these relationships in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An initial linear model\n",
    "\n",
    "The code in the cell below should be familiar. In summary, it performs the following processing:\n",
    "1. Define and train the linear regression model using the training features and labels.\n",
    "2. Display the summary of the model. \n",
    "\n",
    "Execute this code and examine the results for the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'city_mpg ~ C(fuel_type) + C(aspiration) + C(drive_wheels) + horsepower + compression_ratio + curb_weight + engine_size'\n",
    "base_model = smf.ols(formula, data=auto_data_train).fit()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the adjusted $R^2$ and F-statistic, this model seems to do a reasonable job of explaining the variance of the label. However, it is clear from the confidence intervals and p-values of the coefficients that this model is over-fit.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code below to display fit metrics, the distribution of residuals, and the residuals plotted against the predicted values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_predicted):\n",
    "    ## Compute the usual metrics\n",
    "    mse = sklm.mean_squared_error(y_true, y_predicted)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = sklm.median_absolute_error(y_true, y_predicted)\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def print_metrics(df_test, model, label_col='city_mpg'):   \n",
    "    df_test['predicted'] = model.predict(df_test)\n",
    "    mse, rmse, mae = compute_metrics(df_test.loc[:,label_col],df_test.loc[:,'predicted'])   \n",
    "    print('MSE  = {0:6.3f}'.format(mse))\n",
    "    print('RMSE = {0:6.3f}'.format(rmse))\n",
    "    print('MAE  = {0:6.3f}'.format(mae))    \n",
    "\n",
    "\n",
    "def residual_plot(df):\n",
    "    plt.rc('font', size=12)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), ) \n",
    "    RMSE = np.std(df.resids)\n",
    "    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);\n",
    "    plt.axhline(0.0, color='red', linewidth=1.0);\n",
    "    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);\n",
    "    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);\n",
    "    plt.title('Residuals vs. predicted');\n",
    "    plt.xlabel('Predicted values');\n",
    "    plt.ylabel('Residuals');\n",
    "    plt.show()\n",
    "\n",
    "def plot_resid_dist(df):\n",
    "    resids = df.loc[:,'resids']\n",
    "    plt.rc('font', size=12)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3));\n",
    "    ## Plot a histogram\n",
    "    sns.histplot(x=resids, bins=20, kde=True, ax=ax[0]);\n",
    "    ax[0].set_title('Histogram of residuals');\n",
    "    ax[0].set_xlabel('Residual values');\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1]);\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals');\n",
    "    plt.show();    \n",
    "\n",
    "def compute_residuals(df, model, label_col='city_mpg'):\n",
    "    df['predicted'] = model.predict(df)\n",
    "    df['resids'] = np.subtract(df.loc[:,'predicted'], df.loc[:,label_col]) \n",
    "    return df\n",
    "    \n",
    "print_metrics(auto_data_test, base_model)\n",
    "auto_data_train = compute_residuals(auto_data_train, base_model)\n",
    "plot_resid_dist(auto_data_train)\n",
    "residual_plot(auto_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the residuals look well-behaved. The distribution of the residuals is a bit skewed toward the negative, but not too seriously. Further, the residuals are approximately homoskedastic. \n",
    "\n",
    "### Testing the Box-Cox transform   \n",
    "\n",
    "We might be able to improve these results using the **Box-Cox transform** of the label column `city_mpg`. The goal of the Box-Cox transform is to transform the distribution of the label values (dependent variable) to be closer to Normal. The residuals of the resulting model should also be closer to Normally distributed, a key assumption of the least-squares method.       \n",
    "\n",
    "The Box-Cox transform is a **power law transform**, related to the **logarithmic transform**. The Box-Cox transform finds a power, $\\lambda$, that creates a transform of the values to be as close to Normal as possible. The Box-Cox algorithm is defined by the following relations:   \n",
    "$$\n",
    "x^{(\\lambda)}_i = \n",
    "\\begin{cases}\n",
    "      \\frac{x^{\\lambda}_i - 1}{\\lambda},\\ if \\lambda \\ne 0 \\\\\n",
    "      ln(x_i),\\ if \\lambda = 0\n",
    "\\end{cases}  \n",
    "$$\n",
    "\n",
    "Note that in this formulation $\\lambda=0$ is the logarithm since $log(1) = 0$. Any values $0 \\gt \\lambda \\lt 1$ are roots of the values $x$. For example, for square root we get $\\lambda = log(0.5) = -0.69$.  For values $1 \\gt \\lambda \\gt \\inf$ the variable is raised to a power. As another example, if the power is squared, then $\\lambda = log(2.0) = 0.69$, same absolute value of $\\lambda$ with the opposite sign. \n",
    "\n",
    "The code in the cell below applies the Box-Cox transform to the label column and prints the power, $\\lambda$, computed. The code uses the [boxcox](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) function from scipy.stats. The data sample split again into training and test subsets. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Box-Cox transform to the label and print the power used   \n",
    "auto_data.loc[:,'city_mpg'], power = ss.boxcox(auto_data.loc[:,'city_mpg'])\n",
    "print('The power of the Box-Cox transform = {0:6.3f}'.format(power))\n",
    "\n",
    "## Split the data using the transformed label values\n",
    "## Create a mask and use it to split the data into a train and test set   \n",
    "nr.seed(654566)\n",
    "mask = nr.choice(auto_data.index, size = 100, replace=False)\n",
    "auto_data_train = auto_data.iloc[mask,:].reset_index()\n",
    "auto_data_test = auto_data.drop(mask, axis=0).reset_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute a new OLS model, execute the code below and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the model using the transformed label and display the summary\n",
    "base_model = smf.ols(formula, data=auto_data_train).fit()\n",
    "\n",
    "## Display diagnostics using the training data\n",
    "print_metrics(auto_data_test, base_model)   \n",
    "print(base_model.summary())\n",
    "auto_data_train = compute_residuals(auto_data_train, base_model)\n",
    "plot_resid_dist(auto_data_train)\n",
    "residual_plot(auto_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\lambda$ is very close to 0, indicating the Box-Cox transform is close to a logarithm. As a result, the values of adjusted $R^2$, the F-statistic and the log-likelihood have all improved. The distribution of the residuals has indeed changed, but is still noticeably non-Normal in the tail. Given the improved least-squares fit, we will continue with the transformed label.   \n",
    "\n",
    "This model is still significantly over-fit. We employ regularization methods to address this situation. We will use the metrics displayed as a basis of comparison with regularized models.  \n",
    "\n",
    "> **Note:** No one regularization method works in all cases. In the running example you will see deviations from ideal behavior. Do not be surprised if not all methods improve the models. Further, do not generalize the behavior you observe in the exercises to other models. The effectiveness of regularization method is problem and model dependent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply l2 regularization\n",
    "\n",
    "Now, you will apply **l2 regularization** to constrain the model parameter values. Constraining the model parameters over-fitting of the model. This method is also known as **Ridge Regression**. \n",
    "\n",
    "But, how does this work? l2 regularization applies a **penalty** proportional to the **l2** or **Euclidean norm** of the model weights to the loss function. For linear regression using squared error as the metric, the total **loss function** is the sum of the squared error and the regularization term. The total loss function can then be written as follows:  \n",
    "\n",
    "$$J(\\beta) = ||A \\beta - b||^2 + \\alpha^2 ||\\beta||^2$$\n",
    "\n",
    "Where the penalty term on the model coefficients, $\\beta_i$, based on the Euclidean norm:\n",
    "\n",
    "$$|| \\beta||^2 =  \\big(\\beta_1^2 + \\beta_2^2 + \\ldots + \\beta_n^2 \\big)^{\\frac{1}{2}} = \\alpha \\Big( \\sum_{i=1}^n \\beta_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We call $||\\beta||^2$ the **l2 norm** of the coefficients, since we raise the weights of each coefficient to the power of 2, sum the squares and then raise the sum to the power of $\\frac{1}{2}$. \n",
    "\n",
    "You can think of this penalty as constraining the 12 or Euclidean norm of the model weight vector. The value of $\\alpha$ determines how much the norm of the coefficient vector constrains the solution. You can see a geometric interpretation of the l2 penalty constraint in the figure below.  \n",
    "\n",
    "<img src=\"../images/L2.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n",
    "<center>Geometric view of l2 regularization\n",
    "\n",
    "Notice that for a constant value of the l2 norm, the values of the model parameters $B_1$ and $B_2$ are related. The Euclidean or l2 norm of the coefficients is shown as the dotted circle. The constant value of the l2 norm is a constant value of the penalty. Along this circle the coefficients change in relation to each other to maintain a constant l2 norm. For example, if $B_1$ is maximized then $B_2 \\sim 0$, or vice versa. It is important to note that l2 regularization is a **soft constraint**. Coefficients are driven close to, but likely not exactly to, zero.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Eigenvalue Decomposition\n",
    "\n",
    "**Eigenvalues** are characteristic roots or characteristic values of a linear system of equations. The **eigenvalue-eigenvector** decomposition is a factorization of the a matrix. \n",
    "\n",
    "Let's start with a **square matrix**, $A$:\n",
    "\n",
    "$$A = \n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next define a vector, $x$: \n",
    "\n",
    "$$x = \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then an **eigenvalue** of the matrix $A$ has the property: \n",
    "\n",
    "$$A x = \\lambda x$$\n",
    "\n",
    "Or,   \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\lambda \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To see that the eigenvalue, $\\lambda$, is a root of the matrix, $A$ we can rearrange the above as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "Ax - \\lambda x &= 0 \\\\\n",
    "(A - I \\lambda) x &= 0\n",
    "\\end{align}\n",
    "\n",
    "Where, $I$ is the **identity matrix** of 1 on the diagonal and 0 elsewhere. These relationships can be written as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11} - \\lambda  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} - \\lambda  & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn} - \\lambda \n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "0\n",
    "$$\n",
    "\n",
    "\n",
    "The foregoing show that the eigenvalue, $\\lambda$, is a root of the matrix, $A$.\n",
    "\n",
    "For an $n\\ x\\ n$ matrix, $A$, there are $n$ eigenvalues or roots. These can be found by solving the following equation, using the determinant:  \n",
    "\n",
    "$$det(A - x) = 0$$\n",
    "\n",
    "You can find more information on the determinant in this [article](https://en.wikipedia.org/wiki/Determinant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Eigenvalues and the Normal Equations\n",
    "\n",
    "Let's start by examining the **normal equation** formulation of the linear regression problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ and a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "The commonly used normal equation form can help with this problem:\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is a symmetric $m x m$ covariance matrix, where $m$ is the number of model coefficients. This is a significant reduction in size when compared to $A$. \n",
    "\n",
    "Now, we can perform eigenvalue-eigenvector decomposition of $A^TA$:\n",
    "\n",
    "$$A^TA = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$Q = $ unitary matrix of orthonormal **eigenvectors**, and\n",
    "$\\Lambda =$ diagonal matrix of **eigenvalues**. The eigenvalue matrix is diagonal:  \n",
    "\n",
    "$$\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "    \\lambda_1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\lambda_2 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\lambda_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Since Q is unitary (unit norm), the inverse of $A^TA$ is easily computed:\n",
    "\n",
    "$$(A^TA)^{-1} = Q \\Lambda^{-1} Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$$\\Lambda^{-1} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_n}\n",
    "\\end{bmatrix}$$\n",
    "and $\\lambda_i$ is the ith eigenvalue. \n",
    "\n",
    "But, **$A^TA$ can still be rank deficient!** By rank deficient we mean that there are fewer non-zero eigenvalues of $A^TA$ than the dimension, $n$. Even if the ith eigenvalue is close to zero, $\\frac{1}{\\lambda_i}$ becomes very large and destabilizes the inverse. \n",
    "\n",
    "The basic idea of $l_2$ regularization, Tikhonov regularization, or ridge regression is to stabilize the inverse eigenvalue matrix,$\\Lambda$, by **adding a small bias term**, $\\alpha$, to each of the eigenvalues. We can write this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel + \\parallel \\alpha^2 \\cdot I \\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\alpha^2 \\cdot I)^{-1}A^Tx$$\n",
    "\n",
    "In this way, the inverse values of small eigenvalues do not blow up when we compute the inverse. You can see this by writing out the $\\Lambda^+$ matrix with the bias term.\n",
    "\n",
    "$$\\Lambda_{Tikhonov}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1 + \\alpha^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2 + \\alpha^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_m + \\alpha^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term ensures there are no non-zero eigenvalues, and that the inverse of $A^TA$ exists. You can also see that added the bias term adds a 'ridge' along the diagonal of the eigenvalue matrix, giving this method one of its names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian Interpretation\n",
    "\n",
    "Another way to view l2 regularization is using a Bayesian formulation of the problem. Let's start with the posterior distribution of the weight vector $W$ given the features $X$ and labels $Y$. Using Bayes rule we can write this posterior distribution as:\n",
    "\n",
    "$$p( W\\ |\\ \\{X,Y\\} ) = \\frac{p(W)\\ p(\\{X,Y\\}\\  |\\  W\\ )}{p( \\{X,Y\\})}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$p(W) = $ the prior distribution of $W$.   \n",
    "$p(\\{X,Y\\}\\  |\\  W\\ ) = $ the likelihood of $\\{X,Y\\}$ given $W$.\n",
    "\n",
    "We want the **maximum a posteriori** or **MAP** of the weights, $W$. Taking the log of both sides:\n",
    "\n",
    "$$max_W log \\big( p( W\\ |\\ \\{X,Y\\} ) \\big) \\propto max_W\\ \\Big[ log \\big( p(W) \\big)\\  + log \\big( p(\\{X,Y\\}\\  |\\  W\\ ) \\big) \\Big]\\\\\n",
    "= max_W \\Big[ log\\big(prior(W)\\big) + log \\big( likelihood(\\{X,Y\\}\\  |\\  W\\ ) \\big) \\Big]$$\n",
    "\n",
    "For a Gaussian process with l2 loss and l2 regularization we formulate the problem:\n",
    "\n",
    "$$max_W log \\big( p( W\\ |\\ \\{X,Y\\} ) \\big) = max_W \\Big[\\frac{1}{n} \\sum_{i=1}^n (f_W(x_i) - y_i)^2 + \\lambda || W ||^2 \\big) \\Big]$$  \n",
    "\n",
    "where,   \n",
    "$\\frac{1}{n} \\sum_{i=1}^n (f_W(x_i) - y_i)^2 = $ the likelihood.    \n",
    "$\\lambda || W ||^2 = $ the prior acting as a regularization penalty. \n",
    "\n",
    "\n",
    "So how are we to interpret this prior? It is useful to view the prior as constraining the norm of the weight vector close to zero. In other words, we think of the weights as **shrinking** toward zero. Thus, the shrinkage process prevents the weights from reaching extreme values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example    \n",
    "\n",
    "How can we use the eigenvalue decomposition of the covariance matrix to better understand how stable a linear model is? One way to summarize stability is to compute the **condition number**, which is the ratio of the largest to the smallest eigenvalue:   \n",
    "\n",
    "$$condition\\ number = \\frac{largest\\ eigenvalue}{smallest\\ eigenvalue}$$   \n",
    "\n",
    "A linear model with a covariance matrix having a large condition number is unstable and the model coefficients will be poorly determined. In other words, when the condition number is large, we can expect high variance and poor generalization. As a rule of thumb, the condition number of a **well-posed** linear model should be less than about 100. Otherwise, we say the model is **ill-posed**.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-1:**  You will now compute the covariance matrix for the model specified above and evaluate its eigenvalues.     \n",
    "> 1. Construct the design (model) matrix using the training data subset. You can do this with the [patsy.dematrix](https://patsy.readthedocs.io/en/latest/API-reference.html) function.      \n",
    "> 2. Use [numpy.transpose](https://numpy.org/devdocs/reference/generated/numpy.transpose.html) and [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) functions to compute the covariance matrix of the design matrix. Make sure you normalize by dividing by the dimension of the covariance matrix!     \n",
    "> 3. Compute the [numpy.linalg.eigvals](https://numpy.org/devdocs/reference/generated/numpy.linalg.eigvals.html) function to compute the eigenvalues. Save the real part of these complex numbers using [numpy.real](https://numpy.org/devdocs/reference/generated/numpy.real.html) and print the result.   \n",
    "> 4. Compute and print the condition number.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does this condition number tell you about the stability of the coefficient estimates for this model? Is this consistent with what you learned from the model summary?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of ridge regression   \n",
    "\n",
    "When L2 regularization is applied to regression the resulting algorithm is often referred to as **ridge regression**. For this algorithm, the regularization parameter, $\\alpha$, must be selected. For a given problem, there are limited theory-based options to find the best value of $\\alpha$, Therefore, one typically resorts to a **hyperparameter** search. \n",
    "\n",
    "There are several possible approaches to hyperparameter search. An error metric, such as MSE or MAE is used to determine the optimal value.    \n",
    "\n",
    "The simplest approach is to search of grid (or line) of regularly spaced hyperparameter values. At each gird point the model performance metric is computed. The best model is considered the one with the best metric. We will use this simpler approach here.       \n",
    "\n",
    "An alternative is to randomly sample the space of hyperparameter values. This later approach is more efficient.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-2:** You will now perform a search to find a best regularization hyperparameter by the following steps:      \n",
    "> 1. Complete the code in the `regularized_coefs` function to compute a regularized OLS model using the value of alpha and L1_wt using [statsmodels.regression.linear_model.OLS.fit_regularized](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html). For each value of alpha searched, this model will be computed and evaluated.      \n",
    "> 2. In the space provided below, provide code to create an array of alpha values from 0.0 to 0.005 in steps of 0.00005.  \n",
    "> 3. Using the `regularized_coefs` define code for the search over the values of $\\alpha$.   \n",
    "> 4. Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_coefs(df_train, df_test, alphas, L1_wt=0.0, n_coefs=8,\n",
    "                      formula = formula, label='city_mpg'):\n",
    "    '''Function that computes a linear model for each value of the regualarization \n",
    "    parameter alpha and returns an array of the coefficient values. The L1_wt \n",
    "    determines the trade-off between L1 and L2 regualarization'''\n",
    "    coefs = np.zeros((len(alphas),n_coefs + 1))\n",
    "    MSE_train = []\n",
    "    MSE_test = []\n",
    "    for i,alpha in enumerate(alphas):\n",
    "        ## First compute the training MSE\n",
    "        #### Complete the line of code below\n",
    "\n",
    "        \n",
    "        ## Save the coefficient values   \n",
    "        coefs[i,:] = temp_mod.params\n",
    "        ## Compute and save the training RMSE\n",
    "        MSE_train.append(sqrt(np.mean(np.square(df_train[label] - temp_mod.predict(df_train)))))\n",
    "        ## Then compute the test RMSE\n",
    "        MSE_test.append(sqrt(np.mean(np.square(df_test[label] - temp_mod.predict(df_test)))))\n",
    "        \n",
    "    return coefs, MSE_train, MSE_test\n",
    "\n",
    "\n",
    "def plot_coefs(coefs, alphas, MSE_train, MSE_test, ylim=None, \\\n",
    "               title='MSE vs. regularization parameter number',\\\n",
    "               ylab='Root mean squared error', location='lower right'):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis\n",
    "    for i in range(coefs.shape[1]): # Iterate over coefficients\n",
    "        ax[0].plot(alphas, coefs[:,i])\n",
    "    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)\n",
    "    ax[0].set_ylabel('Partial slope values')\n",
    "    ax[0].set_xlabel('alpha')\n",
    "    ax[0].set_title('Parial slopes vs. regularization parameter')\n",
    "    if ylim is not None: ax[0].set_ylim(ylim)\n",
    "    \n",
    "    ax[1].plot(alphas, MSE_train, label='Training error')\n",
    "    ax[1].plot(alphas, MSE_test, label='Test error')\n",
    "    ax[1].set_ylabel(ylab)\n",
    "    ax[1].set_xlabel('alpha')\n",
    "    ax[1].set_title(title)\n",
    "    plt.legend(loc=location)\n",
    "    plt.show()\n",
    "\n",
    "np.random.seed(12856)\n",
    "### You code goes here\n",
    "\n",
    "\n",
    "plot_coefs(Betas, alphas, MSE_train, MSE_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these plots and answer these questions:     \n",
    "> 1. Notice how the training error increases in value with increasing regularization hyperparameter. This is expected, since as the coefficient values of the model are forced toward 0, the training bias increases. Notice however, the behavior of the MSE for the test data. To single digit precision, approximately at which value is the test MSE minimized?         \n",
    "> 2. The parameters with the largest magnitude are intercept and gas fuel. Describe how these parameters change with $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.                \n",
    "> 2.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-3:** Now you will evaluate the L2 regularized model using a value of $\\alpha = 0.001$. This is value of $\\alpha$ provides only mild regularization and will provide a basis for comparison with subsequent models.       \n",
    "> 1. Compute a regularized OLS model using the training data and the [statsmodels.regression.linear_model.OLS.fit_regularized](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html) method.     \n",
    "> 2. Print the coefficient comparison between the base model and the regularized model.   \n",
    "> 3. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 4. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coefficient_comparision(mod, compare_mod):\n",
    "    df = pd.DataFrame(compare_mod.params)  \n",
    "    df = pd.concat([df, pd.Series(mod.params, index=df.index)], axis=1)\n",
    "    df.columns = ['Compare model','Model']                \n",
    "    print(df)\n",
    "    comp_mag = np.linalg.norm(df.loc[:,'Compare model'])\n",
    "    mag = np.linalg.norm(df.loc[:,'Model'])\n",
    "    df.drop('Intercept', axis=0, inplace=True)\n",
    "    comp_mag_nointercept = np.linalg.norm(df.loc[:,'Compare model'])\n",
    "    mag_nointercept = np.linalg.norm(df.loc[:,'Model'])\n",
    "\n",
    "    print('\\nMagnitude of base model = {0:4.2f}  Without intercept = {1:4.2f}'.format(comp_mag, comp_mag_nointercept))\n",
    "    print('Magnitude of new model = {0:4.2f}  Without intercept = {1:4.2f}'.format(mag, mag_nointercept))\n",
    "\n",
    "### You code goes here\n",
    "\n",
    "\n",
    "## Display results   \n",
    "print_coefficient_comparision(L2_model, base_model)\n",
    "print('\\n')\n",
    "print_metrics(auto_data_test, L2_model, label_col='city_mpg')\n",
    "auto_data_test = compute_residuals(auto_data_test, L2_model)\n",
    "plot_resid_dist(auto_data_test)\n",
    "residual_plot(auto_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions:   \n",
    "> 1. Examine the comparison of the model coefficients. What does the change in the norm of the parameter vector tell you about the regularization?        \n",
    "> 2. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse? Keeping in mind that the model is a constrained least squares fit, do the results make sense?   \n",
    "> 3. How does the distribution of the residuals compare to those of the unregularized models in terms of changes of skewness and the outlier?\n",
    "> . Do the residuals still appear approximately homoskedastic? \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.            \n",
    "> 2.           \n",
    "> 3.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-4:** You will now compare the condition number of the regularized covariance matrix to the unregularized covariance matrix you computed earlier.     \n",
    "> 1. Add a matrix with the value of the square root of the optimal $alpha$ value estimated along the diagonal (0 elsewhere) to the covariance matrix you computed in Exercise 24-2. Use [numpy.diag]() to instantiate the diagonal matrix.    \n",
    "> 2. Compute and display the eigenvalues of the regularized covariance matrix.  \n",
    "> 3. Compute and display the condition number of the regularized covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the condition numbers you have computed for the regularized and unregularized condition number. Has regularization made a significant difference? Does the regularized model still appear to have an undesirably large condition number?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply l1 regularizaton\n",
    "\n",
    "Regularization can be performed using norms other than l2. The **l1 regularizaton** or **Lasso**  method limits the sum of the absolute values of the model coefficients. The l1 norm is sometime know as the **Manhattan norm**, since distance are measured as if you were traveling on a rectangular grid of streets. This is in contrast to the l2 norm that measures distance 'as the crow flies'. \n",
    "\n",
    "We can compute the l1 norm of the model coefficients as follows:\n",
    "\n",
    "$$||\\beta||^1 = \\big( |\\beta_1| + |\\beta_2| + \\ldots + |\\beta_n| \\big) = \\Big( \\sum_{i=1}^n |\\beta_i| \\Big)^1$$\n",
    "\n",
    "where $|\\beta_i|$ is the absolute value of $\\beta_i$. \n",
    "\n",
    "Notice that to compute the l1 norm, we raise the sum of the absolute values to the first power.\n",
    "\n",
    "As with l2 regularization, for l1 regularization, a penalty term is multiplied by the l1 norm of the model coefficients. A penalty multiplier, $\\alpha$, determines how much the norm of the coefficient vector constrains values of the weights. The complete loss function is the sum of the squared errors plus the penalty term which becomes: \n",
    "\n",
    "$$J(\\beta) = ||A \\beta - b||^2 + \\alpha^2 ||\\beta||^1$$\n",
    "\n",
    "You can see a geometric interpretation of the l1 norm penalty in the figure below.  \n",
    "\n",
    "<img src=\"../images/L1.jpg\" alt=\"Drawing\" style=\"width:700px; height:400px\"/>\n",
    "<center> Geometric view of L1 regularization\n",
    "\n",
    "The l1 norm is constrained by the sum of the absolute values of the coefficients. This fact means that values of one parameter highly constrain another parameter. The dotted line in the figure above looks as though someone has pulled a rope or lasso around pegs on the axes. This behavior leads the name lasso for l1 regularization.  \n",
    "\n",
    "Notice that in the figure above that if $B_1 = 0$ then $B_2$ has a value at the limit, or vice versa. In other words, using a l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values. Thus, you can think of the l1 norm constraint **knocking out** some weights free the model altogether. In contrast to l2 regularization, l1 regularization does drive some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-5:** Continuing with the running example you will now apply L1 regularization to the model.    \n",
    "> 1. Create an array with values of $\\alpha$ from 0.0 to 0.05 in steps of 0.005.   \n",
    "> 2. Using the hyperparameter search function you created for exercise 24-2, compute the model performance metrics for each value of $\\alpha$ and with `L1_wt` set to 1.0; all weight on L1 regularization. \n",
    "> 3. Plot the results using the `plot_coefs` function. It will help your understanding to set `ylim=[-0.3,0.3]` for the `plot_coefs` function. \n",
    "> 4. Execute your code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these plots and answer these questions.    \n",
    "> 1. Examine the change in parameter values with increasing $\\alpha$. What evidence do you see that the L1 regularization is working as expected?    \n",
    "> 2. Notice how the training error increases with increasing regularization hyperparameter. This is expected, since as the coefficient values of the model are forced toward 0, the training bias increases. Notice the behavior of the MSE for the test data. Where is the minimum? What does this tell you about the bias-variance trade-off?              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.        \n",
    "> 2.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-6:** Now you will evaluate the L1 regularized model using the optimal a value $\\alpha$ where there is a minimum in the test error curve.      \n",
    "> 1. Compute a regularized OLS model using the training data and your estimate of the optimal value of $\\alpha = 0.015and `L1_wt=1.0`.     \n",
    "> 2. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 3. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.   \n",
    "> 4. Print the model coefficients. These coefficients are the `params` attribute of the model object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions.   \n",
    "> 1. Examine the comparison of the model coefficients. What does the change in the norm of the parameter vector tell you about the regularization?    \n",
    "> 2. Examine the model coefficients, noticing that some are 0.0 as expected with L1 regularization. What does this tell you about the usefulness of some of the model features?     \n",
    "> 3. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse and is this outcome expected?    \n",
    "> 4. How does the distribution of the residuals compare to those of the unregularized models in terms of changes of skewness, kurtosis and the outlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.          \n",
    "> 2.           \n",
    "> 3.               \n",
    "> 4.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regularization    \n",
    "\n",
    "We have now examined a bit of theory and examples of L2 and L1 regularization. We can compare the characteristics of these methods as follows:  \n",
    "\n",
    "- L2 regularization works well for **colinear features**    \n",
    "   - Down-weights colinear features   \n",
    "   - But soft constraint so poor model selection \n",
    "\n",
    "- L1 regularization provides **good model selection** by hard constraint    \n",
    "   - But poor selection for colinear features     \n",
    "\n",
    "But, we do not always have to choose between soft constraint of L2 and hard constraint of L1. The **elastic net regularization** combines the behavior of both methods. The loss function for elastic net is expressed:         \n",
    "\n",
    "$$min \\Big[ \\parallel A \\cdot x - b \\parallel +\\ \\lambda\\ \\alpha^2 \\parallel b\\parallel^1 +\\ (1- \\lambda)\\ \\alpha^2 \\parallel b\\parallel^2 \\Big]$$        \n",
    "\n",
    "This model has two hyperparameters:    \n",
    "- $\\lambda$ weights L1 vs. L2 regularization.      \n",
    "- $\\alpha$ sets strength of regularization.   \n",
    "\n",
    "Tuning this model, requires a 2-dimensional hyperparameter search. This search can be done on a grid or by random sampling, as was discussed previously.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-7:**  Continuing with the running example you will now apply elastic net regularization to the model.    \n",
    "> 1. Create an array with values of $\\alpha$ from 0.0 to 0.03 in steps of 0.0005.   \n",
    "> 2. Using the hyperparameter search function you created for exercise 24-2 compute the model performance metrics for each value of $\\alpha$ and with `L1_wt` set to 0.5. \n",
    "> 3. Plot the results using the `plot_coefs` function. It will help your understanding to set `ylim=[-0.5,1.0]` for the `plot_coefs` function.    \n",
    "> 4. Execute your code.  \n",
    "\n",
    "**Note:** In this case, we equal weight L2 and L1 regularization in order to simplify the hyperparameter search.\n",
    " Performance could possibly improved if a 2-hyperparameter search was performed.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One again, the curve of training error does not have a well defined minimum, except at $\\alpha = 0$. \n",
    "> 1. How are the parameter values changing as the regularization parameter increases, in particular coefficients driven to 0?    \n",
    "> 2. Is there any well defined minimum for the test error?     \n",
    "> 3. Based on this behavior, do you expect that elastic net regularization to improve model generalization?      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.          \n",
    "> 2.            \n",
    "> 3.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 24-8:** Now you will evaluate the elastic net regularized model using an arbitrary value of $\\alpha$.      \n",
    "> 1. Compute a regularized OLS model using the training data an estimate of the optimal value of $\\alpha = 0.015$ and `L1_wt=0.5`, putting equal weight on L2 and L1.     \n",
    "> 2. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 3. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.   \n",
    "> 4. Print the model coefficients. These coefficients are the `params` attribute of the model object.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions.   \n",
    "> 1. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse and is this behavior expected? \n",
    "> 2. Do the residuals still appear approximately Normal and homoscedastic?    \n",
    "> 3. Examine the model coefficients? Are any of the coefficients 0? Is this behavior expected from the soft constraint of L2 regularization or the hard constraint of L1 regularization.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.            \n",
    "> 2.              \n",
    "> 3.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Example: GLM with ElasticNet     \n",
    "\n",
    "Let's try an end-to-end example. In previous chapters we worked with an HR dataset with the objective of classifying employees who are likely to leave a company. We will now apply ElasticNet regularization to a generalized linear model (GLM) to this problem.       \n",
    "\n",
    "We will start by loading and preparing the dataset. Execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data = pd.read_csv('../data/HR_comma_sep.csv')\n",
    "for col in ['satisfaction_level','average_montly_hours','last_evaluation', 'number_project', 'time_spend_company']:\n",
    "    hr_data.loc[:,col] = (hr_data.loc[:,col] - np.mean(hr_data.loc[:,col]))/np.std(hr_data.loc[:,col])\n",
    "## Create a mask and use it to split the data into a train and test set  \n",
    "frac = 0.6\n",
    "nr.seed(665)\n",
    "mask = nr.choice(hr_data.index, size = int(frac * hr_data.shape[0]), replace=False)\n",
    "hr_train = hr_data.iloc[mask,:]\n",
    "hr_test = hr_data.drop(mask, axis=0)         \n",
    "hr_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few columns in this dataset. We use most of these columns as independent variables for our GLM model. The code in the cell below constructs the model and prints the summary. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'left ~ satisfaction_level + average_montly_hours + last_evaluation + C(salary) + C(promotion_last_5years) +\\\n",
    "          C(Work_accident) + number_project + time_spend_company + C(promotion_last_5years) + C(sales)'\n",
    "hr_glm = smf.glm(formula=formula, data=hr_train, family=sm.families.Binomial()).fit()\n",
    "hr_glm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model summary. The deviance and Pearson $\\chi^2$ indicate the model makes significant predictions. However, notice that the model is significantly overfit, with nearly all coefficients not being significant.      \n",
    "\n",
    "To evaluate this model execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(hr_test, threshold):\n",
    "    print('\\n\\nPrediction of leaving:')\n",
    "    print(hr_test.loc[:5,'predicted'])\n",
    "    \n",
    "    print('\\nConfusion Matrix')\n",
    "    Confusion_Matrix = metrics.confusion_matrix(hr_test.loc[:,'left'], hr_test.loc[:,'predicted'])\n",
    "    accuracy = metrics.accuracy_score(hr_test.loc[:,'left'], hr_test.loc[:,'predicted'])\n",
    "    precision = metrics.precision_score(hr_test.loc[:,'left'], hr_test.loc[:,'predicted'])\n",
    "    recall = metrics.recall_score(hr_test.loc[:,'left'], hr_test.loc[:,'predicted'])\n",
    "    Confusion_Matrix = pd.DataFrame(Confusion_Matrix, index=['True Stay', 'True Leave'], columns = ['Predicted Stay', 'Predicted Leaving'])\n",
    "    print(Confusion_Matrix)\n",
    "    print(f\"\\nAccuracy = {round(accuracy, 3)}\")\n",
    "    print(f\"Precision = {round(precision, 3)}\")\n",
    "    print(f\"Recall = {round(recall, 3)}\")\n",
    "\n",
    "threshold = 0.4\n",
    "hr_test.loc[:,'predicted_prob'] = hr_glm.predict(hr_test)\n",
    "hr_test.loc[:,'predicted'] = np.where(hr_test.loc[:,'predicted_prob'] > threshold, 1, 0)\n",
    "display_metrics(hr_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these performance metrics as a basis of comparison.     \n",
    "\n",
    "Next, we will test a number of regularization parameter values. Execute the code in the cell below and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_coefs_glm(df_train, df_test, alphas, L1_wt=0.0, n_coefs=18,\n",
    "                      formula = formula, label='left', threshold = 0.4):\n",
    "    '''Function that computes a linear model for each value of the regualarization \n",
    "    parameter alpha and returns an array of the coefficient values. The L1_wt \n",
    "    determines the trade-off between L1 and L2 regualarization'''\n",
    "    coefs = np.zeros((len(alphas),n_coefs + 1))\n",
    "    err_train = []\n",
    "    err_test = []\n",
    "    for i,alpha in enumerate(alphas):\n",
    "        ## First compute the training MSE\n",
    "        #### Complete the line of code below\n",
    "        temp_mod = smf.glm(formula, data=df_train, family=sm.families.Binomial()).fit_regularized(alpha=alpha,L1_wt=L1_wt)\n",
    "        \n",
    "        ## Save the model coefficeints\n",
    "        coefs[i,:] = temp_mod.params\n",
    "        ## Compute training error \n",
    "        err_train.append(np.sum(np.abs(df_train.loc[:,label] - np.where(temp_mod.predict(df_train) > threshold, 1, 0)))/float(len(hr_train)))\n",
    "        ## Then compute the test error\n",
    "        err_test.append(np.sum(np.abs(df_test.loc[:,label] - np.where(temp_mod.predict(df_test) > threshold, 1, 0)))/float(len(df_train)))        \n",
    "    return coefs, err_train, err_test\n",
    "\n",
    "\n",
    "alphas = np.arange(0.0, 0.3, step = 0.02)\n",
    "Betas, err_train, err_test = regularized_coefs_glm(hr_train, hr_test, alphas, L1_wt=0.5, formula = formula)\n",
    "\n",
    "plot_coefs(Betas, alphas, err_train, err_test, title='Error vs. regularization parameter',\\\n",
    "           ylab='Classification error rate', location='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test classification error is minimized at the $\\alpha=0.16$. Notice that at $\\alpha=0.16$ only 7 of the original 19 model coefficients are nonzero.  \n",
    "\n",
    "Finally, we can evaluate this model at the optimal value of $\\alpha$. Execute the code in the cell bellow to construct and evaluate the regularized model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_glm_elastic = smf.glm(formula=formula, data=hr_train, family=sm.families.Binomial()).fit_regularized(alpha=0.16,L1_wt=0.5)\n",
    "\n",
    "print_coefficient_comparision(hr_glm_elastic, hr_glm)\n",
    "\n",
    "threshold = 0.4\n",
    "hr_test.loc[:,'predicted_prob'] = hr_glm_elastic.predict(hr_test)\n",
    "hr_test.loc[:,'predicted'] = np.where(hr_test.loc[:,'predicted_prob'] > threshold, 1, 0)\n",
    "display_metrics(hr_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these results to the unregularized model. These metrics are somewhat improved over the unregularized model. This result is a bit unusual, but seem to arise from the fact that the unregularized model was so severely overfit. The regularization has improved the generalization, hence the better performance with the test dataset.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have explored the basics of regularization. Regularization can prevent machine learning models from being overfit. Regularization is required to help machine learning models generalize when placed in production. Selection of regularization strength involves consideration of the bias-variance trade-off. \n",
    "\n",
    "L2 and l1 regularization constrain model coefficients to prevent overfitting. L2 regularization constrains model coefficients using a Euclidian norm. L2 regularization can drive some coefficients toward zero, usually not to zero. On the other hand, l1 regularization can drive model coefficients to zero.    \n",
    "\n",
    "The elastic net algorithm provides weighted behavior between the L1 and L2 methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024 Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
