{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dc3d4d-aa0b-40aa-b79a-de88718c2b19",
   "metadata": {},
   "source": [
    "# Chapter 19    \n",
    "# Models for Messy Data  \n",
    " \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Real-world data often has complex and messy behavior. In this chapter we explore some methods that can be used to successfully model messy data. In particular, we will explore the following models:     \n",
    "1. Regresion for **zero-inflated** count data using Poisson regression. Zero inflated data arises when there is a prevelance of observations with zero counts.\n",
    "2. Regression for **over-dispursed** data using negative binomial generalized linear model regression. Over-dispursed data arrises when there are observations with counts beyond what can be expected from a Poisson distribution.\n",
    "3. The **influcence of outliers** on the response of linear models. \n",
    "4. **Robust regression** models for datasets containing outliers in the observations.\n",
    "\n",
    "Before conituing with this notebook, exectute the code in the cell below to import the reqquired packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28d126-b6ef-4785-927d-75e1d8c8fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "from statsmodels.discrete.count_model import ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as ss\n",
    "from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "def rmse_mae(df, residuals='resids'):\n",
    "    median = round(df.loc[:,residuals].median(),3)\n",
    "    mean = round(df.loc[:,residuals].mean(), 3)\n",
    "    mae = np.absolute(df.loc[:,residuals]).median()\n",
    "    rmse = np.std(df.loc[:,residuals])\n",
    "    print('Mean of residuals = ' + str(mean) + '   Mean opf residuals = ' + str(median))\n",
    "    print('RMSE = ' + str(round(rmse, 3)) + '  MAE = ' + str(round(mae,3)))\n",
    "    return rmse, mae\n",
    "\n",
    "def plot_resid_dist(resids, title1='Histogram of residuals', title2='Q-Q Normal plot of residuals'):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    ## Plot a histogram\n",
    "    sns.histplot(resids, bins=20, kde=True, ax=ax[0])\n",
    "    ax[0].set_title(title1)\n",
    "    ax[0].set_xlabel('Residual values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1])\n",
    "    ax[1].set_title(title2)\n",
    "    plt.show()\n",
    "\n",
    "def residual_plot(df, predicted='predicted', resids='resids'):\n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    RMSE = np.std(df.loc[:,resids])\n",
    "    sns.scatterplot(x=predicted, y=resids, data=df, ax=ax)\n",
    "    ax.axhline(0.0, color='red', linewidth=1.0)\n",
    "    ax.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.set_title('PLot of residuals vs. predicted')\n",
    "    ax.set_xlabel('Predicted values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8428577-29e9-4281-82d5-0c844b9c44b1",
   "metadata": {},
   "source": [
    "## Zero-Inflated Distributiions      \n",
    "\n",
    "**Zero-inflated data** is a commonly encountered problem, where a large number of observatons have a 0 value. For example, with counts there may be a excess number of observations with a value of 0. There are many real-world examples of zero inflated variables, including the following:     \n",
    "- The number of goals scored by a soccer player per game.\n",
    "- The number of items a visitor to an e-commerce website adds to their cart.\n",
    "- The number of medical patients per month diagnossed with a particular disease.\n",
    "\n",
    "A [**zero-inflated distribution**](https://en.wikipedia.org/wiki/Zero-inflated_model) is actually a **mixture of of two distributions**, a binomial distribution, and another distribution. Some common examples for counts are:     \n",
    "1. A)zero-inflated Poisson distribution is a mixture of a binomial distribution and a Poisson distribution.\n",
    "2. A zero-inflated negative-binomial distribution is a mixture of a binomial distribution and a negative binomial distribution.\n",
    "\n",
    "The basic algorithm for computing the **probability mass function (PMF)** of a zero-inflated distribution is as follows:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "af4deb56-aa04-4e36-b9dd-06af01c0d1a0",
   "metadata": {},
   "source": [
    "set n_samples, distribution     \n",
    "Proceedure pi = parameter of binomial distribution, mu = parameters of other distribution  \n",
    "   for s in n_samples \n",
    "        if binomial(pi) == 1: sample=0\n",
    "        else: sample = distribution(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26507002-1a37-4988-a285-e894b4d67232",
   "metadata": {},
   "source": [
    "### Zero-Inflated Poisson Distribution\n",
    "\n",
    "Let's explore the [**zero-inflated Poisson distribution**](https://en.wikipedia.org/wiki/Zero-inflated_model). The zero-inflated Poisson distribution is a mixture of a binomial distribution and a Poisson distribution. \n",
    "\n",
    "Recall that the PMF of the Poisson distribution is:       \n",
    "\\begin{align}    \n",
    "\\mu &= mean\\ parameter\\ of\\ Poisson\\ distribution\\\\   \n",
    "y_i &= count\\ of\\ i\\\\\n",
    "P(Y=y_i) &= \\frac{\\mu^{y_i}\\ e^{-\\mu}}{y_i!}\n",
    "\\end{align}\n",
    "The Poisson distribution is a mixture of the Poisson distribution and the binomial distribution. The probability mass funciton (PMF) of the zero-inflated distribution is the probability of counts, starting at 0. We can write distribution as:      \n",
    "\\begin{align}\n",
    "\\pi &= binomial\\ probability\\ of\\ success,\\ 0 \\le \\pi \\le 1 \\\\       \n",
    "\\mu &= mean\\ parameter\\ of\\ Poisson\\ distribution\\\\     \n",
    "y_i &= count\\ of\\ i\\\\\n",
    "P(Y=0) &= \\pi + (1 - \\pi) e^{-\\mu}\\\\\n",
    "P(Y=y_i) &= (1 - \\pi) \\frac{\\mu^{y_i}\\ e^{-\\mu}}{y_i!}\n",
    "\\end{align}\n",
    "\n",
    "To better understand this relationship, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ade0e-31e5-4a5f-8612-741a2a4ce8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero_inflated_poisson(pi, mu, x_max=15, n_samples=1000):\n",
    "    ## Instantiate data frame for output   \n",
    "    x=[float(x) for x in range(x_max+1)]\n",
    "    out_frame = pd.DataFrame({'x':x,'pmf':[0.0]*(x_max+1)})\n",
    "\n",
    "    ## Compute number of zeros and add to zero cgolumn  \n",
    "    zeros = np.random.binomial(n_samples, pi)\n",
    "    out_frame.iloc[0,1]=zeros\n",
    "\n",
    "    ## Add 1 to counts of outcomes  \n",
    "    poisson = np.random.poisson(mu, n_samples - zeros)\n",
    "    for x in poisson:\n",
    "        if x < x_max +1: out_frame.iloc[x,1] += 1.0\n",
    "\n",
    "    ## Normalize pmf and return \n",
    "    out_frame.iloc[:,1] = out_frame.iloc[:,1]/float(n_samples)\n",
    "    return out_frame\n",
    "\n",
    "pis = [0.0, 0.0, 0.2, 0.2, 0.5, 0.5]\n",
    "mus = [2.0, 5.0, 2.0, 5.0, 2.0, 5.0]\n",
    "fig, ax = plt.subplots(3,2, figsize=(8,8))\n",
    "ax = ax.flatten()\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "for pi_i, mu_i, ax_i in zip(pis,mus,ax):   \n",
    "    sample_frame = generate_zero_inflated_poisson(pi_i, mu_i)\n",
    "    sns.barplot(data=sample_frame, x='x', y='pmf', color='gray', ax=ax_i)\n",
    "    ax_i.tick_params(axis='x', rotation=90)\n",
    "    ax_i.set_ylim(0.0, 0.6)\n",
    "    ax_i.set_title('$\\pi = $' + str(pi_i) + '   $\\mu = $' + str(mu_i))\n",
    "    ax_i.set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb896e3f-4c59-4d6c-b6ed-95674a01449d",
   "metadata": {},
   "source": [
    "Notice the following properties of the zero-inflated Poisson distribution:   \n",
    "1. The first row of plots shows the Poisson distribution, with mean event counts of $2.0$ and $5.0$, with no zero-inflation.      \n",
    "2. As the parameter of the binomial distribution, $\\pi$, increase in the columns, the number of zeros increases.\n",
    "3. As the mean parameter, $\\mu$, of the Poisson distribution increases the density of larger counts increases.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140804a-14cb-4d8f-b4a3-7d2f5f35938b",
   "metadata": {},
   "source": [
    "### Zero-Inflated Negative Binomial Distribution    \n",
    "\n",
    "The Poisson distribution can be used to model count and arrival data. However, there are many cases where the event count requires a longer-tailed distribution. The [**negative binomial distribution**](https://en.wikipedia.org/wiki/Negative_binomial_distribution) is just such a distribution. The negative binomial distribution can model **over-dispursion**, or heavy heaby-tailed behavior.  \n",
    "\n",
    "The familar binomial distritution can be used to model the number of successes in a series of trials. In contrast, the negative binomial distribution models the number of failures, with the sequence ending with a sucess. Further, the negative binomial distribution has two parameters, allowing greater flexability in determining the shape of the distribution.         \n",
    "\n",
    "The PMF of the negative binomial distribution can be expressed:       \n",
    "$$f(k) = {k + n - 1 \\choose n - 1}\\ p^n (1 - p)^k$$      \n",
    "Where,    \n",
    "$k =$ number of failures.      \n",
    "$n = $ number of trials.     \n",
    "$p =$ probability of success.      \n",
    "\n",
    "For the analysis we are concerned with, another parameterization of this distribution will be useful. Recall that the mean and variance of the Poisson distribution are identical. In contrast, the negative binomial distribution can be parameterized by a mean and an **over-dispersion parameter, $\\alpha$**. This second parameter gives the negaive binomial distribution greater flexibility to model heavy tailed data, compared to the Poisson distribution. The relationship between the mean, $\\mu$, and variance, $\\sigma^2$ of the negative binomial distribution can be expressed:   \n",
    "$$\\sigma^2 = \\mu + \\alpha * \\mu^2$$     \n",
    "The over-dispersion coefficient must be $0 \\le \\alpha$. \n",
    "\n",
    "The relationship between the parameters, *p* and *n*, shown above and $\\mu$ and $\\sigma$ can be expessed:      \n",
    "\\begin{align}\n",
    "p &= \\frac{\\mu}{\\sigma^2}\\\\\n",
    "n &= \\frac{\\mu^2}{\\sigma^2 - \\mu}\n",
    "\\end{align}\n",
    "\n",
    "We can compute the zero-inflated negative binomial distribution as the mixture of the negative binomial distribuiton and a binomial distribution with probability parameter, $0 \\le \\pi \\le 1$:   \n",
    "\\begin{align}\n",
    "f(K=0) &= \\pi + (1-\\pi)\\ p^n\\\\\n",
    "f(K=k) &= (1-\\pi){k + n - 1 \\choose n - 1}\\ p^n (1 - p)^k\n",
    "\\end{align}\n",
    "In an alterntive parameterization the zero-inflation can be parameterized by a negative value, known as the **inflation constant**.\n",
    "\n",
    "The foregoing relationships are rather abstract. To get a feel for what this means, execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce602b5-8e05-4b66-800b-e9a5dec364fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero_inflated_nb(pi, mu, alpha, x_max=20, n_samples=1000):\n",
    "    ## Instantiate data frame for output   \n",
    "    x=[float(x) for x in range(x_max+1)]\n",
    "    out_frame = pd.DataFrame({'x':x,'pmf':[0.0]*(x_max+1)})\n",
    "\n",
    "    ## Compute number of zeros and add to zero cgolumn  \n",
    "    zeros = np.random.binomial(n_samples, pi)\n",
    "    out_frame.iloc[0,1]=zeros\n",
    "\n",
    "    ## Add 1 to counts of outcomes  \n",
    "    sigma_squared = mu + alpha * mu**2 \n",
    "    p = mu / sigma_squared\n",
    "    n = mu**2 / (sigma_squared + mu)\n",
    "    positive_samples = n_samples - zeros\n",
    "    negative_binomial = [int(positive_samples * nb) for nb in ss.nbinom(n, p).pmf(x)]\n",
    "    \n",
    "    out_frame.iloc[:,1] += negative_binomial\n",
    "\n",
    "    ## Normalize pmf and return \n",
    "    out_frame.iloc[:,1] = out_frame.iloc[:,1]/float(n_samples)\n",
    "    return out_frame\n",
    "\n",
    "                       \n",
    "pis = [0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.5, 0.5, 0.5]\n",
    "mus = [2.0, 2.0, 5.0, 2.0, 2.0, 5.0, 2.0, 2.0, 5.0]\n",
    "alphas = [1.0, 4.0, 2.0, 1.0, 4.0, 2.0, 1.0, 2.0, 2.0,]\n",
    "fig, ax = plt.subplots(3,3, figsize=(12,8))\n",
    "ax = ax.flatten()\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "for pi_i, mu_i, alpha_i, ax_i in zip(pis,mus, alphas,ax):   \n",
    "    sample_frame = generate_zero_inflated_nb(pi_i, mu_i, alpha_i)\n",
    "    sns.barplot(data=sample_frame, x='x', y='pmf', color='gray', ax=ax_i)\n",
    "    ax_i.set_title('$\\pi = $' + str(pi_i) + '   $\\mu = $' + str(mu_i) + '  alpha = ' + str(alpha_i))\n",
    "    ax_i.set_ylim(0.0, 0.9)\n",
    "    ax_i.set_xlabel('')\n",
    "    ax_i.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504949e-ee29-435b-b943-c46c59e2b305",
   "metadata": {},
   "source": [
    "Examine the plots above, noticing the following:        \n",
    "- The first row shows plots of the negative binomial distribution, with $\\pi=0$.\n",
    "- As the binomial parameter, $\\pi$, increases along the rows of plots the number of cases with 0 counts increases.\n",
    "- As $\\alpha$ increases, the dispursion of the PMF increases.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d5869-dfe5-4776-8cac-74bcbe5e0d31",
   "metadata": {},
   "source": [
    "### Compairing Zero-Inflated Distributions\n",
    "\n",
    "Having introduced two different zero-inflated distributions for counts, it is worth investigating how their properties compare. The code in the cell below displays side by side charts of the zero-inflated Poisson (ZIP) distribution and zero-inflated negative binomial (ZINB) distribution, , for different values of $\\pi$. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94d988-2b9d-43e1-b2b7-91327357aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=4.0\n",
    "alpha = 2.0\n",
    "pis = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "fig, ax = plt.subplots(6,2, figsize=(8,16))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "for i, pi_i in enumerate(pis):   \n",
    "    sample_frame = generate_zero_inflated_poisson(pi_i, mu)\n",
    "    sns.barplot(data=sample_frame, x='x', y='pmf', color='gray', ax=ax[i,0])\n",
    "    sample_frame = generate_zero_inflated_nb(pi_i, mu, alpha)\n",
    "    sns.barplot(data=sample_frame, x='x', y='pmf', color='gray', ax=ax[i,1])\n",
    "    for j in range(2):\n",
    "        ax[i,j].tick_params(axis='x', rotation=90)\n",
    "        ax[i,j].set_ylim(0.0, 0.8)\n",
    "        ax[i,j].set_xlim(0.0, 20.0)\n",
    "        if j==0: ax[i,j].set_title('$\\pi = $' + str(pi_i) + '   $\\mu = $' + str(mu))\n",
    "        else: ax[i,j].set_title('$\\pi = $' + str(pi_i) + '   $\\mu = $' + str(mu) + '  alpha = ' + str(alpha))\n",
    "        ax[i,j].set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b51a2-bbaf-46b1-9dac-88e0613c8405",
   "metadata": {},
   "source": [
    "There are two improtant differences between these zero-inflated distributions.     \n",
    "\n",
    "First, the number of zeros grows much faster for the negative binomial distribution compared to the Poisson distribution as the binomial parameter $\\pi$ increases. Notice that the Poisson distribution with $\\pi=0.4$ has nearly nearly as high of probability of 0 as the ZINB distribution with $\\pi=0.0$. As a result of this behavior difference, we can expect models to estimate much smaller values of the zero-inflation parameter for the ZINB distribution compaired to the ZIP distribution.    \n",
    "\n",
    "Second, the ZINB distribution has much greater dispersion for a fixed value of $\\mu$. This difference in properties can most easily be seen when $\\pi=0$. Clearly, changing the value of $\\alpha$ will change the dispersion of the ZINB distribution, whereas dispersion is fixed given a value of $\\mu$ for the ZIP distribution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929957d-eb2b-4813-ae40-aec77eba786a",
   "metadata": {},
   "source": [
    "## Exploring and Preparing the Example Dataset     \n",
    "\n",
    "To explore zero-inflated and over-dispursed data we will use the [Fish dataset from the UCLA Advanced Research Computing Group](https://stats.oarc.ucla.edu/r/dae/zip/). A download of this data is available on [Kaggle](https://www.kaggle.com/datasets/alincijov/fish-dataset). The data set has 250 observations of groups of people visiting and camping area. Most of these groups are attempting to catch fish during thier visit. However, most of the parties attempting to fish are not successful, with fish counts of zero.      \n",
    "\n",
    "As a first step, execute the code in the cell below to load this dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1a4d5-0886-4d20-8ebe-ff28696cd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/fish.csv'\n",
    "fish = pd.read_csv(file_path)\n",
    "fish.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102c202-8b1a-46e7-a01f-c6f48bab8128",
   "metadata": {},
   "source": [
    "Examining the head of the data frame, We can see that only some groups actually catch fish. This situation leads to a zero-inflated fish count.   \n",
    "\n",
    "To explore this problem a bit more, it helps to compute a summary table of how many groups caught any fish out of the total 250 groups. Execute the code in the cell below to see a summary number of groups catching and not catching fish. To get a further feel for the for the distribution of these counts execute the code in the cell below to display a histograme of the counts of fish caught.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb3a01-bcb0-4513-a518-3ba94d73ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups = fish.shape[0]\n",
    "fish['caught_fish'] = fish.loc[:,'count'] != 0\n",
    "num_caught_fish = fish['caught_fish'].sum()\n",
    "print('Total groups = ' + str(num_groups))\n",
    "print('Groups fishing = ' + str(fish[fish.loc[:,'nofish']==0].shape[0]))\n",
    "print('Groups catching fish = ' + str(num_caught_fish))\n",
    "print('Groups with no fish = ' + str(num_groups - num_caught_fish))\n",
    "print('Groups fishing with no fish = ' + str(num_groups - fish[fish.loc[:,'nofish']==0].shape[0]))\n",
    "\n",
    "fish_yesfish = fish[fish.loc[:,'nofish']==0]\n",
    "#print('Shape of the fish_yesfish data frame = ' + str(fish_yesfish.shape))\n",
    "\n",
    "cols = ['persons', 'child', 'count']\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "sns.histplot(x='count', data = fish_yesfish, bins=153, ax=ax);\n",
    "ax.set_title('Histogram of count of fish caught by groups fishing');\n",
    "ax.set_xlabel('Fish caught')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167de35-ae29-4571-b74b-c8c2f61b36d5",
   "metadata": {},
   "source": [
    "Notice the large number of zero values. This distribution is zero-inflated. Further, the distribution has a long right tail. It is doubtful that any simple distributional transform will work to standardize this response variable.           \n",
    "\n",
    "To get a better overall feel for the other variable in this dataset, execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15240537-358b-4dc4-99d8-b1a74147fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb4592-c2e0-4779-a6fb-b024b8825d8f",
   "metadata": {},
   "source": [
    "Notice the range of the variables.      \n",
    "1. The 'nofish', 'livebait' and 'camper' variables are binary. Nearly 30% of the parties are not fishing, likely contributing to the zero-inflation.     \n",
    "2. The 'persons' variable ranges from 1 to a maximum party size of 4.    \n",
    "3. The 'child' variable ranges from 0, no childern, to 3, presumable 1 adult with 3 childern.      \n",
    "4. We will ignore the `xb` and `zg` variables.\n",
    "5. The counts range from 0 to a maximum of 149. It appears that there are likely to be outliers or exagerated counts of fish counts.\n",
    "   \n",
    "We will clean the outliers since there are only 2 counts greater than 30. Execute the code in the cell below to filter the outliers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b073c9-c816-4eb4-a4f2-5d52120808e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish = fish[fish.loc[:,'count'] < 30]\n",
    "fish_yesfish = fish[fish.loc[:,'nofish']==0]\n",
    "fish.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5ed9d-e6c1-4612-9810-fcf9368ee0d9",
   "metadata": {},
   "source": [
    "With the outliers removed, we are ready to do some detailed exporatory data analysis (EDA).     \n",
    "\n",
    "Execute the cod ein the cell below to display histograms of two key variables.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd55c2b-9966-47b0-864f-6ef8a44f6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['persons', 'child']\n",
    "fig,ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "for i, col in enumerate(cols): \n",
    "    sns.histplot(x=col, data = fish_yesfish, ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c34334-89f4-47f2-9382-605dde60cc69",
   "metadata": {},
   "source": [
    "You can see that the number of pepople in the groups is realative uniform. Whereas, there are declining numbers of groups with increasing numbers of childern. As noted earlier, there must be at least one adult with groups containing childern.   \n",
    "\n",
    "Next, display the distribution of fish caught by bait type and camper or not-camper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b62a4-c137-496a-91b7-e873354234da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['livebait', 'camper']\n",
    "fig,ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "for i, col in enumerate(cols): \n",
    "    sns.violinplot(x=col, y='count', data = fish_yesfish, ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42e9c4-7eee-45aa-a9f8-18a2940ece46",
   "metadata": {},
   "source": [
    "Aparently, all groups fishing use live bait. We can eliminate this variable from our analysis.       \n",
    "\n",
    "The plot on the right shows that on average parties catch more fish when camping. However, there are outliers of large numbers of fish caught for non-camping party.     \n",
    "\n",
    "This dataset is inherently multi-variate and needs to be explored in a multidimensional manner. In this case we will seek to understand the complex relationshops between the variables. In this case we will use facet-plotting to create multi-dimensionsal views.      \n",
    "\n",
    "Now, execute the code in the cell below to display a facet plot of histograms of fish caught conditioned by persons and childern.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefdcf3-7e4b-4c40-8d07-ada86d23202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(fish_yesfish, row='child', col=\"persons\", height=2, aspect=1.0); \n",
    "g = g.map(sns.histplot, 'count', bins=30) \n",
    "for ax in g.axes.flatten(): \n",
    "    ax.set_xlabel('Number of fish caught')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aabbc2-6b90-4fb6-a713-ab0178204d46",
   "metadata": {},
   "source": [
    "Notie the following about the relationships visible in the plot:     \n",
    "1. There are no obserbvations where the number of childern is greater than or equal to the number of persons.          \n",
    "2. The zero-inflation of the counts is evident.\n",
    "3. The more childern the fewer fish caught.\n",
    "4. Larger parties catch more fish on average.\n",
    "\n",
    "Next, execute the code in the cell below to display histgrams of fish caught conditioned on number of people in the party and if the party is camping or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa29173-deb4-4e3b-a9d8-91890f8752d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(fish_yesfish, row='camper', col=\"persons\", height=2, aspect=1.0);\n",
    "g = g.map(sns.histplot, 'count', bins=30) \n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xlabel('Number of fish caught')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef3326-bc1e-455e-9831-d948554c516c",
   "metadata": {},
   "source": [
    "Notice the     \n",
    "1. The zero-inflation of the counts is evident in these plots.\n",
    "2. On average, campers catch more fish than non-campers. But, there are some clear outliers for non-campers, particularly for large parties.\n",
    "\n",
    "Finally, to examine more details of the relationship between fish caught vs. the number of people and number of childern in the parties and if the party camped or not, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec01a-c781-43e4-9b21-b3168be5a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fish(df, x, c1, c2, response, legend=True):     \n",
    "    g = sns.FacetGrid(df, col=c1, hue=c2, height=3, aspect=0.8);\n",
    "    g = g.map(sns.scatterplot, x, response, s=15, alpha=0.4) \n",
    "    for ax in g.axes.flatten():\n",
    "        ax.set_ylabel('Fish caught')\n",
    "        if legend==True: ax.legend(loc=\"upper left\")\n",
    "    return g\n",
    "\n",
    "plot_fish(fish, 'persons', 'child', 'camper', 'count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fa8c3-83c3-410d-9fc3-25084a9b5ed2",
   "metadata": {},
   "source": [
    "Once agan it is clear that parties with more adults and fewer childern catch more fish. Further, there clear outliers, for both campers and non-campers. For example, notice that there is a party of 2 people with no childern that caught over 20 fish, and a the highest number of fish caught was a party of 4 with no childern that were non-campers.      \n",
    "\n",
    "So far, we have looked at multi-dimensional views where each variable is treated independently. But, it is common in complex data to have non-linear dependencies between variables. Often these non-linear dependencies can be modeled as **interaction terms**. We can explore this posibility using a common approach, an [**interaction plot**](https://statisticsbyjim.com/regression/interaction-effects/). The interactiion plot displays the mean of the response variable for different levels of independent variables. If the dependency between the variables is approximately colinear, there is no interaction. If the mean response diverges between the levels of the variables, an interaction can be inferred.    \n",
    "\n",
    "The code in the cell below displays interaction plots for three vaiable pairs.       \n",
    "1. Number of people n a party and if the party is camping.\n",
    "2. Number of people in the party and the number of childern in the party.    \n",
    "3. Number of childern in the party abd if the party camped or not.     \n",
    "Execute the code.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82111914-6cdd-4ddc-ab02-17f4d3c31f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction(df, x, trace, response, ax):\n",
    "    \"\"\"Function to display an interaction plot using hree the \n",
    "    meam of the respose\n",
    "    df - dataframe of observations    \n",
    "    x - the factor on the x axis on the plot\n",
    "    trace - another factor variable    \n",
    "    ax - a Matplotlib axis object\"\"\"\n",
    "    \n",
    "    ## Find the unique levels of the factors and the combinatuiions\n",
    "    x_levels = df[x].unique()\n",
    "    trace_levels = df[trace].unique()\n",
    "    combinations = list(itertools.product(x_levels, trace_levels))\n",
    "\n",
    "    ## Compute the response leves\n",
    "    response_levels = [df[np.logical_and(fish_yesfish['camper']==comb[1], fish_yesfish['persons']==comb[0])].loc[:,'count'].mean() for comb in combinations]\n",
    "    response_levels = [0.0 if  math.isnan(x) else x for x in response_levels]\n",
    "\n",
    "    ## Create a data frame\n",
    "    out_frame = pd.DataFrame.from_records(combinations, columns=[x,trace])\n",
    "    out_frame['response'] = response_levels\n",
    "    \n",
    "    ## Now create the plot\n",
    "    sns.lineplot(data=out_frame, x=x, y=\"response\", hue=trace, markers=True, ax=ax);\n",
    "    ax.set_ylabel('Mean response');\n",
    "    ax.set_title('Interaction plot of ' + x + ' vs. ' + trace);\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "plot_interaction(fish_yesfish, x='persons', trace='camper', response='count', ax=ax[0])\n",
    "plot_interaction(fish_yesfish, x='persons', trace='child', response='count', ax=ax[1])\n",
    "plot_interaction(fish_yesfish, x='child', trace='camper', response='count', ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84629e-ff06-477b-8207-f4a48b968d14",
   "metadata": {},
   "source": [
    "Examine these plots and notice the following:      \n",
    "1. The plot of fish caught vs. number of people in the party are colinear for campers and non-campers, except for parties of 4. This divergence indicates a possible interaciton between number of people and camping.\n",
    "2. The polt of fish caught vs. people in the party shows muliple divergences between parties with different numbers of childern. All parties with 2 or 3 childern catch no fish on average. Parties with 0 and 1 child are colinear, except for parties of 4 which diverge. These interactions show a complex patern.\n",
    "3. The plot of fish caught vs. number of childern in the party shows colinearity for campers vs. non-campers. There is no evidence of interaction.\n",
    "\n",
    "To summarize there this dataset has complex interactions and significant over-dispersion. Given this complexities and the limited number of observations, we must expect that any model will struggle to incorporate all of this complexity.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97eb18-655e-432d-81b4-d8ae3bc48881",
   "metadata": {},
   "source": [
    "## Zero-Inflated Poisson Regression  \n",
    "\n",
    "Now that we have explored the dataset a bit, we will now attempt to create a model. Possion regression is a reasonable choice, given that the response variable is discrete counts. However, the model must account for the significant zero-inflation observed in the response variable. Therefore, for a first attempt to model these data we will use a zero-inflated Poisson regression.   \n",
    "\n",
    "All of the independent variables have the same order of magnitude so, we will not perform scaling. However, we will zerp center the independent variables so that the intercept and other coefficients are interpretable. Execute the code in the cell below to center the variables.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e64a9c-5ecb-4334-94f2-8a60b0952d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['persons','child','nofish']:\n",
    "    mean = fish.loc[:,col].mean()\n",
    "    fish.loc[:,col] = fish.loc[:,col] - mean\n",
    "    if(col=='persons'): persons_mean = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce568d2f-6b5a-4d49-bd2d-f896bd88b3da",
   "metadata": {},
   "source": [
    "As a first model, we will use the [ZeroInflatedPoisson model from Statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.discrete.count_model.ZeroInflatedPoisson.html) with a basic formula using three variables. Execute the code in the cell below to fit the model and print a summary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d7f33-83d2-4e88-856d-63b5d0dc65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'count ~ -1 + persons + child + camper' \n",
    "zero_inflated_possion_model = ZeroInflatedPoisson.from_formula(formula=formula, data=fish).fit()\n",
    "zero_inflated_possion_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb753471-e4c5-41a1-81e3-740a5147a5e0",
   "metadata": {},
   "source": [
    "> **Note:** how can we interpret the negative value of the `inflate_const`. This coefficeint is on a log scale.      \n",
    "> $$\\pi = exp(inflate\\_const)$$    \n",
    "> The binomial parameter must be within the range $0 \\le \\pi \\le 1$. Thus, the `inflate_const` must be in the range $-\\inf \\le inflate\\_const \\le 0$. The more negative the coefficient the smaller the value of $\\pi$.\n",
    ">\n",
    "> In this case, we can interpret the value of the `inflate_const` in the summary table in terms of the binomial parameter value, $exp(-0.44) = 0.64$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4375a-e851-4a4f-ba1a-87b2c4a21696",
   "metadata": {},
   "source": [
    "To compute and display summary metrics and residual plots for the models execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea234009-01f7-4cd2-b5ef-d425fa135739",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish.loc[:,'predicted'] = zero_inflated_possion_model.predict(fish)\n",
    "fish.loc[:,'resids'] = fish.loc[:,'count'].sub(fish.loc[:,'predicted'])\n",
    "\n",
    "_=rmse_mae(fish)\n",
    "plot_resid_dist(fish.loc[:,'resids'])\n",
    "residual_plot(fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2bb44-00e7-4710-a908-d85d9f42f8b3",
   "metadata": {},
   "source": [
    "Notice that the residuals for the zero-inflated Poisson regression appear to be quite heteroscedastic. But, keep in mind that the residuals for a zero-inflated Poisson regression are not expected to be Normal, given the response of the GLM. It is clear that we need a transformation of the residuals to make these plots more interpretable.      \n",
    "\n",
    "A common approach to transform the residuals for a GLM is to apply the [**Pearson residual adjustment**](https://www.statology.org/pearson-residuals/). This adjustment transforms the distribution of the residuals to $\\chi^2$. For our case, we will use a basic form of this transformation:      \n",
    "\n",
    "$$r_i^{Pear} = \\frac{y - \\hat{y}}{\\sqrt{\\hat{y}}}$$\n",
    "\n",
    "The code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130591c-ac98-4e47-af49-fc3125387694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_residual_plot(df, predicted='predicted', response='count'):\n",
    "    df['resids'] =  np.divide(df.loc[:,response].sub(df.loc[:,predicted]), np.sqrt(df.loc[:,predicted]))\n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    RMSE = np.std(df.loc[:,'resids'])\n",
    "    sns.scatterplot(x=predicted, y='resids', data=df, ax=ax)\n",
    "    ax.axhline(0.0, color='red', linewidth=1.0)\n",
    "    ax.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.set_title('PLot of Pearson adjusted residuals vs. predicted')\n",
    "    ax.set_xlabel('Predicted values')\n",
    "    ax.set_ylabel('Pearson adjusted residuals')\n",
    "    plot_resid_dist(fish.loc[:,'resids'], title1='Histogram of Pearson adjusted residuals', title2='Q-Q Normal plot of Pearson adjusted residuals')\n",
    "    plt.show()\n",
    "\n",
    "fish.loc[:,'predicted'] = zero_inflated_possion_model.predict(fish)\n",
    "\n",
    "_=rmse_mae(fish)\n",
    "pearson_residual_plot(fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b60e6-2ba4-4948-b470-d269200ad1fc",
   "metadata": {},
   "source": [
    "While far from ideal, these residuals look more like what we expect.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf9b05-f79d-4ac4-b04d-d1b247266cee",
   "metadata": {},
   "source": [
    "> **Exercise 25-1:** Examine the summary of the model and the residual plots and answer the following questions:      \n",
    "> 1. Has the model explained much of the variance of the data?      \n",
    "> 2. Is there evidence of the model being overfit, and if so what?      \n",
    "> 3. Is the zero inflation statistically significant and why?   \n",
    "> 4. Examine residual plots and the mean and median of the residuals. Keep in mind that if is impossible for Poisson regression to predict negative values. From these residuals, what can you say about the fit of this model and the skew of the residuals?           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f611-91f8-4e80-bcd1-6176384ebcaf",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.          \n",
    "> 2.                \n",
    "> 3.             \n",
    "> 4.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e6058-359b-4bb8-9df8-983c54286d8a",
   "metadata": {},
   "source": [
    "## Modeling with Over-Dispersion    \n",
    "\n",
    "From the results of the zero-inflated Poisson model, it is clear that that we should search for another model with a better fit. To address the over-dispersion of the response variable we will try a zero-inflated negative binomial distribution regression. The negative binomial distribution explicity models over-disperion, but adds one extra parameter that must be estimated. With a fairly small dataset, estiamting an extra parameter can prove problematic.         \n",
    "\n",
    "There are a number of models one could try at this point. You can see a sample of the possibilities of model formulas in the code cell below. Only a simple model is not commented out. Other models were either overfit, or more complex and with on better fit. Therefore, we will proceed with the simple model.          \n",
    "\n",
    "Execute the code in the cell below to fit the model and display the summary.  \n",
    "\n",
    "**Note:** Yoy may see a warning about inverting the Hessian. For this model it is safe to ignore this warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dfe75-8dd9-4b8d-8991-d31095dd8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formula = 'count ~ persons + child + camper + nofish'  es the \n",
    "#formula = 'count ~ persons + child + camper + livebait'  \n",
    "#formula = 'count ~ persons + child + camper'\n",
    "formula = 'count ~ -1 + persons + child + camper'   \n",
    "#formula = 'count ~ persons + child + nofish'   \n",
    "#formula = 'count ~ persons + np.sqrt(child) + camper'   \n",
    "#formula = 'count ~ persons + child + camper + persons:child'  \n",
    "#formula = 'count ~ persons + persons:child + persons:camper + nofish' \n",
    "#formula = 'count ~ persons + persons:child + nofish + camper' \n",
    "#formula = 'count ~ persons + persons:child + camper' \n",
    "#formula = 'count ~ -1 + persons:child + persons:camper' \n",
    "#formula = 'count ~ persons:child + camper + nofish' \n",
    "#formula = 'count ~ np.square(persons) + np.sqrt(child) + camper'   \n",
    "zero_inflated_negative_binomial_model = ZeroInflatedNegativeBinomialP.from_formula(formula=formula, data=fish).fit(maxiter=100, cov_type='HC1')\n",
    "zero_inflated_negative_binomial_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a214c97-50ca-4799-8fc3-9e055b8e2d05",
   "metadata": {},
   "source": [
    "> *Note:* There is likely to be a warning about a failure to invert the Hessian. For the purpose of our analysis here, this problem will likely not matter since we are not using the estimate of the Hessian for our interpretation of the model.\n",
    "\n",
    "We can interpret the inflation coefficient as follows.  \n",
    "\n",
    "$$\\pi = exp(-2.0) = .14$$\n",
    "\n",
    "As is expected from the properties of the distributions, a much lower inflation coefficeint is expected for the ZINB distribuiton with with over-dispersion parameter, $\\alpha=1.4$, compared to the ZIP distribution.   \n",
    "\n",
    "Next, the issue of model fit must be addressed. Execute the code in the cell below to display summary fit metrics and residual plots.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5346c55-3f71-4d98-89e3-41b62ea58468",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish.loc[:,'predicted'] = zero_inflated_negative_binomial_model.predict(fish)\n",
    "fish.loc[:,'resids'] = fish.loc[:,'count'].sub(fish.loc[:,'predicted'])\n",
    "fish['predicted']\n",
    "\n",
    "_=rmse_mae(fish)\n",
    "pearson_residual_plot(fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026e3a8-487a-4bd9-a68e-430f08fe6949",
   "metadata": {},
   "source": [
    "> **Exercise 25-2:** Examine the model summary, performance metrics and residual plots for the zero-inflated negative binomial regression and compare them to the zero-inflated Poisson regression and answer the following questions:\n",
    "> 1. Is the model over fit or not and why?\n",
    "> 2. Examine residual plots and the mean and median of the residuals. Keep in mind that if is impossible for negative binomial regression to predict negative values. From these residuals, what can you say about the fit of this model?  How do these residuals compare to the zero-inflated Poisson model?\n",
    "> 3. Does the model indicate there is statistically significant zero-inflation, and if so what is the evidence?        \n",
    "> 4. Does the model indicate there is statistically significant over-dispersion, and if so what is the evidence?    \n",
    "> 5. Compare the performance metrics btween the two models. What might the differences tell you about the differences of the two model fits if any? Given that the residuals have extreme values, do you think the MAE or RMSE is more reliable measure of model fit?\n",
    "> 6. Examine the model coefficients and answer these questions:\n",
    ">     - a. If you were planning to fish by yourself and another person joins you, how many additional fish on average would you expect to catch.\n",
    ">     - b. If you are fishing, how many fewer fish would you expect to catch if a child is added to your party.\n",
    ">     - c. If your party camps, how many more fish on average would your party expect to catch?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7791e2d-72c2-4123-be51-d4cbfb268879",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.              \n",
    "> 2.            \n",
    "> 3.           \n",
    "> 4.                  \n",
    "> 5.                \n",
    "> 6.            \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed0f15-96a9-419c-8e17-7047ba943450",
   "metadata": {},
   "source": [
    "## Bootstrap Resampling the Model\n",
    "\n",
    "A question we should ask is what is the distribution of the model parameters and how much can we rely on the estimates of the model parameters and the predictions made by the model. To answer this question, we can bootstrap resample the zero-inflated negative binomial regression model. The code in the cell below perform this bootstrap resampling of the model. Execute the code and examine the results.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca5c68-e4ff-4ba3-97fb-8908f102647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "def compute_CI(values, parameter, p=0.05):   \n",
    "    mean = np.mean(values)\n",
    "    p = 100 * p / 2.0\n",
    "    UCI = np.percentile(values, 100 - p)\n",
    "    LCI = np.percentile(values, p)\n",
    "    print('Parameter = ' + parameter)\n",
    "    print(f'Mean = {mean}')\n",
    "    print(f'Upper confidence interval = {UCI}')\n",
    "    print(f'Lower confidence interval = {LCI}')\n",
    "    return(mean, UCI, LCI)\n",
    "\n",
    "def plot_boot_params(params, parameter='intercept'):\n",
    "    mean, UCI, LCI = compute_CI(params, parameter)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ## Plot a histogram\n",
    "    ax[0].hist(params, bins=20)\n",
    "    ax[0].axvline(mean, color='red', linewidth=1)\n",
    "    ax[0].axvline(UCI, color='red', linewidth=1, linestyle='--')\n",
    "    ax[0].axvline(LCI, color='red', linewidth=1, linestyle='--')\n",
    "     \n",
    "    ax[0].set_title('Histogram of parameter\\n' + parameter)\n",
    "    ax[0].set_xlabel('Model parameter values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(params, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of parameter\\n' + parameter)\n",
    "    plt.show()\n",
    "\n",
    "def resample_regression_negative_binomial(df, n_boots, n_params=2, formula='y ~ x', simulate=False):\n",
    "    ## array to hold the bootstrap samples of the parameters\n",
    "    boot_samples = np.zeros((n_boots,n_params))\n",
    "    n_samples = df.shape[0]\n",
    "    ## Loop over the number of resamples\n",
    "    for i in range(n_boots):\n",
    "        ## Create a bootstrap sample of the data frame\n",
    "        boot_sample = df.sample(n=n_samples, replace=True)\n",
    "        ## Compute the OLS model\n",
    "        boot_model = ZeroInflatedNegativeBinomialP.from_formula(formula=formula, data=boot_sample).fit(maxiter=100, cov_type='HC1')\n",
    "        ## Save the model parameters in the array\n",
    "        boot_samples[i,:] = boot_model._results.params        \n",
    "        if simulate:\n",
    "            df['simulation' + str(i)] = boot_model.predict(df)\n",
    "    return boot_samples, df\n",
    "\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    poisson_param_boots, simulation = resample_regression_negative_binomial(fish, 2000, n_params=5, formula=formula)\n",
    "for i,parameter in enumerate(['inflate_const', 'persons','child', 'camper', 'alpha']):  \n",
    "    plot_boot_params(poisson_param_boots[:,i], parameter=parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b9f6b-d64e-4014-a34c-18cfb141ece8",
   "metadata": {},
   "source": [
    "> **Exercise 25-3:** Examine these results and answer the following questions:    \n",
    "> 1. Based on the confidence intervals of the bootstrap resampled values of the model parameters are all these parameters statistically significant?\n",
    "> 2. Examine the bootstrap distributions of the model parameters. Are they all approximately Normally distributed, and if not which ones and why?\n",
    "> 3. Consider the odd distribution of parameter(s) with non-Normal distribution. Keep in mind that the bootstrap resampling is done from a small set of observations that contain outliers of the response variable in this case. How can you explain the bootstrap distirbution of the parameter(s) and what is the possible impact on the interpretation of the parameter and the model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2cb80-f970-4c17-940c-a962fd1d4df8",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.              \n",
    "> 2.            \n",
    "> 3.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aeea25-4919-499d-b835-286f33977034",
   "metadata": {},
   "source": [
    "Another question we should ask, how variable are the predictions of the bootstrap realizations of the model and how well do these values agree with the responses of the observation. To find out execute the code in the cell below to creater 100 realizations of the model.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c077c1-807f-44e0-b4f5-38c610ad5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fish_simulation(df, x, c1, c2, n_simulations, g=None):     \n",
    "    if g==None:\n",
    "        g = sns.FacetGrid(df, col=c1, hue=c2, height=5, aspect=0.8);\n",
    "    for i in range(n_simulations):\n",
    "        g = g.map(sns.lineplot, x, 'simulation' + str(i), alpha=0.2, linewidth = 0.3)\n",
    "    for i, ax in enumerate(g.axes.flatten()):\n",
    "        ax.set_ylabel('Fish caught')\n",
    "        ax.set_title(str(i) + ' child')\n",
    "    g.add_legend()\n",
    "    return g\n",
    "\n",
    "\n",
    "n_simulations=50\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    poisson_param_boots, simulation = resample_regression_negative_binomial(fish, n_simulations, n_params=5, formula=formula, simulate=True)\n",
    "\n",
    "## Add the mean back to the persons column to improve interpretability  \n",
    "fish['persons'] = fish['persons'] + persons_mean\n",
    "\n",
    "## Plot the results\n",
    "g = plot_fish(fish, 'persons', 'child', 'camper', 'count')\n",
    "plot_fish_simulation(simulation, 'persons', 'child', 'camper', n_simulations, g=g); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa384e-1014-4088-9fbe-821c2dc593f7",
   "metadata": {},
   "source": [
    "> **Exercise 25-4:** Examine the range of outcomes for from the bootstrap realizations and answer the following questions:\n",
    "> 1. Describe at least three asapects of the bootstrap model responses thst are consistent with the observed responses.\n",
    "> 2. What does the dispersion of the responses of the bootstraped models tell you about the sensitivity of the model to the data?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db640a6-f457-4016-8897-2a95c94faf79",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "> 1.              \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99964090-6076-4fae-b125-88d4437e183c",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "We have just explored a situation where the distribution of the response variable is over-dispursed. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreme they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence, all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is **Cook's distance**, introduced by Dennis Cook in 1977. The influence for the ith data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{n (p+1)\\hat{\\sigma^2}}$$\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y}_{j} &= the\\ jth\\ prediction\\ computed\\ with\\ all\\ observations\\\\\n",
    "\\hat{Y}_{j(i)} &= the\\ jth\\ prediction\\ computed\\ without\\ the\\ ith\\ observation\\\\\n",
    "p &= number\\ of\\ parameters\\\\\n",
    "\\hat{\\sigma^2} &= empirical\\ variance\\ estimate\\ of\\ the\\ complete\\ set\\ of\\ residuals\\\\\n",
    "n &= number\\ of\\ data\\ points\n",
    "\\end{align}\n",
    "\n",
    "In effect, cooks distance compares the mean squared difference between predicted values with and without a given data point. This calculation is updated excluding each observation one at a time. Thus, Cook's distance is a resampling method. Therefore, computing Cook's distance can be computationally intensive for large datasets. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "A great many methods have been developed to analyze leverage and influence. The Statsmodels packages uses a version of Cook's distance that does not adjust for the number of samples and uses a different DoF correction:    \n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{p\\ \\hat{\\sigma^2}}$$\n",
    "\n",
    "Let's make these concepts concrete with some examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafa1c5-53fb-4915-85ed-77ab6bd6e883",
   "metadata": {},
   "source": [
    "### A synthetic example   \n",
    "\n",
    "Let's start with a simple example using synthetic data. For this example we will create a dataet, fit a regression model, the add an outlier to the ataset and compute a new linear model.     \n",
    "\n",
    "The code in the cell below does the following:      \n",
    "- Generates a synthetic dataset with a zero-centered independent variable.     \n",
    "- Defines and fits an OLS model to the data set and prints a summary.      \n",
    "- Adds a single outlier to the dataset.      \n",
    "- Computes a new OLS model with the dataset containing the outlier, and prints a summaruy.    \n",
    "- Plots the dataset along with the predictions from the models, with and without the outlier.    \n",
    "\n",
    "Execute this code and examine the result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885fe49-9bb1-47e8-ab0d-798b3e8e761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = -5.0, 5.0\n",
    "y_start, y_end = 0.0, 10.0\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "np.random.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in a dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "# OLS model and prediciton with no outlier\n",
    "ols_model_no_outlier = smf.ols(formula = 'y ~ x', data=sim_data).fit()\n",
    "print('OLS model with no outlier')\n",
    "print(ols_model_no_outlier.summary())\n",
    "\n",
    "\n",
    "## Add single outlier to dataframe\n",
    "outliers = pd.DataFrame({'x':[-5.0], 'y':[15], 'predicted':[0.0], 'resids':[0.0]})\n",
    "sim_data_outlier = pd.concat([outliers,sim_data]).reset_index().drop('index', axis=1)\n",
    "\n",
    "## Compute OLS model\n",
    "\n",
    "## Define the regresson model, fit it to the data and predict \n",
    "sim_data_outlier['x'] = np.subtract(sim_data_outlier.loc[:,'x'], np.mean(sim_data_outlier.loc[:,'x']))\n",
    "ols_model = smf.ols(formula = 'y ~ x', data=sim_data_outlier).fit()\n",
    "sim_data_outlier['predicted_outlier'] = ols_model.predict(sim_data_outlier.x)\n",
    "sim_data_outlier['predicted_no_outlier'] = ols_model_no_outlier.predict(sim_data.x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax = sns.scatterplot(x='x', y='y', data=sim_data_outlier, ax=ax)\n",
    "ax = sns.lineplot(x='x', y='predicted_outlier', data=sim_data_outlier, color='red', label='With outlier')\n",
    "ax = sns.lineplot(x='x', y='predicted_no_outlier', data=sim_data_outlier, color='blue',  linestyle = 'dotted', label='No outlier')\n",
    "_=ax.set_title('Observations with outlier');\n",
    "plt.legend();\n",
    "print('\\n\\n\\nOLS model with outlier')\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf5c18-c617-4a41-93e5-031c10ec4183",
   "metadata": {},
   "source": [
    "Notice the following about the results of the two models.       \n",
    "1. Notice that the intercept of the two models are quite similar. However, there is a noticeable change slope coefficient. However, the change in slope is just within the confidence intervals cannot be considered significant. Still, the outlier has introduced a bias in the model results.        \n",
    "2. The OLS regression line computed without the outlier shows a fit to the bulk of the data. Notice how the slope of the regression line changes because of the outlier, and does not seem to represent the buld of the data well.\n",
    "3. These change in the models is an example of effect of a **high leverage outlier**.    \n",
    "\n",
    "You can create an **influence plot**, which shows the influence and leverage of the observations vs. the standardized residuals. The [statsmodels.graphics.regressionplots.influence_plot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.influence_plot.html) displays Cook's distance as a measure of influence. Execute the code in the cell below to display the influence plot for the OLS model of the data with the outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4daff-a319-4bd9-afad-f069cc59b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "_=influence_plot(ols_model, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5eecfb-5054-49c3-b626-cdd15b6eca3e",
   "metadata": {},
   "source": [
    "The resulting plot shows variance standardized residuals on the vertical axis vs. Cook's distance leverage on the horizontal axis. The size of the marker is indicates the influence of the observation on the model. There is one observation with a large residual, the outlier or point 0, which has significant influence on the regression model. This observation has a large standardized residual and significant leverage. Other observations have high leverage, but small residuals. \n",
    "\n",
    "Stats models also provides the [statsmodels.graphics.regressionplots.plot_regress_exog](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.plot_regress_exog.html) plotting method which displays a series of **partial regression plots**. A partial regression plot displays various measures of the regression model fit vs. a selected exogenous variable or feature. Execute the code in the cell below to see an example of these plots and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9d3b5-fd08-46c4-8c09-41d712af18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "_=plot_regress_exog(ols_model, 'x', fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a3def-b267-4ad6-9e0b-e8a83321b3aa",
   "metadata": {},
   "source": [
    "These plots provide several views of the fit of the model to the observations.     \n",
    "1, A plot of the fitted values vs the observations.     \n",
    "2. A residual plot, clearly shoing the outlier.     \n",
    "2. Plots shoing the regression line and observations and the residuals with fitted values on the CCPR plot.       \n",
    "\n",
    "Next, execute the code in the cell below to display the characteristics of the distribution of the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f94d1-c55e-4829-8bc5-76142e903c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_outlier['resids'] = np.subtract(sim_data_outlier.predicted_outlier, sim_data_outlier.y)\n",
    "\n",
    "## Your code below\n",
    "print('The mean of the residuals = {0:4.3f}  RMSE = {1:4.3f}'.format(np.mean(sim_data_outlier.resids), np.std(sim_data_outlier.resids)))\n",
    "\n",
    "plot_resid_dist(sim_data_outlier.resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bb3c7-bd11-47ef-8329-1988f2413f74",
   "metadata": {},
   "source": [
    "WIth the exeption of the outlier, these residuals are close to Normally distributed. The outlier clearly has an effect on the residuals.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf43bbd-d5bc-4d93-a4e4-7dac0ddf3d01",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "\n",
    "Let's try another example with a small real dataset. Stack loss is an undesireable effect in an industrial chemical process. Understanding the dependence of stack loss on the other variables could help improve the chemical process.  \n",
    "\n",
    "To start, execute the code in the cell below to load the dataset and display the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39505b3-44e0-4edf-ae73-6e1d3618e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "StackLoss = sm.datasets.stackloss.load_pandas().data    #sm.datasets.get_rdataset(\"stackloss\")\n",
    "StackLoss.columns = ['Stackloss','Airflow','Watertemp','Acidconc']\n",
    "\n",
    "for col in ['Airflow','Watertemp','Acidconc']:\n",
    "    StackLoss.loc[:,col] = StackLoss.loc[:,col] - StackLoss.loc[:,col].mean()\n",
    "\n",
    "StackLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24775c-e15c-4c17-be96-22647fcd683f",
   "metadata": {},
   "source": [
    "The dependent or response variable is `Stackloss`. There are three independent varaibles `Airflow`, `Watertemp`, and `Acidconc'.   \n",
    "\n",
    "To get a better feeling for this dataset, execute the code in the cell below to display a scatter plot matrix of the observations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70247d5-4b92-4a9b-90d2-7be2bdb17bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(StackLoss, height = 1.5, plot_kws={\"s\": 15, 'alpha':0.3});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183ff2-334f-4296-9623-6d114f5b27e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417470cb-b813-4155-bdc4-a08ca83a1444",
   "metadata": {},
   "source": [
    "> **Exercise 25-5:** We can try a more complex model for the price of the autos by introducing an additional explanatory variable. Additionally, we introduce an interaction term between the two explanatory variables. The interaction term being the product of the two variables. To perform this analysis do the following steps:     \n",
    "> 1. Define a model named `stackloss_OLS` with model formula `Stackloss ~ Airflow + Watertemp + Acidconc`.   \n",
    "> 2.Print the summary of the model\n",
    "> 3. Compute the predicted values and store them in the `predicted` column of the data frame.   \n",
    "> 4. Compute the residuals and store them in the `resids` column of the data frame.   \n",
    "> 5. Print the mean and the RMSE of the residuals.    \n",
    "> 6. Display the distribution plots of the residuals.   \n",
    "> 7. Display a plot of residuals vs. predicted values.   \n",
    "> 8. Finally, display the influence plot for the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5f033-a207-48e5-8d28-6871e77a3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Mean of the residuals = ' + str(round(np.mean(StackLoss['resids']), 3)))\n",
    "print('RMSE of the residuals = ' + str(round(np.std(StackLoss['resids']), 3)))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "influence_plot(stackloss_OLS, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07802ef-6ba6-4274-a010-b3cf8a2c3ccf",
   "metadata": {},
   "source": [
    "> 8. Execute the code in the cell below that creates a scatterplot matrix highlighting the high influence and high leverage data points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa749f9d-db71-4275-bf5b-3f6f0bca2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "StackLoss['Influence'] = [0]*len(StackLoss)\n",
    "StackLoss.loc[[16,20],'Influence'] = 1\n",
    "\n",
    "sns.pairplot(StackLoss.loc[:,['Stackloss','Airflow','Watertemp','Acidconc','Influence']], hue='Influence', height = 1.5, plot_kws={\"s\": 15});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9167aa-153a-4793-9986-a976b20bc449",
   "metadata": {},
   "source": [
    "> Now answer these questions:    \n",
    "> 1. Examine the summary of the coefficient values. Are all the model coefficient significant and how can you tell?        \n",
    "> 2. Are the Residuals approximately Normal and homoscendasitc?           \n",
    "> 3. Examine the leverage and residuals on the influence plot. Are there any high-leverage residuals shown on this plot?\n",
    "> 4. Examine the scatter plot matrix showing the high influence and high leverage points along with the other observations. Does at least one of these points stand out as an outlier, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe64a34-eae5-4da7-9556-b7e80cefbe96",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.        \n",
    "> 2.         \n",
    "> 3.           \n",
    "> 4.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa83745-2497-445d-be9a-02e6f98579d4",
   "metadata": {},
   "source": [
    "## Robust Regression Algorithms  \n",
    "\n",
    "So far we have only been working with linear ordinary least squares (OLS). The response of a linear model to an observation value is linear, by definition. Consequently, there will be a large proportionate response to any outlier. This effect can strongly influence the training of linear models, as you have seen from the OLS model previously computed and evaluated. This effect is defined by the **influence function**.         \n",
    "\n",
    "**Robust regression** models limit the influence of outliers when training a model. There have been many types of robust regression models that been developed over many years. These models are based on different types of influence functions.    \n",
    "\n",
    "There are trade-offs with using robust regression. OLS models are generally **unbiased minimum variance estimators**. However, by limiting the influence function a robust model is no longer unbiased or minimum variance. In technical terms we say that the robust models are not **efficient**. \n",
    "\n",
    "Given the aforementioned trade-off, one seeks to design robust estimators with the following properties:   \n",
    "1. The estimator can be linear, and therefore unbiased, near the expected value\n",
    "2. Limit the influence of outliers by using a nonlinear influence function beyond the linear region.   \n",
    "3. The linear response near the expected value allows use of MLE methods. \n",
    "\n",
    "The median estimator is a well-known to be robust, but is not idea since it does not have all of these properties. This situation can be understood by examining the figure below.   \n",
    "\n",
    "<img src=\"../images/MeanMedianInfluence.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence functions of linear and median estimators: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "Comparing these influence functions we can see: \n",
    "1. The influence function of the linear model is labeled T=mean. This function is linear and unbounded, meaning that the influence of an extreme outlier is also unbounded. The linear behavior and continuous derivatives of the influence function mean that this estimator is both an efficient and minimum variance MLE.   \n",
    "2. The median estimator is far from linear and has discontinuous derivatives about the estimated value. These properties lead to high variance from median estimates.  \n",
    "\n",
    "For a state of the art discussion of robust estimators see [Maronna, et.al., Robust Statistics, Theory and Methods (with R), Second Edition, 2019, Wiley](https://www.wiley.com/en-us/Robust+Statistics:+Theory+and+Methods+(with+R),+2nd+Edition-p-9781119214687).  \n",
    "\n",
    "### Alpha-trimming       \n",
    "\n",
    "A simple robust estimator is called **alpha trimming**. The idea is to simply delete the observations with the largest positive and negative residuals. OLS regression is trained using the remaining values.       \n",
    "\n",
    "The fraction of the observations removed in this manner is known as $\\alpha$. Typically, $\\alpha/2$ of the positive and $\\alpha/2$ of the negative residuals are deleted. Typical value of $\\alpha$ are 10%, 20% or 30%, but most any value less than 50% can be used.     \n",
    "\n",
    "The larger the value of $\\alpha$ the more robust the estimator is. We say that the larger value of $\\alpha$ gives the estimator a higher **breakdown point**. The breakdown point is equal to  $\\alpha$ for this estimator.        \n",
    "\n",
    "Examples of influence functions for alpha trimmed estimators are shown in the figure below.  \n",
    "\n",
    "<img src=\"../images/AlphaTrimming.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence functions of linear and alpha-trimmed estimators: Credit, Hampel, et.al. Robust Statistics, 1986</center>    \n",
    "    \n",
    "Examine the figure above and notice the following.     \n",
    "1. As stated before, the influence function of the linear model, labeled $\\bar{X}$, is linear and unbounded.   \n",
    "2. The influence function of alpha-trimmed estimators are locally linear, but then bounded. This bounding limits the influence of outliers to a constant. The slopes of the linear portion of the alpha-trimmed estimators are not the same as the linear estimator. The combination of the increased slope and bounding of the influence function results in bias and the estimator no longer being minimum variance. Estimators with higher values of alpha have more bias.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b0088-8f22-4235-84b0-02135ee23a87",
   "metadata": {},
   "source": [
    "### Huber estimators    \n",
    "\n",
    "Peter Huber (1964, 1984) proposed an estimator that possess the minimum variance properties near the expected value but limits the influence of outliers to a constant. In contrast to alpha-trimming, the Huber estimator does not discard observations, but rather places a hard limit on their influence. The influence function of the Huber estimator can be seen in the figure below.     \n",
    "\n",
    "<img src=\"../images/Huber.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence function of Huber estimator: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "Examining the above figure, we can see some key properties of the Huber estimator:   \n",
    "1. The influence function is linear near the mean but constant away from the mean.   \n",
    "   - The **hinge point** is at $\\pm t * MAD$, where $MAD =$ **median absolute deviation**.    \n",
    "   - Robustness and bias increases as $t$ decreases. \n",
    "2. Huber estimator is low bias.      \n",
    "   - The estimator is unbiased for samples near the point estimate.   \n",
    "   - There is constant influence away from the point estimate, for outliers beyond the hinge points, $\\pm t$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744797e2-d574-4ff7-8d51-8270754da52e",
   "metadata": {},
   "source": [
    "### M-estimators   \n",
    "\n",
    "A large family of robust estimators are known as **M-estimators**. M-estimators attempt to improve on the properties of the Huber estimator by slowly decreasing the influence of extreme outliers toward 0. Whereas a Huber estimator has constant influence beyond the hinge point, an M-estimator will down-weight extreme outliers. Quite a number of M-estimators have been proposed.    \n",
    "\n",
    "All M-estimators require somewhat complicated selection of hyperparameter values. The Huber estimator has a single hyperparameter, the hinge point. M-estimators often have two or more hyperparameters.   \n",
    "\n",
    "A fairly simple example of an M-estimator is the Tukey biweight estimator (Tukey 1981). The influence function of the biweight estimator can be seen in the figure below.   \n",
    "\n",
    "<img src=\"../images/TukeysBiweight.png\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n",
    "<center>Influence function of the Tukey biweight estimator: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "\n",
    "\n",
    "The biweight estimator uses a smooth influence function that is locally linear around the point estimate, eventually tapering to 0. The point at which the biweight becomes zero is the single hyperparameter, $r$. Both Robustness and bias increase with decreasing $r$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8e277-25c7-4114-9d0e-4281f080b4f3",
   "metadata": {},
   "source": [
    "> **Exercise 25-6:** You will now create and evaluate a robust regression model for the `sim_data_outlier` dataset by the following steps:       \n",
    "> 1. Define a robust regression model using [statsmodels.formula.api.rlm](https://www.statsmodels.org/dev/generated/statsmodels.formula.api.rlm.html) using the formula `\"y ~ x\"`,with the argument `M=sm.robust.norms.HuberT()` and name your model `ols_model_huber`.  \n",
    "> 2. Print the summary of the model.     \n",
    "> 3. Compute the predictions and save them in a `predicted_huber` column of the data frame.    \n",
    "> 4. Compute the residuals and save them in a `residuals_huber` column of the data frame.   \n",
    "> 5. Display the distribution plots of the residuals.        \n",
    "> 6. Plot the residuals vs. the predicted values. \n",
    "> 7. Display a plot with the following:    \n",
    ">     - A scatter plot of the x-y values of the `sim_data_outlier` dataset.     \n",
    ">     - A line showing the predicted values from the robust regression.    \n",
    ">     - A line, using a different line type and color, showing the predicted values from the OLS model, computed with the outlier in the introduction to the synthetic dataset.    \n",
    ">     - Include a legend on your plot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985f374-e06f-4324-b387-09cc9b4b6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_resid_dist(sim_data_outlier.loc[:,'residuals_huber'])\n",
    "residual_plot(sim_data_outlier, predicted='predicted_huber', resids='residuals_huber')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(sim_data_outlier.x, sim_data_outlier.predicted_huber, color='red', label='Huber')\n",
    "ax = sns.lineplot(x='x', y='predicted_outlier', data=sim_data_outlier, color='blue', linestyle = 'dashed', label='OLS')\n",
    "ax = sns.lineplot(x='x', y='predicted_no_outlier', data=sim_data_outlier, color='orange', linestyle = 'dotted', label='OLS no outlier')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_outlier, ax=ax)\n",
    "ax.set_title('Scatterplot showing three regression lines')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ded8f-4f20-49f0-8e79-89e78a65b0c3",
   "metadata": {},
   "source": [
    "> Answer the following questions:      \n",
    "> 1. Are there significant differences between the coefficients of the Huber and OLS model and which ones are closer to the OLS values computed wiothout the outlier?          \n",
    "> 2. Notice the CIs for the Huber and OLS model computed with the outlier. What systematic difference can you see and what does this tell you about the influence of the outlier?\n",
    "> 3. Are the residuals close to Normally distributed and homoscedastic, except the outlier?         \n",
    "> 4. From the scatter plot, does in appear that the Huber estimator model fits the bulk of the data better?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5bf16-0a70-468f-bb5a-df5c0d194f5f",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.          \n",
    "> 2.            \n",
    "> 3.          \n",
    "> 4.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028205c8-55d1-4667-9a2d-6c6071fb2ed8",
   "metadata": {},
   "source": [
    "### Another example\n",
    "\n",
    "We will return to the stack loss data and apply a robust model. Comparing the robust model to the OLS model can provide some insioght on the effect of limiting the influence of outliers.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e577489-7f5d-421f-ad59-79ff6f23cc6c",
   "metadata": {},
   "source": [
    "> **Exercise 25-7:** You will now create and evaluate a robust regression model for the `Stackloss` dataset by the following steps:       \n",
    "> 1. Define a robust regression model using [statsmodels.formula.api.rlm](https://www.statsmodels.org/dev/generated/statsmodels.formula.api.rlm.html) using the formula `\"Stackloss ~ Airflow + Watertemp + Acidconc\"`,with the argument `M=sm.robust.norms.HuberT()` and name your model `stackloss_huber`.  \n",
    "> 2. Print the summary of the model.     \n",
    "> 3. Compute the predictions and save them in a `predicted_huber` column of the data frame.    \n",
    "> 4. Compute the residuals and save them in a `residuals_huber` column of the data frame.   \n",
    "> 5. Display the distribution plots of the residuals.        \n",
    "> 6. Plot the residuals vs. the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042de01f-beb3-4d8a-b93a-afec9796d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_resid_dist(StackLoss.loc[:,'residuals_huber'])\n",
    "residual_plot(StackLoss, predicted='predicted_huber', resids='residuals_huber')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb616deb-a679-42c3-9ad1-25a6e73680c0",
   "metadata": {},
   "source": [
    "> Examine the results and compare them to the results from the OLS model and answer the following questions:\n",
    "> 1. Compare the model coefficients computed for the Huber and OLS models. Are there noticeable differences in the coefficeint values, and are the differences statisticallty significant given the confidence intervals?     \n",
    "> 2. Compare the width of the confidence intervals between the Huber and OLS models. What difference can you identify and how can you explain them?\n",
    "> 3. What differnce in the distribution of the residuals can you see between the Huber and OLS models?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90a74a-48a7-4c30-960e-e6027e48b48a",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.           \n",
    "> 2.            \n",
    "> 3.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3629659-fd60-488b-9477-7604ba405951",
   "metadata": {},
   "source": [
    "As a final step, we can display the predictions for both models along with the observations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28395659-97b8-4ca5-994c-4f1945c55bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.5))\n",
    "for col, ax_i in zip(['Airflow','Watertemp', 'Acidconc'], ax):\n",
    "    sns.scatterplot(data=StackLoss, x=col, y='Stackloss', ax=ax_i, color='black', alpha=0.3, s=25, marker=\"$\\circ$\", label='Stackloss')\n",
    "    sns.scatterplot(data=StackLoss, x=col, y='predicted', ax=ax_i, color='orange', alpha=0.5, s=15, label='OLS')\n",
    "    sns.scatterplot(data=StackLoss, x=col, y='predicted_huber', ax=ax_i, color='green', alpha=0.3, marker=\"$\\circ$\", s=15, label='Huber')\n",
    "    ax_i.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86900827-64bb-4765-a360-1acb15d4c776",
   "metadata": {},
   "source": [
    "Notice that many of the predictions for both models are close to the observed values in the Stackloss dataset. There does seem to be a slight increase in the dispersion of the predictions for the OLS model with respect to the observatons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208563e8-15b6-46fe-9ea9-31d51a56b471",
   "metadata": {},
   "source": [
    "#### Copyright 2022, 2023, 2024, Stephen F Elston. All right reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceee29c-55f3-4195-8a1d-7a3ca259df33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
