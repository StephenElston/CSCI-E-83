{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19    \n",
    "# Linear models; the workhorse of statistics\n",
    " \n",
    "\n",
    "## Introduction\n",
    "\n",
    "The concept of the linear model is the basis of many statistical and machine learning models. Further, an understanding of linear models is a good basis for understanding many other types of statistical and machine learning models.   \n",
    "\n",
    "In this chapter we will focus on regression models, but the lessons drawn from this discussion can be applied to many other types of models. By developing an understanding of linear regression, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.     \n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.     \n",
    "\n",
    "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.   \n",
    "\n",
    "Regression models attempt to predict the value of one **dependent variable** using the information from other **independent variables**. Unfortunately, the terminology used for these variables is not consistent across authors, statistical software packages, and application domains. The table below list some, but by no means all of the terms used for these variables. \n",
    "\n",
    "### Confusing terminology \n",
    "\n",
    "Given that linear models have been developed in many areas for a long period of times, different terminology has developed for the same things. For people trying to learn the subject this differing terminology is confusing and seemingly conflicting.    \n",
    "\n",
    "The main division in terminology arises from different communities within statistics and machine learning. The table below shows some of the different terms commonly used in the two lineages:       \n",
    "\n",
    "| Machine Learning Terminology | Statistical Terminology          |\n",
    "|:---------------------------|:------------------------------|\n",
    "| Regression vs classification   | Continuous numeric vs categorical response      |\n",
    "| Learning algorithm or model    | Model                                |\n",
    "| Features                       | Predictor, explanatory, exogenous, or independent variables   |\n",
    "| Training                       | Fitting                              |\n",
    "| Trained model                  | Fitted model                         |\n",
    "| Supervised learning            | Predictive modeling      \n",
    "\n",
    "For the specific case of regression there are further differences in terminology. These arise not just between the statistical and machine learning communities. One difference in terminology is the naming of the variables used in regression and other machine learning models. The table below outlines some of these differences:          \n",
    "\n",
    "Predicted Variable | Variables Used to Predict    \n",
    ":----------------------- | :------------------------------     \n",
    " y | x   \n",
    " Dependent | Independent    \n",
    " Endogenous | Exogenous    \n",
    " Response | Predictor    \n",
    " Response | Explanatory    \n",
    " Label | Feature    \n",
    " Regressand | Regressors    \n",
    " Outcome | Design   \n",
    " Left Hand Side | Right Hand Side     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The idea of averaging errors have been applied for nearly three centuries. The first known publication of a *method of averages* was by the German astronomer Tobias Mayer in 1750. Lapace used a similar method which he published in 1788.\n",
    "\n",
    "<img src=\"../images/TobiasMayer.jpg\" alt=\"TobiasMayer\" style=\"width: 200px;\"/>\n",
    "<center>Credit wikipedia commons</center>\n",
    "\n",
    "The first publication of the **method or least squares** was by the French mathematician Adrien-Marie Legendre in 1805. Legendre was a brilliant mathematician, known for his unpleasant personality.  \n",
    "\n",
    "![](../images/Legendre.jpg)\n",
    "<center>Caricature of Legendre, published method of least squares: credit Wikipedia commons</center>\n",
    "\n",
    "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "![](../images/Carl_Friedrich_Gauss.jpg)\n",
    "<center>Carl Friedrich Gauss, early adopter of the least squares method: credit Wikipedia commons</center>\n",
    "\n",
    "The first use of the term **regression** was by Francis Gaulton, a cousin of Charles Darwin, in 1886. Gaulton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. Gaulton used the term **regression to the mean** to describe the natural processes he observed in inherited traits.  \n",
    "\n",
    "<img src=\"../images/Francis_Galton.jpg\" alt=\"Drawing\" style=\"width:225px; height:250px\"/>\n",
    "<center>Francis Galton, inventor of regression: credit Wikipedia commons</center>\n",
    "\n",
    "While Gaulton invented a form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1886 publication proposed a method of regression as we understand it today. \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times, the interest in machine learning has lead to a rapid increase in the variety of regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theory of Linear Regression\n",
    "\n",
    "We will focus on the theory of **linear models**, which are foundational. Key properties of linear models include:\n",
    "- Derived with linear algebra.\n",
    "- Include any model **linear in coefficients**, including polynomials, splines, Gaussian kernels and many other nonlinear functions.    \n",
    "- Understanding linear models is basis for understanding behavior of many other statistical or machine learning models.\n",
    "- Linear models are the basis of many time series and survival models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model of a strait line\n",
    "\n",
    "Let's have a look at the simple case of a regression model for a straight line. For this example we will work with single regression with one feature and one label. The data are in the form of some number of values pairs, $\\{x_i,y_i \\}$. \n",
    "\n",
    "The goal of this regression model is to find a straight line that best fits the observed data. We can define the line by two coefficients or **parameters**, the **slope** and the **intercept**. A general representation of this parameterization of a straight line is illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model: credit wikipedia commons**</center>\n",
    "\n",
    "Where,  \n",
    "\n",
    "\\begin{align}\n",
    "m &= slope = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y &= b\\ at\\ x = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For each of the pairs of observed values, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "\\begin{align}\n",
    "y_i &= mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i &= error\n",
    "\\end{align}\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"../images/LSRegression.jpg\" alt=\"LSRegression\" style=\"width: 450px;\"/>\n",
    "<center>Example of least squares regression with errors shown as vertical lines: credit wikipedia commons</center>\n",
    "\n",
    "Notice that these errors are only along the y-axis. In other words, the **linear regression model accounts for errors in the predicted value**, not errors in the values of the independent values. We want to solve for $m$ and $b$ by minimizing these errors, $\\sum_i \\epsilon_i$. This leads us to the  **least squares regression** problem.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "One can see the equivalence to the Normal log-likelihood:   \n",
    "\n",
    "$$l(\\mathbf{X}\\ |\\ \\mu, \\sigma ) = - \\frac{n}{2} log( 2 \\pi \\sigma^2 ) - \\frac{1}{2 \\sigma^2} \\sum_{j=1}^n (x_j - \\mu)^2$$\n",
    "\n",
    "For a fixed variance, $\\sigma^2$, the log-likelihood is maximized when the least square error condition in met. This observation has two important implications:   \n",
    "1. The error, or **residuals**, arising from a least squares model are expected to be Normally distributed.   \n",
    "2. There are computationally efficient algorithms for finding minimums of equations. In a previous chapter we investigated the stochastic gradient descent (SGD) algorithm for maximum likelihood estimation (MLE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression assumptions\n",
    "\n",
    "Before going any further, we should discuss a few key assumptions of linear regression, also known as **ordianary least squares (OLS)**. Keep these points in mind whenever you use a regression model. \n",
    "\n",
    "1. There is a **linear relationship** between dependent variable and the **coefficients** of the independent variables. This does not mean the function approximation used must be linear. Only that the model must be linear in the coefficients. \n",
    "2. Measurement error is independent and random. Technically, we say that the error is **independent identical distributed, or iid**.\n",
    "3. Errors arise from the dependent variable only. Other models, such as complete regression, must be used if there are errors in the independent variable. \n",
    "The diagram below illustrates the iid errors for the dependent variable only.\n",
    "\n",
    "![](../images/IndependentErrors.jpg)\n",
    "<center>Credit wikipedia commons</center>\n",
    "\n",
    "4. There is no **multicolinearity** between the features or independent variables. In other words, there is no significant correlation between the features.\n",
    "5. The **residuals** are independent identically distributed (iid) Normal and **homoscedastic** (constant variance).  In other words, the errors are the same across all values of the independent variables. We will explore this fundamental property further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Regression Model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "%matplotlib inline\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by executing the code in the cell below. Notice that the points nearly fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('x vs y')\n",
    "ax.set_ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering explanatory variable\n",
    "\n",
    "We want to be in a position to interpret the results of the linear model. The problem is the intercept term. By definition, the intercept is the crossing point of the regression line on the y-axis (dependent) variable. The independent variables must all have values of 0, at the intercept. However, this intercept value is often quite arbitrary and meaningless in terms of the values of the independent (predictor) variables. In fact, the independent variables may not reasonably ever have values of 0.  As an example, the intercept can have impossible values, such as a negative life expectancy.     \n",
    "\n",
    "The solution is to **center** the independent variables. Centering these variables transforms the intercept term to the **mean of the response variable**. This practice eliminates the aforementioned problems by ensuring the intercept point is in a reasonable range of the independent variables.       \n",
    "\n",
    "> ***Note:** It is standard practice to transform independent variables to be both zero mean (centered) and unit variance. This treatment can be essential if the scale of multiple independent variables is quite different. If variance standardization is not applied in such cases, variables with a large range of numeric values can dominate the model training. Rather, we want variables with the best explanatory power to dominate model training regardless of the range of values. For single (independent variable) regression, scalling is not necessary. See the section below of scaling data for more detail. \n",
    "\n",
    "Execute the code in the cell below to compute a centered independent variable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data['x_centered'] = np.subtract(sim_data.loc[:,'x'], np.mean(sim_data.loc[:,'x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fitting a Linear Regression Model\n",
    "\n",
    "Now, you are ready to build and evaluate a regression model using Python. There are a number of Python libraries that contain linear modeling capabilities.\n",
    "\n",
    "The [scikit-learn](https://scikit-learn.org/stable/) package has many different types of machine learning algorithms. Scikit-lean model interfaces take a machine learning perspective. The data arguments for Scikit-learn models are numpy arrays which must be dimensioned properly.\n",
    "\n",
    "[Statsmodels](https://www.statsmodels.org/stable/index.html) is another Python package with extensive linear model capability. This package takes a statistical perspective, which we adopt here. A nice feature of statsmodels is that the data argument is a Pandas data frame. \n",
    "\n",
    "You can specify statsmodels models using the [R-style model language](https://www.statsmodels.org/devel/example_formulas.html). If you are not familiar with the R model language interface, read the summary below before proceeding. For those that have experience with the R programming language, statsmodels will seem familiar since it provides a R-like model language interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **R-Style Model Formulas**    \n",
    "> The code in the cell below uses an R style model formula. This modeling language was introduced in [Chambers and Hastie, 1992, Statistical Models in S](https://www.taylorfrancis.com/books/e/9780203738535).     \n",
    ">\n",
    "> In statsmodels there is a specific implementation of the R formula language, documented [here](https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html). For a good [**cheatsheet and summary of the R modeling language**](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) look at the posting by Richard Hahn of the Chicago Booth School.    \n",
    ">\n",
    "> Models are defined by an equation using the $\\sim$ symbol, meaning *modeled by*. The variable to be modeled is always on the left. The relationship between the independent variables shown on the right. This basic scheme can be written: \n",
    "$$dependent\\ variable\\sim indepenent\\ variables$$\n",
    "> For example, if the dependent variable (dv) is modeled by two independent variables (var1 and var2), with no interaction, the formula is:\n",
    "$$dv \\sim var1 + var2$$\n",
    "> - Example; dependent variable (dv) is modeled by independent variables (var1) and its square. The $I()$ operator is used to wrap a function of a variable:\n",
    "$$dv \\sim var1 + I(var1**2)$$\n",
    "> - Example; dependent variable (dv) is modeled by two independent variables (var1 and var2) and the **interaction term** between them:\n",
    "$$dv \\sim  var1*var2$$\n",
    "> The expansion of this notation is: \n",
    "\\begin{align}\n",
    "var1:var &= 1 + var1 + var 2 + var1:var2\\\\\n",
    "var1*var &= intercept + sum\\ of\\ variables + interaction\\ of\\ variables\n",
    "\\end{align}\n",
    "> - Example; dependent variable (dv) modeled by independent numeric variable (var1) and a categorical variable (var2) using the $C()$ operator to encode the levels of the categorical variable:\n",
    "$$dv \\sim var1 + C(var2)$$\n",
    "> - Example of using $-$ operation to drop terms in from the model. In this case both intercept is dropped by the $-1$ term and the independent $var2$ term is dropped:    \n",
    "\\begin{align}\n",
    "dv &\\sim  -1 - var2 + var1*var2\\\\\n",
    "dv &\\sim no\\ intercept + var1 + interaction\\ of\\ variables\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, there is only one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- The model formula is specified as $y \\sim x$.\n",
    "- An [statsmodels.formula.api.ols (ordinary least squares)](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html?highlight=statsmodels%20formula%20ols#statsmodels.formula.api.ols) model object is specified using the model formula and the data frame. Here, we use the lower case *ols* function so that the model language can be specified in the call. \n",
    "- The *fit* method is applied to the ols object. \n",
    "- The slope and intercept point estimates are printed. \n",
    "\n",
    "Execute this code and note the coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = smf.ols(formula = 'y ~ x_centered', data=sim_data).fit()\n",
    "\n",
    "## Print the model coefficient\n",
    "print('Intercept = {0:4.3f}  Slope = {1:4.3f}'.format(ols_model._results.params[0], ols_model._results.params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept and slope are close to the actual values of 1.0 and 1.0. However, we need a more thorough examination of the results before we can say this is a good model for these data.  \n",
    "\n",
    "As a first step toward evaluating this model, we can compute the predicted values of y given the values of x. Execute the code in the cell below which uses the *predict* method to compute these predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = ols_model.predict(sim_data.x_centered)\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single regression model, we can plot the values of the predicted line along with the actual data values on a 2-dimensional plot. For models with multiple features, [partial regression plots](https://en.wikipedia.org/wiki/Partial_regression_plot) can be created. \n",
    "\n",
    "Execute the code in the cell below to create the plot and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.lineplot(x='x_centered', y='predicted', data=sim_data, color='red')\n",
    "sns.scatterplot(x='x_centered', y='y', data=sim_data, ax=ax)\n",
    "ax.set_title('Observed and predicted values vs. centered x indpendent variable')\n",
    "_=ax.set_ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The predicted regression line does seem to fit the data well. But, how can we quantify the performance of this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model Parameters    \n",
    "\n",
    "How can we interpret the model parameters? First, recall that the model is linear and constructed using a zero-centered independent variable. The result is both simple and intuitive:    \n",
    "- The intercept of about 6.0 is the **mean** of the response variable y.    \n",
    "- The slope coefficient of 0.94 indicates that the response variable increases by 0.94 for each unit of increase of the independent variable.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. There is no one metric that can be used to evaluate a linear model, or any other type of machine learning model. Here as in any other case, we will in fact use multiple metrics to evaluate the linear regression model. \n",
    "\n",
    "The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"../images/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "<center>Measuring errors for a regression model: credit, Wikipedia commons</center>  \n",
    "    \n",
    "    \n",
    "Let's start with the observed values of the feature, $X$, and label, $Y$.      \n",
    "\n",
    "\\begin{align}\n",
    "X &= [x_1, x_2, \\ldots, x_n]\\\\\n",
    "Y &= [y_1, y_2, \\ldots, y_n]\\\\\n",
    "where\\\\\n",
    "x_i &= ith\\ feature\\ value\\\\\n",
    "y_i &= ith\\ label\\ value\\\\\n",
    "\\end{align}\n",
    "\n",
    "The results of the regression model are **estimates** which we write:   \n",
    "\n",
    "\\begin{align}\n",
    "\\bar{Y} &= mean(Y)\\\\\n",
    "\\hat{y_i} &= regression\\ estimate\\ of\\ y_i\n",
    "\\end{align}  \n",
    "\n",
    "Given the above we can define the follow **sum of squares** relationships:   \n",
    "\n",
    "\\begin{align}\n",
    "SSE &= sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR &= sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST &= sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2 \\\\\n",
    "SST &= SSR + SSE\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$: variance explained\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. In other words, when fitting the model we wish to explain the maximum amount of the variance in the original data. We can quantify the **faction of squared error explained** with the **coefficient of determination** also known as $R^2$. We can express $R^2$ as follows:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}$$\n",
    "\n",
    "The $R^2$ for a perfect model would behave as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "SSR &\\rightarrow 0\\\\\n",
    "which\\ leads\\ to \\\\\n",
    "R^2 &\\rightarrow 1\n",
    "\\end{align}\n",
    "\n",
    "In words, a model which perfectly explains the data has $R^2 = 1$. For a model which does not explain the data at all we can write: \n",
    "\n",
    "\\begin{align}\n",
    "SSR &= SST \\\\ \n",
    "and \\\\ \n",
    "R^2 &= 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.    \n",
    " \n",
    "To addresses these related issues, we use **adjusted $R^2$**.\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_{SSR} &= SSR\\ degrees\\ of\\ freedom\\\\ \n",
    "df_{SST} &= SST\\ degrees\\ of\\ freedom\n",
    "\\end{align}\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n &= number\\ of\\ data\\ samples\\\\\n",
    "k &= number\\ of\\ model\\ coefficients\n",
    "\\end{align}\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F test on variance explained        \n",
    "\n",
    "Adjusted $R^2$ is one approach to comparing the variance reduction of a regression model. Another approach is to perform an hypothesis test. For this, we can use the F-test, a test on the ratio of variances. For a model with $k$ model parameters, fitted with $n$ observations. We can compute the F statistic as:       \n",
    "\n",
    "\\begin{align}\n",
    "F &= \\frac{Var_{between}}{Var_{within}} = \\frac{\\frac{SS_{between}}{bdof}}{\\frac{SS_{within}}{wdof}}\\\\\n",
    "&where\\\\       \n",
    "bdof &= k-1\\\\        \n",
    "wdof &= n - k\n",
    "\\end{align}  \n",
    "\n",
    "The F-test on the significance of the F statistic determines the significance of the regression model. A large value of the F-statistic indicates a low probability that the reduction in variance is from random sampling alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of model summary\n",
    "\n",
    "You can see an extensive summary of the fit of the linear model with the *summary* method. Execute the code in the cell below and examine the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table gives us a lot of information about the fit of this model. Let's examine some of these values.   \n",
    "- Starting with the **model coefficients**, the report includes an hypothesis test on the statistical significance of the model coefficients. In this case we can interpret this significance as follows: For both the **intercept** and **slope** the t-statistic is large, p-value is small, and the confidence interval does not include zero. These coefficients are statistically significant.\n",
    "- The **F-statistic** and **Prob (F-statistic)** are a measure of the significance of the model against a **null model that does not explain the data**. In this case the large F-statistic and small probability indicate that we can reject this null hypothesis and say the model is significant in terms of explaining the data.  \n",
    "- The **Omnibus statistic** is used in Statsmodels as a test of Normallity. In general, the omnibus statistic is a variance ratio test, with a $\\chi^2$ distributed statistic. In this case, the null hypothesis is that the residuals are Normally distributed. If the residuals have a significantly different distribution from the Normal, the hypothesis of Normallity can be rejected. *Note, the use of an omnibus statistic in Statsmodels is, unlike other statistical software packages. More typically, the omnibus statistic is a variance ratio test between the model and a null model.*      \n",
    "- The $R^2$ value is shown in the upper right corner. The value of 0.91 indicates a relatively good model fit.   \n",
    "- The **adjusted $R^2$** shown in the summary indicates that the model is a good fit. To find this quantity, notice the following: \n",
    "  - The **number of observations** and is the $df_{SST}$. \n",
    "  - The **degrees of freedom residuals** is the $df_{SSR}$. \n",
    "  - Notice that $df_{SST} - df_{SSR} =$ number of model coefficients. \n",
    "- The [**Jarque-Bera**](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) statistic is a test on the skewness and krutosis of the residuals. Given the large probability (p-value) we cannot reject the null hypothesis that the residuals have significant skewness and krutosis. \n",
    "- The **condition number** is a measure of how well defined the solution is to the system of linear equations solved. A low conditon number ($C \\lt 100$) is generally considered ideal.  \n",
    "  \n",
    "> **Warning:** The hypothesis tests on model coefficients suffer from the same problems of any hypothesis test. These problems are especially prevalent when the are large numbers of model parameters. For example, finding coefficients significant that are not, or vice versa is not uncommon. This situation can be aggravated when features have significant colinearity (correlation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Residuals\n",
    "\n",
    "There is one more important topic in evaluating regression models, the analysis of the **residuals**. The residuals of a regression model are the difference between the predicted values and actual values of the label. In other words, the residuals are the error term we write as $\\epsilon_i$ for the ith observation.\n",
    "\n",
    "A good linear regression model should have residuals with the following properties: \n",
    "\n",
    "1. The residuals should be approximately **Normally distributed with zero mean**. This criteria applied to any regression model using a least squares loss function. The least squares fitting criteria is only optimal for Normally distributed and zero mean residuals. We can express this important relationship mathematically as:  \n",
    "\n",
    "\\begin{align}\n",
    "y_i &=  mx_i + b + \\epsilon_i \\\\\n",
    "where, \\\\\n",
    "\\epsilon_i &\\sim N(0, \\sigma)\n",
    "\\end{align}\n",
    "\n",
    "2. The residuals should be **homoscedastic** with respect to the predicted values, $\\hat{Y}$. Homoscedastic residuals have constant variance, $\\sigma$ with respect to the predicted values. This criteria applies to any form of regression model. If this is not the case, we say that the residuals are **heteroscedastic**, with variance changing with respect to the predicted values. In other words, the variance is a function of the predicted values, $\\sigma(x_i) = f(x_i)$. A model with heteroscedastic residuals will have a better fit for small predicted values than large predicted values, or vice versa. We can write a model for hetroscedastic residuals as: \n",
    "\n",
    "$$\\epsilon_i \\sim N(0, f(x_i))$$\n",
    "\n",
    "As an example, consider a situation where the variance of the residuals grows exponentially with the value of the predictor.    \n",
    "$$\\epsilon_i \\sim N(0, e^{x_i})$$    \n",
    "\n",
    "In this case, the residuals will be strongly heteroscedastic. This outcome should alert any modeler to the fact that a better model is required. A logarithmic transformation of the variable is one possibility here. \n",
    "\n",
    "To start our analysis of residuals, execute the code in the cell below to compute residuals for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the dispersion of the residuals as a measure of regression performance. The metric is root mean square error or RMSE, which is very close to the standard deviation:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{ \\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}}= \\sqrt{\\frac{SSR}{n}}$$\n",
    "\n",
    "Since the residuals should be distributed as $N(0, \\sigma)$, we should also determine if the mean of the residuals is approximately 0. \n",
    "\n",
    "Execute the code in the cell below to compute and display the mean and RMSE for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean of the residuals = {0:4.3f}  RMSE = {1:4.3f}'.format(np.mean(sim_data.resids), np.std(sim_data.resids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonable value for RMSE considering the scale of the lable, $\\{0,11\\}$. Further, the mean of the residuals is effectively 0 as required by our model. \n",
    "\n",
    "Next, we need to determine if the distribution of the residuals is approximately Normal. Execute the code in the cell below to plot a histogram and Q-Q Normal plot of the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resid_dist(resids):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    ## Plot a histogram\n",
    "    sns.histplot(resids, bins=20, kde=True, ax=ax[0])\n",
    "    ax[0].set_title('Histogram of residuals')\n",
    "    ax[0].set_xlabel('Residual values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals')\n",
    "    plt.show()\n",
    "\n",
    "plot_resid_dist(sim_data.resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are reasonably close to Normally distributed.   \n",
    "\n",
    "Now, we must explore if the residuals are homoscedastic. A robust diagnostic for homoscedasticy is to create a **residual plot** with the residuals on the vertical axis plotted against the predicted values on the horizontal axis. Execute the code in the cell below to display an example.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(df, predicted='predicted', resids='resids'):\n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    RMSE = np.std(df.loc[:,resids])\n",
    "    sns.scatterplot(x=predicted, y=resids, data=df, ax=ax)\n",
    "    ax.axhline(0.0, color='red', linewidth=1.0)\n",
    "    ax.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.set_title('PLot of residuals vs. predicted')\n",
    "    ax.set_xlabel('Predicted values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    plt.show()\n",
    "    \n",
    "residual_plot(sim_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals look quite homoscedastic. In summary, the fit of the linear model looks quite good.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-1:** You will now construct a single regression model for the price of automobiles given the weight. Using statistical terminology we can stay we want to model the dependent variable, price, by the independent variable, or explanatory variable, weight. Now, follow these steps:   \n",
    "> 1. Execute the code in the cell below to load the required dataset and center (zero mean) the independent variables you will work with.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_price = pd.read_csv('../data//AutoPricesClean.csv')\n",
    "for col,new_col in zip(['curb_weight','engine_size'],['curb_weight_centered','engine_size_centered']):\n",
    "    auto_price[new_col] = auto_price.loc[:,col] - np.mean(auto_price.loc[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Next, in the cell below create and execute the code to define and fit the OLS model and print the summary. Use a formula `'price ~ curb_weight_centered'`. Name your model `auto_ols`.  \n",
    "> 3. Print the summary of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "## Fit the model and get the linear model summaries/plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals as following:      \n",
    ">  - Print the mean and the root mean square of the residuals.     \n",
    ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
    ">  - Create a residual plot of the residuals vs. the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now answer these questions:      \n",
    "> 1. Are the residuals 0 centered and what does this tell you about the model?       \n",
    "> 2. Are these residuals Normally distributed and if not what does this tell you about the model?    \n",
    "> 3. Are the residuals homoscedastic or heteroscedastic and why do you reach this conclusion?      \n",
    "> 4. Considering the adjusted R squared, the F statistic and the Omnibus statistic, does this model have explainatry power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.               \n",
    "> 2.      \n",
    "> 3.      \n",
    "> 4.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-2:** You might be able to improve the model by a transformation of the response variable. To find out, do the following:       \n",
    "> 1. Using the formula `'np.log(price) ~ curb_weight_centered` compute a new model, named `auto_ols_log`.  \n",
    "> 2. Print the summary of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "## Fit the model and get the linear model summaries/plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals of the new model as following:  \n",
    ">  - Compute the predicted values from the model and place them in a column in the data frame.    \n",
    ">  - Using the predicted values, compute the residuals and place them in a column in the data frame. \n",
    ">  - Print the mean and the root mean square of the residuals.     \n",
    ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
    ">  - Create a residual plot of the residuals vs. the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Next execute the code in the cell below to plot the observed values and the curve of the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_price['predicted_transformed'] = np.exp(auto_price['predicted'])\n",
    "## Plot the data and predicted values\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.lineplot(x='curb_weight', y='predicted_transformed', data=auto_price, color='red');\n",
    "sns.scatterplot(x='curb_weight', y='price', data=auto_price, ax=ax);\n",
    "ax.set_ylabel('Vehicle Weight');\n",
    "ax.set_xlabel('Vehicle Price');\n",
    "ax.set_title('Vehicle price vs. weight with fitted model line');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and consider the answers to the following questions:\n",
    "> 1. Compare the $R^2$, adjusted $R^2$ and the F-statistic between the original model and the model using the log transformed price. What does the difference say about the change in variance explained?             \n",
    "> 2. How has the mean of the residuals and RMSE changed between the models?     \n",
    "> 3. Do the residuals of the log price model appear to be approximately Normally distributed with zero mean?     \n",
    "> 4. Are the residuals of the log transformed price model homoscedastic?     \n",
    "> 5. What do the foregoing observations tell you about the fit of the log transformed price model?        \n",
    "> 6. How can you interpret the model coefficients in terms of the transformed response variable?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.              \n",
    "> 2.              \n",
    "> 3.              \n",
    "> 4.               \n",
    "> 5.              \n",
    "> 6.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Linear regressions are not just for straight lines\n",
    "\n",
    "A linear model is linear in its coefficients. But, that does not mean we are limited to straight lines, **a common misconception**.  A partial list of functions which can be included in a linear model includes:\n",
    "\n",
    "- Polynomials, but beware of polynomials of degree 3 or above.\n",
    "- Splines and smoothing kernels.\n",
    "- trigonometric functions.\n",
    "- Logarithmic and exponential functions.\n",
    "- Interaction terms, which are the product of feature values. For example, the two-way interaction of var1 and var2. In the R modeling language, interactions are specified as $var1:var2$. You can express the using both varibles plus the interaction as, $var1*var2 = var1 + var2 + var1:var2$.  \n",
    "\n",
    "You have already worked a first example which employed a logrithmic transformation. We will continue with another example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Another Example\n",
    "\n",
    "To clarify these concepts, let's look at an example. The code in the cell below computes a data set with a polynomial trend. THe $x$ variable is zero centered over a range -5.0 to 5.0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = -5.0, 5.0\n",
    "y_sd = 4.0\n",
    "incpt = 5.0\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(474747)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + 0.6 * np.square(x_data) + y_error + incpt # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data_poly = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes an simple ordinary least squares regression model for the data just created. Exectute this code and note the values of the slope and intecept coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = smf.ols(formula = 'y ~ x', data=sim_data_poly).fit()\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "sim_data_poly['predicted'] = ols_model.predict(sim_data_poly.x)\n",
    "\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for how well this model fits the data, execute the code in the cell below to plot the regression line and the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data_poly, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_poly, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit is not too bad, but you can see that the straight line is far from perfect.   \n",
    "\n",
    "Next, execute the code in the cell below to compute and plot the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_poly['resids'] = np.subtract(sim_data_poly.predicted, sim_data_poly.y)\n",
    "\n",
    "print('The mean of the residuals = {0:4.3f} median of residual = {1:4.3f}'.format(np.mean(sim_data_poly.resids),np.mean(sim_data_poly.resids)))\n",
    "print('RMSE = {0:4.3f}'.format(np.std(sim_data_poly.resids)))\n",
    "\n",
    "plot_resid_dist(sim_data_poly.resids)\n",
    "residual_plot(sim_data_poly)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the residuals is far from Normal. Further, it is clear from the residiual plot that the residuals are from homoscedastic. The residuals are negative at the extremes of the predicted values and positive in the middle.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polynomial Features\n",
    "\n",
    "Given the poor fit of the first model, it is time to try something else. In this case, we will try using polynomial terms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Scaling Data\n",
    "\n",
    "When performing regression with multiple numeric features, you will often need to **scale the data**.  Scaling data is important not just for regression, but most other machine learning models. Some reasons to scale regression data include:\n",
    "\n",
    "- Scaling prevents features with a large numerical range from overwhelming features with small numerical values. Numerical range is not an indicator of feature importance!\n",
    "- Related to the above point, you may encounter convergence problems with many machine learning algorithms if the numeric range of the features is different. \n",
    "\n",
    "There are several possible approaches to scaling data:\n",
    " - Scale the features or independent variables. This is the most common practice.\n",
    " - Scale the label or dependent variable, which is rarely done. It is generally better practice to perform a transform the distribution to be closer to Normal. \n",
    " - Scale both features and label. Rarely required.  \n",
    " \n",
    "In this case, we take advantage of the fact that `statsmodels.formula.api` models perform the scaling. For example if our model formula is `y ~ x + I(x**2)`, statsmodels scales the features, $x$ and $x^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-3:** In the cell below create and execute code to do the following:\n",
    "> 1. Define and fit an OLS model using the x_centered and x_centered squared features. This model will be linear in the coefficients of these features. The model formula uses the `I()` operator to create the higher order terms of the model. For example, the $x^2$ term is defined as `I(x**2)`. Name your model `ols_model_poly`.\n",
    "> 2. Print the summary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "## Define the regresson model and fit it to the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Now, you will evaluate the behavior of the residuals. In the cell below, create and execute code to do the following:    \n",
    ">   - Compute the predicted values from the model and save these in the predicted column of the data frame.   \n",
    ">   - Compute the residuals and save these in the resids column of the data frame. \n",
    ">   - Compute and print the mean and RMSE of the residuals. \n",
    ">   - PLot the histogram and Q-Q Normal plot of the residuals. \n",
    ">   - Display the residual plot the residuals vs. the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. To get a feel for the model fit create and execute code to plot the regression line (predicted values) and the observations in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the plots and consider the answers to these questions:\n",
    "> 1. Are all model coefficients significant or only some and why?    \n",
    "> 2. Is the polynomial model significant based on the F-statistic and why?    \n",
    "> 3. How have $R^2$, adjusted $R^2$ and RMSE changed with the addition of the higher order feature and what does this mean in terms of the fit of the model?      \n",
    "> 4. Do the residuals appear to be approximately Normally distributed with zero mean?\n",
    "> 5. Are the residuals approximately homoscedastic?    \n",
    "> 6. Now, consider the entire evaluation of the model. Is the model a good fit to these data?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.              \n",
    "> 2.             \n",
    "> 3.                 \n",
    "> 4.             \n",
    "> 5.                \n",
    "> 6.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024 Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
