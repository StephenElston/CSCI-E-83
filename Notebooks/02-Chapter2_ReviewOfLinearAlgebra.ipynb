{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e138c8",
   "metadata": {},
   "source": [
    "# Chapter 2   \n",
    "\n",
    "# Review of Numerical Linear Algebra \n",
    "## Using Numpy and Pandas   \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data manipulation and numerical linear algebra are core skills for anyone performing computational statistics. Not only that, but this background is essential for you as your study of the rest of this book. Thus, this topic is a natural starting point.        \n",
    "\n",
    "In this chapter we will first review numerical linear algebra using the Python NumPy package. We will explore some basics of linear algebra, the algebra of arrays. Our exploration will incldue basic algebraic manipulation of arrays and the often confusing topic of eigen decomposition. The focus, is on the use of the Numpy package for numerical linear algebra. There are many excellent texts which treat linear algebra in depth, including @Strang and @Lay. Those interested in an in-depth discussion of numerical algorithms for linear algebra can consult @GolubVanLoan.  \n",
    "\n",
    "Second, this chapter introduces the basics of data manipulation with the both the Pandas data frame package and NumPy. Even though data manipulation is not the subject of this book, we will be using Numpy and Pandas throughout this book for data manipulaiton, often reffered to by data scientists as 'data munging'. @McKinney presents a broad overview of data manipulation and management using Pandas and NumPy.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7981e39",
   "metadata": {},
   "source": [
    "## Linear Algebra and NumPy\n",
    "\n",
    "In this section we will review basic computational linear algebra. An understanding of computational linear algebra is essential to practicing computational statistics. Our focus will be numerical computing with the NumPy package Numpy.\n",
    "\n",
    "In this book we only review the basics of linear algebra with NumPy. You can find more details in the NumPy tutorial.\n",
    "\n",
    "Programming Note: NumPy is widely used for compuational statistics. NumPy computations are generally optimized. However, NumPy does not take full advantage of today's massively parallel cluster computing. Consequently, platforms like Spark [@Spark], TensorFlow [@tensorflow2015], and Torch [@paszke2017] take full advantage of today's massive scale custer computing environments. We do not address use of these platforms in this book. However, the principles of numerical linear algebra are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c43f77",
   "metadata": {},
   "source": [
    "## Types of Arrays\n",
    "\n",
    "Linear algebra is the algebra of arrays. An array is a regular arrangement of numbers. An array can have any number of dimensions, at least in principle. The type of an array is not only defined by shape but also data type. In this discussion we focus on arrays of floating point numbers. \n",
    "\n",
    "A single numeric value is a **scalar**. A scalar has dimensional of 0.  \n",
    "\n",
    "A **vector** is a one dimensional array. As an example a vector of length n, and the $ith$ value or element expressed as $x_i$, is written, $\\mathbf{x} = [x_1, x_2, \\ldots, x_n ]$. We can also express a **column** vector:   \n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "   x_{1} \\\\\n",
    "   x_{2} \\\\\n",
    "   \\vdots \\\\\n",
    "   x_{n}\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A **matrix** is a 2 dimensional array. As an example, consider a matrix with dimension $n$ rows by $m$ columns, $\\mathbf{A}$. An element of this matrix for the $ith$ row and $jth$ column can be written, $x_{i,j}$. In general, this matrix can be expressed:  \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "   a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\\n",
    "   a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   a_{n,1} & a_{n,2} & \\ldots & a_{n,m}\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As has already been mentioned, arrays can have any number of dimensions. We will focus on vectors and matrices, as these are used most often in computational statistics.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc0d52",
   "metadata": {},
   "source": [
    "## Element-Wise Operations\n",
    "\n",
    "We will start with simple element-wise arithmetic operations which can be performed on arrays. These operations include addition, subtraction, and multiplication. Elementwise operations have a **local effect** only, with the result determined by each paired set of values.   \n",
    "\n",
    "Element-wise operations require both arrays to be **conformable**. By conformable arrays we mean arrays which have dimensions that conform to the shape required for the operation being performed. For element-wise operations, conformable arrays must have the exactly the same dimensions so that each element in each array has a corresponding element in the other array. There is an exception, when one array is a scalar, therefore having dimension 0.      \n",
    "\n",
    "As a first step we will start an example by creating two vectors of length 3. In the code below, the NumPy [array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) function is used to create these vectors. The vector is printed along with its type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import numpy.linalg as npla\n",
    "import io\n",
    "import requests\n",
    "from math import sqrt, acos\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "y = np.array([2]*3)\n",
    "print('Array y = {}, with type {}'.format(y,type(y)))\n",
    "x = np.arange(1, 4)\n",
    "print('Array x = {} with type {}'.format(x,type(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebe287",
   "metadata": {},
   "source": [
    "We can perform element-wise operations on these array with a scalar. Some examples are shown below. Notice that the usual Python arithmetic operators are used for element-wise operations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_scalar = 1.0\n",
    "print(a_scalar + y)\n",
    "print(a_scalar - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5ec58",
   "metadata": {},
   "source": [
    "As further examples, the code below applies some element-wise operations to the arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db401c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y + x)\n",
    "print(y - x)\n",
    "print(y * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f16ce",
   "metadata": {},
   "source": [
    "We can also perform element-wise operations on matrices. The code below creates a 4x3 (four rows x three columns) array all with the same value, 2.0, using the NumPy [full](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.full.html) function and prints the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9365d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.full((4,3), 2.0)\n",
    "print('A = \\n{}'.format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b1518",
   "metadata": {},
   "source": [
    "Notice how this array is displayed. The rows are shown in order from the first row to the last. The values of each column for each row are shown within the row vectors. This is an important property of NumPy arrays, known as **row major order**. Other programming languages may use the alternative column major order. \n",
    "\n",
    "The code below creates another 4x3 array containing a sequence of numbers from 1 to 12, with the following steps: \n",
    "1. The NumPy [arange](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html) function creates a 1-dimensional 1x12 array (vector) of the values.\n",
    "2. The shape attribute is changed to 4x3 using the [reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) method.  \n",
    "3. The `shape` attribute is then accessed and printed. Notice there is a difference between changing the shape attribute with the `reshape` function and accessing the shape attribute of the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa828cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.arange(1,13).reshape((4,3))\n",
    "print('Matrix B with shape {} \\n{}'.format(B.shape, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234a6d1",
   "metadata": {},
   "source": [
    "We can add a scalar to the matrix as shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c897269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} + A = \\n{}'.format(a_scalar, a_scalar + A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0859fc5",
   "metadata": {},
   "source": [
    "And, we can add the two conformable arrays as shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcf54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('A + B = \\n{}'.format(A + B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950f1a2",
   "metadata": {},
   "source": [
    "> **Exercise 02-1:** In the foregoing code example two conformal matrices are summed element-wise. Now, you will create conformable matrix with all the elements with a value of 4.0. Then, multiply this matrix by the matrix B. Check that your result is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07ae6f",
   "metadata": {},
   "source": [
    "## The Dot Product, A Fundamental Array Operation\n",
    "\n",
    "After the foregoing introduction to the element-wise operations we move on to other array operations. We will start with perhaps the most fundamental multi-element operations, the **inner product** also known as the **dot product** or **scalar project**. Many other linear algebra calculations can be constructed using the dot product. If your software can do dot projects efficiently many other operations can be performed efficiently. This observation is the basis of many scalable machine learning platforms such as Torch, TensorFlow and SPARC.    \n",
    "\n",
    "The dot product between two vectors is defined as follows an is denoted as wither $a \\cdot b$ or $<a,b>$: \n",
    "\n",
    "$$\n",
    "dot\\ product = a \\cdot b =\\ <a,b>\\ = \\Sigma_i^n a_i \\cdot b_i \n",
    "$$\n",
    "\n",
    "A useful property is that the **Euclidian norm** (length), or **L2 norm**, of a vector is the square root of the dot product with itself. This can be expressed:\n",
    "\n",
    "$$\n",
    "\\parallel a \\parallel = length\\ of\\ vector\\ a = \\sqrt{a \\cdot a}\n",
    "$$\n",
    "\n",
    "But, how can you interpret the dot product? The dot product is the **projection** of one vector on another. This concept can be expressed mathematically:   \n",
    "\n",
    "$$\n",
    "a \\cdot b = \\parallel a \\parallel \\parallel b \\parallel cos(\\theta)\n",
    "$$\n",
    "\n",
    "Rearranging terms we can find the **cosine distance** between two vectors as: \n",
    "\n",
    "$$\n",
    "cos(\\theta) = \\frac{a \\cdot b}{\\parallel a \\parallel \\parallel b \\parallel}\n",
    "$$\n",
    "\n",
    "Notice that the dot product of orthogonal (perpendicular) vectors is $0$. In this case there is no projection of either vector on the other. Whereas, for parallel vectors the dot product is at a maximum.    \n",
    "\n",
    "These concepts are illustrated in the figure below. The dot product is the projection of one vector on another. The angle between the two vectors, $\\theta$, determines the projection. You can see that if $\\theta = \\frac{\\pi}{2}$ the projection will be 0. If $\\theta = 0$ the projection is maximized.  \n",
    "\n",
    "<img src=\"../images/DotProduct.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "The code in the cell below computes the dot product of the two vectors, a and b using the NumPy [dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For vectors')\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "np.dot(x,y)  \n",
    "print('<x,y> = {}'.format(np.dot(x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388f878",
   "metadata": {},
   "source": [
    "The result is the expected 12 = 2 + 4 + 6.  \n",
    "\n",
    "As has been mentioned, the dot product is the projection of one vector on another. The code below demonstrates this point by taking the dot product of a vector with 3 times the same vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f351842",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(y, np.dot(3,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1075c",
   "metadata": {},
   "source": [
    "This dot project is 3 times the dot product of $y$ with itself. Demonstrating that the projection scales with the length of the vectors.    \n",
    "\n",
    "**Orthogonal vectors** should have a dot product of 0. Intuitively, orthogonal vectors have no common directions. This useful property can be used to the test if vectors are orthogonal. This concept is demonstrated by the code below which takes the dot product of two orthogonal vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0,1.0,0.0])\n",
    "z = np.array([0.0,0.0,1.0])\n",
    "np.dot(w, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1923366",
   "metadata": {},
   "source": [
    "The result is 0, as expected. \n",
    "\n",
    "As already stated, dot or inner product can be used to compute the L2 norm of a vector. The code below uses the [`norm`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html) function from the [`numpy.linalg`](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html) package to compute the norm of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b76b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "npla.norm(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e685aa",
   "metadata": {},
   "source": [
    "> **Exercise 02-2:** In the cell below create and excute the code to computes the cosign of the angle between the vectors x and y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2d2bf",
   "metadata": {},
   "source": [
    "## The matrix transpose\n",
    "\n",
    "In many cases, we need the **transpose** of a matrix. The transpose is found by interchanging the row and column indices of a matrix.   \n",
    "\n",
    "Consider a 1-dimension vector of dimension $1 x n$. The transpose of this array is a column vector of dimension $n x 1$. \n",
    "\n",
    "The elements of a 2-dimensional matrix are indexed first by a row index and then a column index, or $A_{i,j}$. The transpose is then, $A_{j,i}$, with the indices reversed. For example, using the notation #B^T# for the transpose, we can write:\n",
    "\n",
    "$$\n",
    "B_{ji} = B_{ij}^T\n",
    "$$\n",
    "\n",
    "where,   \n",
    "$B$ has dimensions n x m.   \n",
    "$B^T$ has dimensions m x n. \n",
    "\n",
    "To illustrate this idea, the code below uses the NumPy [transpose]() function to find the transpose of a matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a578ba",
   "metadata": {},
   "source": [
    "## The matrix product\n",
    "\n",
    "Can we take other products of arrays? The answer is yes. We can multiply a vector times a matrix, or a matrix times a matrix. As you will see all of these operations are built with dot products. This is why the dot product is so important in computational statistics.   \n",
    "\n",
    "To start, consider how to compute the product of a matrix and a vector. You can think of this operation as a series of dot products between the rows of the matrix and the column vector. More specifically, an $m x n$ matrix, $\\mathbf{A}$, is multiplied by a vector, $\\mathbf{x}$, of length $m$ by taking the $n$ dot products. The result is a vector of length $n$. This concept is illustrated in the relationship below. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\\n",
    "   a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   a_{n,1} & a_{n,2} & \\ldots & a_{n,m}\n",
    "   \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   x_{1} \\\\\n",
    "   x_{2} \\\\\n",
    "   \\vdots \\\\\n",
    "   x_{n}\n",
    "   \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "   \\mathbf{A}_{row\\ 1} \\cdot \\mathbf{x} \\\\\n",
    "   \\mathbf{A}_{row\\ 2} \\cdot \\mathbf{x} \\\\\n",
    "   \\vdots \\\\\n",
    "   \\mathbf{A}_{row\\ n} \\cdot \\mathbf{x}\n",
    "   \\end{bmatrix}   =\n",
    "\\begin{bmatrix}\n",
    "   y_1 \\\\\n",
    "   y_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   y_n\n",
    "   \\end{bmatrix}   \n",
    "$$\n",
    "\n",
    "We can summarize the above for the $ith$ element of the resulting vector as:\n",
    "\n",
    "$$\n",
    "y_i = \\Sigma_j^m A_{ij} \\cdot x_j\n",
    "$$\n",
    "\n",
    "\n",
    "To demonstrate this concept, the code in the cell below uses the NumPy `dot` function to compute the product of a matrix and a vector.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e59157",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(B, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022497e",
   "metadata": {},
   "source": [
    "How do we extend the above computation to compute the product of two matrices? The answer is simple and intuitive. Each element of the result is the dot product between a row of the first matrix and a column of the second matrix. This is known as the row by column or **RC rule**. The product of an $n x m$ matrix with an $m x n$ matrix is an $m x m$ dimensional array, computed by taking $m^2$ dot products. For example, the element $Y_{ij}$ of the result matrix is computed as follows:\n",
    "\n",
    "$$\n",
    "Y_{ij} = \\Sigma_j^m A_{ij} \\cdot B_{ji}\n",
    "$$\n",
    "\n",
    "Notice that to be conformable the number of columns, $m$, of the first matrix must equal the number of rows of second matrix. And, that the number of rows, $n$ of the first matrix must equal the number of columns of the second matrix.\n",
    "\n",
    "The code in the cell below computes a matrix product. As both matrices are of dimension $4x3$, it is necessary to take the transpose of one or the other.  \n",
    "\n",
    "> **Programming Note:** For this example, we could use the Numpy `dot` function. However, here we use the [`matmul`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul) function. For 1 and 2 dimensional arrays, the results will be the same. But, `matmul` works with arrays of higher dimenstions, but not with scalars. The arrays must be conformable for either function to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.transpose(A) , B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef87a1",
   "metadata": {},
   "source": [
    "The above operation resulted in a $3 x 3$ matrix. Taking the transpose of the second matrix results in a conformable operation, but the result will have different dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(A, np.transpose(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1cef0",
   "metadata": {},
   "source": [
    "Now, the product is a $4 x 4$ matrix. \n",
    "\n",
    "> **Programming Note:** In the foregoing you have seen how matrix products are built from dot products. If the dot product can be computed efficiently, then so to can matrix products be computed efficiently. Not only that, but each of the dot products required for these operations can be computed in parallel, as there is no dependency of one on another. These properties are at the basis of many of today's high performance computing environments used for machine learning and AI.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f630e",
   "metadata": {},
   "source": [
    "> **Exercise 02-3:** Perhaps, you are curious about what happens if you try to multiply non-conformable arrays. To find out, multiply the matrix A by the matrix B, but without the transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f7bd5",
   "metadata": {},
   "source": [
    "> Why is there a mismatch in the dimensions when attempting to compute this matrix product? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a0c54",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0311b",
   "metadata": {},
   "source": [
    "## The identity matrix and the inverse\n",
    "\n",
    "Is it possible to divide one matrix by another? Yes in a certain way, but not in a direct manner. Instead, this operation is performed by finding the **inverse** of a matrix and multiplying by that inverse by the other matrix.   \n",
    "\n",
    "Before explaining the inverse, we need to examine a matrix with special properties, the **identity matrix**. An identity matrix is a square $n x n$ matrix with 1s on the diagonal and 0s everywhere else. We can write the identity matrix as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "I = \\begin{bmatrix}\n",
    "    1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & 1 & 0 & \\dots & 0 \\\\\n",
    "    0  & 0 & 1 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The code in the cell below creates a $3 x 3$ identity matrix using the NumPy [eye](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html) function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = np.eye(3)\n",
    "I3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63484fca",
   "metadata": {},
   "source": [
    "hat will be the result if we multiply the identity by another square matrix? To understand the result, consider that multiplying by an identity matrix in linear algebra is the same as multiplying by a 1 in ordinary algebra.   \n",
    "\n",
    "The identity multiplied by any matrix gives that same matrix. If $A$ is a square matrix (conformable) then:\n",
    "\n",
    "$$\n",
    "A = I \\cdot A = A \\cdot I\n",
    "$$\n",
    "\n",
    "Let's illustrate this concept with an example. First, the code in the cell below creates a $3 x 3$ numeric matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[1,3,6],\n",
    "              [2,2,1],\n",
    "              [3,1,4]])\n",
    "print('C = \\n{}'.format(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cddbb2",
   "metadata": {},
   "source": [
    "Now, we compute the product of the above matrix with the identity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903177dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('C I3 = \\n{}'.format(np.dot(C,I3)))\n",
    "print('I3 C = \\n{}'.format(np.dot(I3, C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1beae92",
   "metadata": {},
   "source": [
    "The result is the same as the original matrix. This example demonstrates that multiplying a square matrix by an identity matrix of the same dimension is the same as multiplying by 1 in regular algebra. \n",
    "\n",
    "Let's get back to the inverse of a matrix. In principle we can compute an inverse of a square matrix so that the following relationships hold:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{split}\n",
    "A &= A\\\\\n",
    "A &= AI \\\\\n",
    "A^{-1}A &= I\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "\n",
    "Here, $A^{-1}$ is the inverse of $A$.\n",
    "\n",
    "To illustrate this concept the code in the cell below computes the inverse of the matrix using the [inv](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) from the NumPy linalg package. The inverse is then multiplied by the original matrix. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2305bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_C = npla.inv(C)\n",
    "print('inv_C = \\n{}'.format(inv_C))\n",
    "print('inv_C C = \\n{}'.format(np.dot(inv_C,C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42374391",
   "metadata": {},
   "source": [
    "The result is an identity matrix, as it should be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee7b21",
   "metadata": {},
   "source": [
    "> **Exercise 02-4:** The identity matrix has a special property, that it is its own inverse. To demonstate this property, create a $4x4$ identity matrix and take its inverse. Verity that the result the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11231a",
   "metadata": {},
   "source": [
    "## Slicing NumPy arrays\n",
    "\n",
    "When using NumPy, you will very often need a subset of an array for some operations. Fortunately, NumPy arrays are easy to subset, an operation known as [**slicing** or **indexing**](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html). For a NumPy array, the slicing operator are the square brackets, `[]`. For a 2-dimensional matrix the syntax for this operator is:\n",
    "\n",
    "$$\n",
    "slice = array[row\\_start:row\\_end,\\ column\\_start,column\\_end]\n",
    "$$\n",
    "\n",
    "Where, keeping in mind that NumPy indices are zero based,     \n",
    "$row\\_start =$ the index of the first row in the slice.     \n",
    "$row\\_start =$ the index of the first row not in the slice.    \n",
    "$row\\_start =$ the index of the first column in the slice.     \n",
    "$row\\_start =$ the index of the first column not in the slice.    \n",
    "\n",
    "The figure below shows an example of slicing a 2-dimensional array, or matrix. The matrix has dimension $8 x 6$, and a slice of dimension $4 x 3$ is created. Notice the syntax of the row and column slices. \n",
    "\n",
    "<img src=\"../images/NumpySlice.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "There are some other ways to specify slices that use implied starts or ends, or a specific list of indices. \n",
    "\n",
    "| Required Operation | Syntax |\n",
    "| --- | --- |\n",
    "| All rows or all columns | `:` |     \n",
    "| first n rows or columns | `:n`|   \n",
    "| Rows or columns after the nth | `n:` |   \n",
    "| All but the last n rows or columns | `:-n` |   \n",
    "| Specific rows or columns | List of indices |       \n",
    "| Specific rows or columns | List of logicals |      \n",
    "\n",
    "To illustrate these operations, let's try some examples. The code below creates a slice of columns 1 and 2 a NumPy matrix, keeping all rows:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e305ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[:, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024ad76",
   "metadata": {},
   "source": [
    "The code below creates a slice of the first 2 rows of a NumPy matrix, while keeping all columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312d7bf",
   "metadata": {},
   "source": [
    "And, the code below creates a slice with rows 0 and 2. Don't be confused by the two sets of square brackets, `[]`. The outer square brackets are the slicing operator whereas, the inner square brackets are for the list of row indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[[0,2], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba76d5",
   "metadata": {},
   "source": [
    "The same result can be achieved with a list of logical type, as shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fda6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[[True,False,True,False], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df0a78",
   "metadata": {},
   "source": [
    "The results are the same for both of the above cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0628215",
   "metadata": {},
   "source": [
    "> **Exercise 02-5:** In the foregoing discussion of the matrix inverse, we did not address an important and common problem. Simply put, matricies with colinear or parallel columns are said to be **singular**. The inverse of a singular matrix does not exist. You will now determine if the columns of a matrix are nearly colinear. In compuational linear algebra, nearly colinear columns will cause matrix inverse algorithms to fail. Dealing with this problem is a focus of Chapter 24 of this book. You will now complete the code below to do the following: \n",
    "> 1. First you will construct a nearly sigular $4 x 4$ matrix with sequential values 1 to 16 running row-wise. Name this matrix `E`.    \n",
    "> 2. Next, using the indices createded by the Itertools [`combinations`](https://docs.python.org/2/library/itertools.html#itertools.combinations) function you will slice the array column-wise, for every possible pair of columns. Notice that you do not need to hard code the instantiation of the vector of possible column indicies.  \n",
    "> 3. Compute the cosign of the angle between each of the column-wise slices. \n",
    "> 4. Print the indices and the cosign of the angle between them. \n",
    "> 5. Compute the inverse of the matrix and print the result.\n",
    "> 6. Display the product of the matrix and the inverse computed.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77279127",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94953c3f",
   "metadata": {},
   "source": [
    "> Now, answer the following questions:    \n",
    "> 1. What does the cosine of the angle between the vectors tell you about the colinearity of the columns of the matrix?    \n",
    "> 2. Examine values of the inverse computed. What do these large values tell you about the stability of the inverse calculations?    \n",
    "> 3. Examine the product of the matrix with its inverse. What does this result tell you about the invese?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d844c27",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.       \n",
    "> 2.        \n",
    "> 3.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b20a9",
   "metadata": {},
   "source": [
    "## Copies of NumPy arrays\n",
    "\n",
    "NumPy operations follow the same assignment rules as other Python operators. However, these assignment rules create pitfalls for newcomers. A simple `=` only assigns a new name to the same array. To ensure the ssignment creates a copy, one must use the [`copy`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.copy.html) function. This idea is illustrated in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.copy(B)\n",
    "B = B + 1.0\n",
    "print('B = \\n{}'.format(B))\n",
    "print('C = \\n{}'.format(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdc1f4",
   "metadata": {},
   "source": [
    "These results are what we intended. Failure to use the `copy` function can result in unexpected and confusing errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126c449",
   "metadata": {},
   "source": [
    "## Broadcasting for Numeric Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df74624",
   "metadata": {},
   "source": [
    "## Pandas, a Data Scientist's Friend\n",
    "\n",
    "Now we will turn our attention to the Pandas package [@Pandas]. Pandas has become a standard tool in data scientists' tool kit for data management and manipulation. \n",
    "\n",
    "Pandas is an extension of NumPy's arrays. Pandas provides significant capabilities to mange and manipulate tabular data. The focus of this book is not on the data manipulation capabilities of Pandas, so only an overview is provided here. The [tutorials on the Pandas.org website](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html#getting-started) provide greater depth.      \n",
    "\n",
    "### What is a data frame?\n",
    "\n",
    "The basic Pandas data structure is the data frame. A data frame is a tabular data structure with a number of special attributes. The most important of these attributes are the row indices, column indices and data types of the columns. A schematic view of a Pandas data frame with these important attributes is illustrated in the figure below.   \n",
    "\n",
    "<img src=\"../images/PandasDataFrame.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "As you can see from the above figure, the basic Pandas data frame is an $n x m$ table. The data are arranged in $m$ columns. Each column has an index, which can be a column name as a string, and a data type. Each column can have a different data type, but each column can only have one type. There is also a set of row $n$ indices. Row indices are typically numeric, in the range $\\{0,n-1\\}$, but can be names in the form of strings.   \n",
    "\n",
    "An example will help illustrate these points. There are a number of ways to construct a Pandas data frame. Perhaps, the simplest is creating a data frame from an array, using the Pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) method, as is done here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80af9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe = pd.DataFrame([[1, 3, 5],\n",
    "                            [2, 4, 8],\n",
    "                            [10, 20, 30],\n",
    "                            [15, 25, 35]])\n",
    "a_dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780ed41",
   "metadata": {},
   "source": [
    "You can see that the data frame has integer column and row indices at this point. We can assign new values to these attributes, as shown in the code below. The `index` attribute changes the row indices and the `columns` attribute changes the column names.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.index = ['first_row','second_row','third_row','forth_row']\n",
    "a_dataframe.columns = ['first_col','second_col','third_col']\n",
    "a_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044b782",
   "metadata": {},
   "source": [
    "Let's add another column to this data frame. This can be done using the [`loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) method. The `loc` method allows you to specify the rows as the first argument and the columns as the second argument. A colon, `:` indicates all rows or all columns. The code below adds a new column named 'animal'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c07ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_dataframe.loc[:,'animal'] = ['chicken','chicken','duck','duck']\n",
    "a_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dd4f0",
   "metadata": {},
   "source": [
    "Let's have a look at the type attributes of the columns, the `dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fcc5a",
   "metadata": {},
   "source": [
    "The first three columns are integer type. With Pandas, the type of a string column is shown as `object`.\n",
    "\n",
    "It is possible to change the data types of columns. In the code below we will change the type of the third column to floating point, using the `flaot64` type from NumPy. The string column is coerced to categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1bb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.loc[:,'animal'] = a_dataframe.loc[:,'animal'].astype('category')\n",
    "a_dataframe.loc[:,'third_col'] = a_dataframe.loc[:,'third_col'].astype(np.float64)\n",
    "a_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514dd09",
   "metadata": {},
   "source": [
    "Notice the new dtypes of the last two columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b268",
   "metadata": {},
   "source": [
    "### Slicing Pandas Data Frames\n",
    "\n",
    "Just as we did for NumPy arrays, [Pandas data frame can be sliced](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html). There are a number of ways to define slices for Pandas data frame. Here we will focus on the computationally efficient [`loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) and [`iloc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html) methods. You have already seen an example of using `loc` above. \n",
    "\n",
    "The `iloc` method slices using row and column indices, hence the 'i'. The rules and syntax are essentially identical to those used for slicing NumPy arrays, which we have already discussed. \n",
    "\n",
    "The `loc` method performs slicing operations on Pandas data frames using column or row names, as well as logical selection. The syntax and rules are similar to NumPy array slicing. For example, the code below creates a slice with the columns up to and including `third_col`.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e133f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.loc[:,:'third_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fe3c1",
   "metadata": {},
   "source": [
    "As another example, the code below creates a slice starting with `second_col` and ending with, and including, `third_col`.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.loc[:,'second_col':'third_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761d4e2",
   "metadata": {},
   "source": [
    "The `loc` method can be used to find a slice of a data frame including the columns in a list, as shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.loc[:,['second_col','animal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36caae52",
   "metadata": {},
   "source": [
    "Finally, a logical operation can be used to for slicing, as shown below. In this case, the `loc` method is used twice. Once for the slicing the data frame and once for the logical operator.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataframe.loc[a_dataframe.loc[:,'animal']=='duck',:'third_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f368b",
   "metadata": {},
   "source": [
    "### Copies of Pandas data frames\n",
    "\n",
    "Just like NumPy operations, Pandas operations follow the same assignment rules as other Python operators.  An `=` operator only assigns a new name to the same array. Further,  the `loc` and `iloc` methods only create references, they do not return copies. This behavior optimizes memory use for scalability. If you want to ensure you have actually made a copy use the [`copy`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html) method.     \n",
    "\n",
    "This idea is illustrated in the code below which performs the following steps:\n",
    "1. A slice is assigned to a new name. \n",
    "2. A slice is created and copied and assigned to another name. \n",
    "3. A A constant is added to the first slice using the Pandas [`add`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.add.html) method.\n",
    "4. The values for each name are printed. \n",
    "\n",
    "> **Programming Note:** Notice there are two methods in the second line of code, This is an example of **chaining operators**. Chaining operators is a powerful approach for computational statistics. We will chain operators many times in the remainder of this book.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_dataframe = a_dataframe.loc[a_dataframe.loc[:,'animal']=='duck',:'third_col']\n",
    "copy_dataframe = a_dataframe.loc[a_dataframe.loc[:,'animal']=='duck',:'third_col'].copy()\n",
    "another_dataframe = another_dataframe.add(1.0)\n",
    "another_dataframe\n",
    "copy_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b029df",
   "metadata": {},
   "source": [
    "Notice the difference in these results. A copy is not the same as a slice of the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495f18d",
   "metadata": {},
   "source": [
    "### The Pandas series\n",
    "\n",
    "We have been working with the 2-dimensional Pandas data frames. There is a special case for univariate data, the [Pandas series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html). This distinction is important. Panads series have different attributes from data frames. This means different methods are applicable. Failure to abide by this distinction will often produce unexpected results or raise exceptions.   \n",
    "\n",
    "The code below creates a single column slice from a data frame. The result is Pandas series. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_series = a_dataframe.loc[:,'second_col'].copy()\n",
    "a_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82755f4",
   "metadata": {},
   "source": [
    "A noticeable difference for this series is that there is no column name attribute. Since a Pandas series has only one column, there is no need for this attribute.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789dfa17",
   "metadata": {},
   "source": [
    "> **Exercise 02-6:** A limited number of mathematical operatons can be performed directly on numeric columns in a Pandas data frame. To use the richer capabilities of NumPy the columns of a of a Pandas data frame can be coerced to a Numpy array with the Pandas [`as_matrix`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.as_matrix.html) method. To exercise this capability, do the following:      \n",
    "> 1. Create a data frame from the columns named `first_col` and `second_col` of the data frame `a_dataframe`.      \n",
    "> 2. Take the element-wise square toot of the resulting fully numeric data frame using the Numpy [`sqrt`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html) function.     \n",
    "> 3. Assign the resulting Numpy array values back to the columns in the Data Frame.     \n",
    "> 4. Print the data frame and verify that the result is correct.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb29914",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e531e41",
   "metadata": {},
   "source": [
    "## Eigen-Decomposition\n",
    "\n",
    "Eigen-decomposition is a factorization of a squre matrix. The eigen-decomposition plays a fundamental role in many data science algorithms. Therefore, having an undersanding of this factorization is a foundation to deeper understanding.   \n",
    "\n",
    "In eigen-decomposition, the matrix is decomposed into **eigenvalues** and corresponding **eigenvectors**. Eigenvalues and eigenvectors represent characteristic roots or characteristic values of a linear system of equations. For an eigenvalue $\\lambda$ and corresponding eigenvector $x$, we can write:  \n",
    "\n",
    "\\begin{align}\n",
    "A x &= \\lambda x\\\\\n",
    "\\begin{bmatrix}\n",
    "   a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\\n",
    "   a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   a_{n,1} & a_{n,2} & \\ldots & a_{n,m}\n",
    "   \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   x_{1} \\\\\n",
    "   x_{2} \\\\\n",
    "   \\vdots \\\\\n",
    "   x_{n}\n",
    "   \\end{bmatrix}\n",
    "&= \\lambda\n",
    "\\begin{bmatrix}\n",
    "   x_{1} \\\\\n",
    "   x_{2} \\\\\n",
    "   \\vdots \\\\\n",
    "   x_{n}\n",
    "   \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "At first, this does not look much like the type of polynomial equation for which you can find roots. But a bit of rearranging of terms leads to a form that looks more promising:    \n",
    "\n",
    "\\begin{align}\n",
    "Ax - \\lambda x &= 0\\\\\n",
    "(A - \\lambda) X &=0\n",
    "\\end{align}\n",
    "\n",
    "In the form above it is clear that the eigendecomposition of A is in terms of its roots.     \n",
    "\n",
    "As with any polynomial there are multiple roots. In fact, for an $n \\times n$ square matrix there are $n$ eigenvalues and $n$ corresponding eigenvectors. The full eigen-decomposition of the matrix $A$ can be written:     \n",
    "\n",
    "\\begin{align}\n",
    " Q &= Q \\Lambda Q^T\\\\\n",
    " &= \\begin{bmatrix}\n",
    "   q_1^{(1)} & q_1^{(2)} & \\ldots & q_1^{(n)} \\\\\n",
    "   q_2^{(1)} & q_2^{(2)} & \\ldots & q_2^{(n)} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   q_n^{(1)} & q_n^{(2)} & \\ldots & q_n^{(n)}\n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix}\n",
    "   \\lambda_1 & 0 & \\ldots & 0 \\\\\n",
    "   0 & \\lambda_2 & \\ldots & 0 \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   0 & 0 & \\ldots & \\lambda_n\n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix}\n",
    "   q_1^{(1)} & q_2^{(1)} & \\ldots & q_n^{(1)} \\\\\n",
    "   q_1^{(2)} & q_2^{(2)} & \\ldots & q_n^{(2)} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   q_1^{(n)} & q_2^{(n)} & \\ldots & q_n^{(n)}\n",
    "   \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Where some key properties are:   \n",
    "- The eigenvectors are the components of the decomposition and have unit Euclidean or l2 norm, $\\big( q^{(i)} \\cdot q^{(i)} \\big)^{1/2} = 1$, or are termed **unitary**. Further, the eigenvectors are orthogonal, $\\big( q^{(i)} \\cdot q^{(j)} \\big)^{1/2} = 0, \\forall j \\ne i$.      \n",
    "- The magnitudes of the eigenvalues are ordered, $|\\lambda_1| \\ge |\\lambda_2| \\ldots \\ge |\\lambda_n| \\ge 0$. The eigenvalues scale the components of the decomposition. Notice that some eigenvalues can have the same values, and can have values of 0.     \n",
    "\n",
    "Let's try an example. Start by creating a square matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,3], [3, 1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34951b",
   "metadata": {},
   "source": [
    "The eigen-decomposition is computed using [numpy.linalg.eig](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eigendecomposition\n",
    "eigs, Q = np.linalg.eig(A)\n",
    "print(np.diag(eigs))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b21b6a",
   "metadata": {},
   "source": [
    "As expected, the $2 \\times 2$ matrix has two eigenvalues. The eigenvectors are in the columns of the second matrix.      \n",
    "\n",
    "We can verify that the eigenvectors are unitary by computing the Euclidean norm of the rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9743d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that Q is unitary and orthogonal   \n",
    "print('Euclidean norm of rows of Q')\n",
    "print(np.linalg.norm(Q, axis=1)) \n",
    "print('\\nEuclidean norm of columns of Q')\n",
    "print(np.linalg.norm(Q, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67394ea",
   "metadata": {},
   "source": [
    "Next, we can verify that the rows and columns of the eigenvector matrix are orthogonal.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe05ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that Q orthogonal   \n",
    "combs = combinations(range(Q.shape[0]), 2)\n",
    "print('Any non-orthogonal rows? ' + str(np.any([np.dot(Q[i,:],Q[j,:]) for i, j in combs])))\n",
    "print('Any non-orthogonal columns? ' + str(np.any([np.dot(Q[:,i],Q[:,j]) for i, j in combs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40adf2b",
   "metadata": {},
   "source": [
    "Alternatively we can compute the dot product of the rows and of the columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ca6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dot product of columns = ' + str(np.dot(Q[:,0],Q[:,1])))\n",
    "print('Dot product of rows = ' + str(np.dot(Q[0,:],Q[1,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d88ee6",
   "metadata": {},
   "source": [
    "> **Exercise 02-7**: You will now verify that the product of eigenvectors and eigvalues shown above will reconstruct the original matrix. Use the numpy.dot, [numpy.diag](https://numpy.org/doc/stable/reference/generated/numpy.diag.html) and [numpy.transpose](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) functions to perform the calculation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11476935",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code in the cell below  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457c650",
   "metadata": {},
   "source": [
    "### Condition number and rank  \n",
    "\n",
    "The eigenvalues can be used to determine a fundamental property of a matrix known as the **condition number**. The condition number is the ratio of the magnitudes of the largest eigenvalue to the smallest eigenvalue:       \n",
    "\n",
    "$$Cond\\ \\# = \\frac{\\lambda_{max}}{\\lambda_{min}}$$\n",
    "\n",
    "A matrix with a condition number less than a few hunderd is considered as having a stable inverse. On the other hand, the calculation of the inverse for a matrix with a large condition number will likely be unstable.     \n",
    "\n",
    "As an example, we can compute the condition number of matrix created for the eigen-decomposition example.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the conditon number\n",
    "Cond_num = abs(eigs[0]/eigs[1])\n",
    "Cond_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e68a5",
   "metadata": {},
   "source": [
    "A concept related to the condition number is the **rank** of a matrix. The rank of a matrix is the number of non-zero eigenvalues. A **full-rank** $n \\times n$ matrix has $n$ nonzero eigenvalues. An $n \\times n$ **rank-deficient** matrix has fewer than $n$ non-zero eigenvalues. The inverse of a rank-deficient matrix does not exist.     \n",
    "\n",
    "**Example:** The $2 \\times 2$ matrix $A$ has 2 non-zero eigenvalues, and therefore has full rank.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac18637",
   "metadata": {},
   "source": [
    "> **Exercise 02-8:** Recall that in an earlier exercise you found that the inverse of the matrix $E$ could not be computed. To explore the properties of this matrix do the following:   \n",
    "> 1. Display the eigenvalues of the matrix.       \n",
    "> 2. Compute and display the condition nimber of this matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0627b5",
   "metadata": {},
   "source": [
    "> Answer these questions:    \n",
    "> 1. Is the matrix rank-deficient and why?    \n",
    "> 2. What does the rank and condition number tell you about the existance of an inverse of this matrix?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdcd6c",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.     \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786ece3",
   "metadata": {},
   "source": [
    "### Eigen-decomposition and matrix inverse  \n",
    "\n",
    "You have already seen how a square matrix can be reconstructed from the eigenvalues and eigenvectors. Similarly, the inverse of a square matrix can be computed from the eigen-decomposition as follows:        \n",
    "\n",
    "$$A^{-1} = Q \\Lambda^{-1} Q^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef7988",
   "metadata": {},
   "source": [
    "> **Exercise 02-9:** You can use the foregoing formulation to compute the inverse of the matrix $A$. Do the following:      \n",
    "> 1. Compute and print the inverse of $A$.      \n",
    "> 2. Compute and print the product of $A$ and $A^{-1}$, and verify that the result is an identity matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3402b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd90da",
   "metadata": {},
   "source": [
    "## Eigen-Decomposition of Covariance   \n",
    "\n",
    "You may be worndering how you can apply eigen-decomposition theory to real world data. The anaswer often is through the eigen-decomposition of the **covariance matrix**.     \n",
    "\n",
    "To understand the covariance matrix, consider the structure of a data table or **feature matrix**. The **features** or **variables** are represented in the columns and the **cases** or **samples** are represented in the rows.     \n",
    "\n",
    "The covariance matrix represnts a measure of the statistical properties of the data. The covariance matrix can be computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma &= \\frac{1}{n} A^T A \\\\\n",
    "Where: & \\\\\n",
    "\\Sigma &= \\frac{1}{n} \\begin{bmatrix}\n",
    "   \\sigma_{1,1} & \\sigma_{1,2} & \\ldots & \\sigma_{1,m} \\\\\n",
    "   \\sigma_{2,1} & \\sigma_{2,2} & \\ldots & \\sigma_{2,m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   \\sigma_{n,1} & \\sigma_{n,2} & \\ldots & \\sigma_{n,m}\n",
    "   \\end{bmatrix}\\\\\n",
    "   And\\  & in\\ terms\\ of\\ expectation\\ \\mathbb{E}() \\\\\n",
    "   \\sigma_{i,j} &= \\mathbb{E} \\Big[ \\big( a_{.,i} - \\mathbb{E}( a_{.,i}) \\big) \\cdot a_{.,j} - \\mathbb{E}( a_{.,j}) \\Big]\\\\\n",
    "Or, & \\\\\n",
    "   \\sigma_{i,j} &= \\frac{1}{n} \\sum_k ( a_{k,i} - \\bar{a}_{.,i} ) (a_{k,j} - \\bar{a}_{.,j}) \n",
    "\\end{align}\n",
    "\n",
    "The covariance matrix has a number of important properties including:    \n",
    "1. For an $n \\times p$ data matrix the covariance matrix is $p \\times p$.\n",
    "2. The diagonal elements of this matrix are the variances of the variables. \n",
    "3. The off-diagonal terms are a linear measure of dependency between pairs of variables. If variables have no dependency, the corresponding off diagonal term will be zero.    \n",
    "4. Covariance is symmetric and $\\sigma_{i,j} = \\sigma_{j,i}$.     \n",
    "5. The covariance estimate is mean ($\\bar{a}_{.,i}$) invariant.  \n",
    "\n",
    "> **Computational note:** We have formulated the covariance following the convention used for most data science or statistical methods. An alternative is to define the convariance matrix for an $\\n \\times p$ data matrix is $\\Sigma = \\frac{1}{p} A A^T$, which results in an $p \\times p$ covaniance matrix. This form is sometimes useful, such as when the number of columns of the data matrix is larger than the number of rows.  \n",
    "\n",
    "To illustrate the application of this theory let's work through a simple example. The code in the cell below does the following:    \n",
    "1. Define a known $2 \\times 2$ covaiance structure.   \n",
    "2. Make random draws from a multivaiate normal distribution.   \n",
    "3. Plot the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(124)\n",
    "cov = np.array([[1.0, 0.6], [0.6, 1.0]])\n",
    "mean = np.array([0.0, 0.0])\n",
    "\n",
    "sample = nr.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0bc15",
   "metadata": {},
   "source": [
    "The sampled values are form a generally eliptical cluster. The shape of this elipse reflects the covariance structure.      \n",
    "\n",
    "Next, we compute and display the covariance of these data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the covariance \n",
    "cov_sample = (1.0/len(sample)) * np.dot(np.transpose(sample), sample)\n",
    "cov_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893be0ed",
   "metadata": {},
   "source": [
    "Notice that the covariance matrix is square and we can compute the eigen-decomposition. The code in the cell below computes the eigen-decomposition, displays the results, and computes and prints the condition number.            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a458e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Eigendecomposition of covariance  \n",
    "sample_eigs, sample_Q = np.linalg.eig(cov_sample)\n",
    "print(sample_eigs)\n",
    "print('\\n')\n",
    "print(sample_Q)\n",
    "print('\\nThe condition number = {0:6.3}'.format(np.max(sample_eigs)/np.min(sample_eigs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52174fa4",
   "metadata": {},
   "source": [
    "The matrix appears to be well conditioned.    \n",
    "\n",
    "We can reconstruct the convaiance matrix from the eigendecomposition using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d67d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reconstruct the covariance matrix   \n",
    "np.dot(sample_Q, np.dot(np.diag(sample_eigs), np.transpose(sample_Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f677140",
   "metadata": {},
   "source": [
    "Given the low condition number, we can easily compute the inverse of the covariance matrix. The code in the cell below computes and displays the inverse eigenvalues and the inverse covariance matrix. Finally the inverse is tested to ensure the product of the inverse with the original covariance is an identity matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find and test the inverse   \n",
    "inv_B_eigs = np.linalg.inv(np.diag(sample_eigs))\n",
    "print(inv_B_eigs)\n",
    "print('\\nInverse Convariance matrix')\n",
    "inv_covariance = np.dot(sample_Q,(np.dot(inv_B_eigs, np.transpose(sample_Q))))\n",
    "print(inv_covariance)\n",
    "print('\\nProduct of inverse and covariance matrix')\n",
    "print(np.dot(inv_covariance, cov_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd1bc4",
   "metadata": {},
   "source": [
    "The inverse for the covariance matrix is well determined.    \n",
    "\n",
    "> **Exercise 02-10:** You will now try an example with highly colinear feature values. The code in the cell below generates multivariate Normally distributed samples with a high dependency between the variables. Execute the code in the cell below.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Increase colinearity of variables \n",
    "nr.seed(124)\n",
    "cov = np.array([[1.0, 0.997], [0.997, 1.0]])\n",
    "mean = np.array([0.0, 0.0])\n",
    "\n",
    "sample2 = nr.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "plt.scatter(sample2[:,0], sample2[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3d937",
   "metadata": {},
   "source": [
    "> In the cell below create and execute code to do the following:     \n",
    "> 1. Compute the covariance matrix of this second data sample.    \n",
    "> 2. Compute and display the eigen-decomposition of the covariance matrix.   \n",
    "> 3. Compute and display the condition number.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "## Compute covariance and eigendecompositon   \n",
    "cov_sample2 = (1.0/len(sample2)) * np.dot(np.transpose(sample2), sample2)/4\n",
    "cov_sample2\n",
    "print('\\n')\n",
    "sample2_eigs, sample2_Q = np.linalg.eig(cov_sample2)\n",
    "print(sample2_eigs)\n",
    "print('\\n')\n",
    "print(sample2_Q)\n",
    "print('\\nThe condition number = {0:6.3}'.format(sample2_eigs[1]/sample2_eigs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc25c9",
   "metadata": {},
   "source": [
    "> In the cell below create and execute code to do the following:      \n",
    "> 4. Compute and display the inverse eigenvalues.     \n",
    "> 5. Compute and display the inverse covariance matrix.  \n",
    "> 6. Compute and display the product of the inverse covariance matrix and the original covariance matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311626ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below   \n",
    "## Find and test the inverse   \n",
    "inv_B_eigs2 = np.linalg.inv(np.diag(sample2_eigs))\n",
    "print(inv_B_eigs2)\n",
    "print('\\nInverse Convariance matrix')\n",
    "inv_covariance2 = np.dot(sample2_Q,(np.dot(inv_B_eigs2, np.transpose(sample2_Q))))\n",
    "print(inv_covariance2)\n",
    "print('\\nProduct of inverse and covariance matrix')\n",
    "print(np.dot(inv_covariance2, cov_sample2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9ae39",
   "metadata": {},
   "source": [
    "> Answer the following questions:    \n",
    "> 1. How does the condition number of the new sample compare to the first one?      \n",
    "> 2. Is the inverse of this matrix well-determined and why?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25cbb5",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.         \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f8470",
   "metadata": {},
   "source": [
    "## Singlular Value Decomposition (SVD)  \n",
    "\n",
    "So far, we have worked with eigen-decomposition which only work for square matricies, such as the covariance matricies. Therefore in principle, you can therefore decompose a rectangular data matrix by computing the covariance matrix first. But, there is a direct, but approximate, method which can be applied to rectangualar matrices, **singular value decomposition** or **SVD**.  \n",
    "\n",
    "The singular value decomposition of an $n \\times p$ matrix $A$ is written:      \n",
    "\n",
    "\\begin{align}     \n",
    "A &\\simeq U S V \\\\  \n",
    "Where: & \\\\\n",
    "U &= n \\times p\\ orthonormal\\ left\\ singular\\ vectors\\\\\n",
    "S &= p \\times p\\ diagonal\\ matrix\\ of\\ singular\\ values\\\\ \n",
    "V &= p \\times p\\ orthonormal\\ right\\ singular\\ vectors\n",
    "\\end{align}\n",
    "\n",
    "Consider a simple example of the SVD decomposition of $n \\times p$ data matrix, $A$. The SVD is shown below. Pay particular attention to the dimensions of the arrays required to make the matrix product conformable.       \n",
    "\n",
    "\\begin{align}   \n",
    "\\begin{bmatrix}\n",
    "   a_{1,1} & a_{1,2}\\\\\n",
    "   a_{2,1} & a_{2,2}\\\\\n",
    "   a_{3,1} & a_{3,2}\\\\\n",
    "   a_{4,1} & a_{4,2}\n",
    "   \\end{bmatrix}\n",
    " =\n",
    "\\begin{bmatrix}\n",
    "   u_{1,1} & u_{1,2}\\\\\n",
    "   u_{2,1} & u_{2,2}\\\\\n",
    "   u_{3,1} & u_{3,2}\\\\\n",
    "   u_{4,1} & u_{4,2}\n",
    "   \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   s_1 & 0  \\\\\n",
    "   0 & s_2 \n",
    "   \\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   v_{1,1} & v_{2,1}\\\\\n",
    "   a_{1,2} & a_{2,2}\n",
    "   \\end{bmatrix}  \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The code in the cell below uses the [numpy.linalg.svd](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) function to compute the SVD of the sample data and returns the components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVD decomposition of sample  \n",
    "U, S, V = np.linalg.svd(sample2)\n",
    "U = U[:,:2]\n",
    "print('U')\n",
    "print(U)\n",
    "print('Dimensions of U = ' + str(U.shape))\n",
    "print('\\nS')\n",
    "print(np.diag(S))\n",
    "print('\\nV')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c4b61",
   "metadata": {},
   "source": [
    "> **Exercise 02-11:** As already discussed, the SVD is only an approximation. You will now determine how good the approximation is in this case. To determine how good the approximation is, do the following:     \n",
    "> 1. Compute the reconstructed data matrix using the formula given above.     \n",
    "> 2. Print the dimensions of the reconstructed matrix.   \n",
    "> 3. Compute and display the Euclidean norm of the difference between the original data matrix and the reconstructed matrix.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf107f5b",
   "metadata": {},
   "source": [
    "Finally, we can use the SVD to find an approximate inverse of the data matrix in the form shown here:       \n",
    "\n",
    "$$A^{-1} = V S^{-1} U$$  \n",
    "\n",
    "The code in the cell below computes and tests the approximation of the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788011e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test inverse of decompostion\n",
    "S_inverse = np.diag(1.0 / S)\n",
    "sample2_svd_inverse = np.dot(V, np.dot(S_inverse, np.transpose(U)))\n",
    "np.dot(sample2_svd_inverse, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07722b",
   "metadata": {},
   "source": [
    "#### Copyright 2020, 2021, 2022, 2023, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de92c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
