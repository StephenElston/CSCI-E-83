{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 25, Taming Model Behavior with Regularization   \n",
    "\n",
    "### Steve Elston\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "**Overfitting** is a constant danger with machine learning models. Overfit models fit the training data well. However, an overfit model will not **generalize**. A model that generalizes is a model which exhibits good performance on data cases beyond the ones used in training. Models that generalize will be useful in production. \n",
    "\n",
    "As a general rule, an overfit model has learned the training data too well. The overfitting likely involved learning noise present in the training data. The noise in the data is random and uninformative. When a new data case is presented to such a model it may produce unexpected results since the random noise will be different. \n",
    "\n",
    "So, what is one to do to prevent overfitting of machine learning models? The most widely used set of tools for preventing overfitting are known as **regularization methods**. Regularization methods take a number of forms, but all have the same goal, to prevent overfitting of machine learning models. \n",
    "\n",
    "Regularization is not free however. While regularization reduces the **variance** in the model results, it introduces **bias**. Whereas, an overfit model exhibits low bias the variance is high. The high variance leads to unpredictable results when the model is exposed to new data cases. On the other hand, the stronger the regularization of a model the lower the variance, but the greater the bias. This all means that when applying regularization you will need to contend with the **bias-variance trade-off**. \n",
    "\n",
    "To better understand the bias variance trade-off consider the following examples of extreme model cases:\n",
    "\n",
    "- If the prediction for all cases is just the mean (or median), variance is minimized. The estimate for all cases is the same, so the bias of the estimates is zero. However, there is likely considerable variance in these estimates. \n",
    "- On the other hand, consider what happens when the data are fit with a kNN model with k=1. The training data will fit this model perfectly, since there is one model coefficient per training data point. The variance will be low. However, the model will have considerable bias when applied to test data. \n",
    "\n",
    "In either case, these extreme models will not generalize well and will exhibit large errors on any independent test data. Any practical model must come to terms with the trade-off between bias and variance to make accurate predictions. \n",
    "\n",
    "To better understand this trade-off you can consider the example of the mean square error, which can be decomposed into its components. The mean square error can be written as:\n",
    "\n",
    "$$\\Delta x = E \\Big[ \\big(Y - \\hat{f}(X) \\big)^2 \\Big] = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat{f}(x_i) \\big)^2 $$\n",
    "\n",
    "Where,\n",
    "$Y = $ the label vector.  \n",
    "$X = $ the feature matrix.   \n",
    "$\\hat{f}(x) = $ the trained model.   \n",
    "\n",
    "Expanding the representation of the mean square error:\n",
    "\n",
    "$$\\Delta x = \\big( E[ \\hat{f}(X)] - \\hat{f}(X) \\big)^2 + E \\big[ ( \\hat{f}(X) - E[ \\hat{f}(X)])^2 \\big] + \\sigma^2\\\\\n",
    "\\Delta x = Bias^2 + Variance + Irreducible\\ Error$$\n",
    "\n",
    "The relationship between bias and variance is illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/BiasVariance.png\" alt=\"Drawing\" style=\"width:600px; height:400px\"/>\n",
    "<center> Trade-off between bias and variance for machine learning model <center>      \n",
    "    \n",
    "\n",
    "Study this relationship. Notice that as regularization reduces variance, bias increases. The irreducible error will remain unchanged. Regularization parameters are chosen to minimize $\\Delta x$. In many cases, this will prove challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split the Dataset\n",
    "\n",
    "With the above bit of theory in mind, it is time to try an example. In this example you will compute and compare linear regression models using different levels and types of regularization. \n",
    "\n",
    "Execute the code in the cell below to load the packages required for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf  \n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import sklearn.metrics as sklm\n",
    "from patsy import dmatrices\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the following to load and prepare the data set:    \n",
    "1. Load the data set.   \n",
    "2. Scale the numeric columns except the columns used as labels for the examples. \n",
    "3. Split the 195 cases into 100 training cases and 95 test cases.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Load the data frame   \n",
    "auto_data = pd.read_csv('../data/AutoPricesClean.csv')\n",
    "\n",
    "## Remove unwanted columns and normalize numeric features   \n",
    "auto_data.drop(auto_data.columns[:3], axis=1, inplace=True)\n",
    "numeric_columns = [col for col_type,col in zip(auto_data.iloc[:,:-3].dtypes,auto_data.iloc[:,:-3].columns) if col_type in ['int64','float64']]\n",
    "auto_data.loc[:,numeric_columns] = StandardScaler().fit_transform(auto_data.loc[:,numeric_columns])\n",
    "\n",
    "## Create a mask and use it to split the data into a train and test set   \n",
    "nr.seed(6665)\n",
    "mask = nr.choice(auto_data.index, size = 100, replace=False)\n",
    "auto_data_train = auto_data.iloc[mask,:]\n",
    "auto_data_test = auto_data.drop(mask, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first linear regression model\n",
    "\n",
    "First you will create a model 15 features or variables and no regularization. In the terminology used before this model has high variance and low bias. In other words, this model is over-fit and provides a baseline for comparison for regularized models. \n",
    "\n",
    "The code in the cell below should be familiar. In summary, it performs the following processing:\n",
    "1. Define and train the linear regression model using the training features and labels.\n",
    "2. Display the summary of the model. \n",
    "3. Plot a histogram of the residuals of the model using the test partition.\n",
    "4. Plot a Q-Q Normal plot of the residuals of the model using the test partition.\n",
    "5. Plot the residuals of the model vs. the predicted values using the test partition. \n",
    "6. Compute and display key performance metrics for the model.     \n",
    "\n",
    "Execute this code and examine the results for the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'city_mpg ~ C(fuel_type) + C(body_style) + C(aspiration) + C(drive_wheels) + C(engine_location)\\\n",
    "          + curb_weight + C(engine_type) + engine_size'\n",
    "#formula = 'log_price ~ fuel_type + aspiration + drive_wheels + curb_weight + engine_size' # + bore + stroke + compression_ratio + horsepower + peak_rpm'\n",
    "\n",
    "base_model = smf.ols(formula, data=auto_data_train).fit()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the adjusted $R^2$ and F-statistic, this model seems to do a reasonable job of explaining the variance of the label. However, it is clear from the confidence intervals and p-values of the coefficients that this model is over-fit.   \n",
    "\n",
    "Next, execute the code below to examine the distribution of residuals and the residuals plotted against the predicted values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def residual_plot(df):\n",
    "    plt.rc('font', size=12)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), ) \n",
    "    RMSE = np.std(df.resids)\n",
    "    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);\n",
    "    plt.axhline(0.0, color='red', linewidth=1.0);\n",
    "    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);\n",
    "    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);\n",
    "    plt.title('PLot of residuals vs. predicted');\n",
    "    plt.xlabel('Predicted values');\n",
    "    plt.ylabel('Residuals');\n",
    "    plt.show()\n",
    "\n",
    "def plot_resid_dist(df):\n",
    "    resids = df.loc[:,'resids']\n",
    "    plt.rc('font', size=12)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3));\n",
    "    ## Plot a histogram\n",
    "    sns.histplot(x=resids, bins=20, kde=True, ax=ax[0]);\n",
    "    ax[0].set_title('Histogram of residuals');\n",
    "    ax[0].set_xlabel('Residual values');\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1]);\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals');\n",
    "    plt.show();    \n",
    "\n",
    "def compute_residuals(df, model, label_col='city_mpg'):\n",
    "    df['predicted'] = model.predict(df)\n",
    "    df['resids'] = np.subtract(df.loc[:,'predicted'],df.loc[:,label_col]) \n",
    "    return df\n",
    "    \n",
    "\n",
    "auto_data_train = compute_residuals(auto_data_train, base_model)\n",
    "plot_resid_dist(auto_data_train)\n",
    "residual_plot(auto_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall these results are reasonably good. The distribution of the residuals is a bit skewed toward the negative. Further, the residuals are approximately homoskedastic. But residuals are distinctly left-skewed.    \n",
    "\n",
    "### Testing the Box-Cox transform   \n",
    "\n",
    "We might be able to improve these results using the **Box-Cox transform** of the label column `city_mpg`. The goal of the Box-Cox transform is to transform the distribution of the label values (dependent variable) to be closer to Normal. The residuals of the resulting model should also be closer to Normally distributed, a key assumption of the least-squares method.       \n",
    "\n",
    "The Box-Cox transform is a **power law transform**, related to the **logarithmic transform**. The Box-Cox transform finds a power, $\\lambda$, that creates a transform of the values to be as close to Normal as possible. The Box-Cox transform can be written:      \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  y = \\left \\{\n",
    "  \\begin{aligned}\n",
    "    &(x^\\lambda - 1) / \\lambda, && \\lambda \\ne 0 \\\\\n",
    "    &log(x), && \\lambda = 0\n",
    "  \\end{aligned} \\right.\n",
    "\\end{equation} \n",
    "\n",
    "The value of $\\lambda$ can be positive or negative, given the skew of the original distribution. Notice that for the case of $\\lambda = 0$ the Box-Cox transform is a logarithmic transform. This means that the Box-Cox transform is only defined for label values > 0.     \n",
    "\n",
    "The code in the cell below applies the Box-Cox transform to the label column and prints the power, $\\lambda$, computed. The code uses the [boxcox](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) function from scipy.stats. The data sample split again into training and test subsets. A new OLS model is computed and the model summary is printed and the diagnostic plots are displayed. Execute this code and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Box-Cox transform to the label and print the power used   \n",
    "auto_data.loc[:,'city_mpg'], power = ss.boxcox(auto_data.loc[:,'city_mpg'])\n",
    "print('The power of the Box-Cox transform = {0:6.3f}'.format(power))\n",
    "\n",
    "## Split the data using the transformed label values\n",
    "## Create a mask and use it to split the data into a train and test set   \n",
    "nr.seed(654566)\n",
    "mask = nr.choice(auto_data.index, size = 100, replace=False)\n",
    "auto_data_train = auto_data.iloc[mask,:]\n",
    "auto_data_test = auto_data.drop(mask, axis=0) \n",
    "\n",
    "## Compute the model using the transformed label and display the summary\n",
    "base_model = smf.ols(formula, data=auto_data_train).fit()\n",
    "print(base_model.summary())\n",
    "\n",
    "## Display diagnostics using the training data\n",
    "auto_data_train = compute_residuals(auto_data_train, base_model)\n",
    "plot_resid_dist(auto_data_train)\n",
    "residual_plot(auto_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\lambda$ is very close to 1.0, indicating the Box-Cox transform had minimal effect on the distribution of the label values. None-the-less, the values of adjusted $R^2$, the F-statistic and the log-likelihood have all improved. The distribution of the residuals has indeed changed, but is still noticeably non-Normal in the tail. Given the improved least-squares fit, we will continue with the transformed label.   \n",
    "\n",
    "\n",
    "The code in the cell below computes and prints some common evaluation metrics. In addition plots of the model fit are displayed. The calculations are performed on the test data subset. Execute the code below and examine the results.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_predicted):\n",
    "    ## Compute the usual metrics\n",
    "    mse = sklm.mean_squared_error(y_true, y_predicted)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = sklm.median_absolute_error(y_true, y_predicted)\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def print_metrics(df_test, model, label_col='city_mpg'): \n",
    "    df_test['predicted'] = model.predict(df_test)\n",
    "    mse, rmse, mae = compute_metrics(df_test.loc[:,label_col],df_test.loc[:,'predicted'])   \n",
    "    print('MSE  = {0:6.3f}'.format(mse))\n",
    "    print('RMSE = {0:6.3f}'.format(rmse))\n",
    "    print('MAE  = {0:6.3f}'.format(mae))\n",
    "\n",
    "    \n",
    "print_metrics(auto_data_test, base_model)   \n",
    "auto_data_test = compute_residuals(auto_data_test, base_model)\n",
    "plot_resid_dist(auto_data_test)\n",
    "residual_plot(auto_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these metrics as a basis of comparison with regularized models.  \n",
    "\n",
    "The distribution of the residuals show a bit of kurtosis (heavy trails), but are close to Normally distributed. Further, the residuals are approximately homoskedastic.  \n",
    "\n",
    "> **Note:** No one regularization method works in all cases. In the running example you will see deviations from ideal behavior. Do not be surprised if not all methods improve the models. Further, do not generalize the behavior you observe in the exercises to other models. The effectiveness of regularization method is problem and model dependent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply l2 regularization\n",
    "\n",
    "Now, you will apply **l2 regularization** to constrains the model parameters. Constraining the model parameters prevent over-fitting of the model. This method is also known as **Ridge Regression**. \n",
    "\n",
    "But, how does this work? l2 regularization applies a **penalty** proportional to the **l2** or **Euclidean norm** of the model weights to the loss function. For linear regression using squared error as the metric, the total **loss function** is the sum of the squared error and the regularization term. The total loss function can then be written as follows:  \n",
    "\n",
    "$$J(\\beta) = ||A \\beta - b||^2 + \\alpha ||\\beta||^2$$\n",
    "\n",
    "Where the penalty term on the model coefficients, $\\beta_i$, based on the Euclidean norm:\n",
    "\n",
    "$$\\alpha || \\beta||^2 = \\alpha \\big(\\beta_1^2 + \\beta_2^2 + \\ldots + \\beta_n^2 \\big)^{\\frac{1}{2}} = \\alpha \\Big( \\sum_{i=1}^n \\beta_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We call $||\\beta||^2$ the **l2 norm** of the coefficients, since we raise the weights of each coefficient to the power of 2, sum the squares and then raise the sum to the power of $\\frac{1}{2}$. \n",
    "\n",
    "You can think of this penalty as constraining the 12 or Euclidean norm of the model weight vector. The value of $\\alpha$ determines how much the norm of the coefficient vector constrains the solution. You can see a geometric interpretation of the l2 penalty constraint in the figure below.  \n",
    "\n",
    "<img src=\"../images/L2.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n",
    "<center>Geometric view of l2 regularization\n",
    "\n",
    "Notice that for a constant value of the l2 norm, the values of the model parameters $B_1$ and $B_2$ are related. The Euclidean or l2 norm of the coefficients is shown as the dotted circle. The constant value of the l2 norm is a constant value of the penalty. Along this circle the coefficients change in relation to each other to maintain a constant l2 norm. For example, if $B_1$ is maximized then $B_2 \\sim 0$, or vice versa. It is important to note that l2 regularization is a **soft constraint**. Coefficients are driven close to, but likely not exactly to, zero.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Eigenvalue Decomposition\n",
    "\n",
    "**Eigenvalues** are characteristic roots or characteristic values of a linear system of equations. The **eigenvalue-eigenvector** decomposition is a factorization of the a matrix. \n",
    "\n",
    "Let's start with a **square matrix**, $A$:\n",
    "\n",
    "$$A = \n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next define a vector, $x$: \n",
    "\n",
    "$$x = \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then an **eigenvalue** of the matrix $A$ has the property: \n",
    "\n",
    "$$A x = \\lambda x$$\n",
    "\n",
    "Or,   \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\lambda \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To see that the eigenvalue, $\\lambda$, is a root of the matrix, $A$ we can rearrange the above as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "Ax - \\lambda x &= 0 \\\\\n",
    "(A - I \\lambda) x &= 0\n",
    "\\end{align}\n",
    "\n",
    "Where, $I$ is the **identity matrix** of 1 on the diagonal and 0 elsewhere. These relationships can be written as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11} - \\lambda  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} - \\lambda  & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn} - \\lambda \n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The foregoing show that the eigenvalue, $\\lambda$, is a root of the matrix, $A$. Further, notice that the eigin \n",
    "\n",
    "For an $n\\ x\\ n$ matrix, $A$, there are $n$ eigenvalues or roots. These can be found by solving the following equation, using the determinant:  \n",
    "\n",
    "$$det(A - x) = 0$$\n",
    "\n",
    "You can find more information on the determinant in this [article](https://en.wikipedia.org/wiki/Determinant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Eigenvalues and the Normal Equations\n",
    "\n",
    "Let's start by examining the **normal equation** formulation of the linear regression problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ and a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "The commonly used normal equation form can help with this problem:\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is a symmetric $m x m$ covariance matrix, where $m$ is the number of model coefficients. This is a significant reduction in size when compared to $A$. \n",
    "\n",
    "Now, we can perform eigenvalue-eigenvector decomposition of $A^TA$:\n",
    "\n",
    "$$A^TA = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$Q = $ unitary matrix of orthonormal **eigenvectors**, and\n",
    "$\\Lambda =$ diagonal matrix of **eigenvalues**. The eigenvalue matrix is diagonal:  \n",
    "\n",
    "$$\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "    \\lambda_1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\lambda_2 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\lambda_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Since Q is unitary (unit norm), the inverse of $A^TA$ is easily computed:\n",
    "\n",
    "$$(A^TA)^{-1} = Q \\Lambda^{-1} Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$$\\Lambda^{-1} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_n}\n",
    "\\end{bmatrix}$$\n",
    "and $\\lambda_i$ is the ith eigenvalue. \n",
    "\n",
    "But, **$A^TA$ can still be rank deficient!** By rank deficient we mean that there are fewer non-zero eigenvalues of $A^TA$ than the dimension, $n$. Even is the ith eigenvalue is close to zero, $\\frac{1}{\\lambda_i}$ becomes very large and destabilizes the inverse. \n",
    "\n",
    "The basic idea of $l_2$ regularization, Tikhonov regularization, or ridge regression is to stabilize the inverse eigenvalue matrix,$\\Lambda$, by **adding a small bias term**, $\\alpha$, to each of the eigenvalues. We can write this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel + \\parallel \\alpha^2 \\cdot I \\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\alpha^2 \\cdot I)^{-1}A^Tx$$\n",
    "\n",
    "In this way, the inverse values of small eigenvalues do not blow up when we compute the inverse. You can see this by writing out the $\\Lambda^+$ matrix with the bias term.\n",
    "\n",
    "$$\\Lambda_{Tikhonov}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1 + \\alpha^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2 + \\alpha^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_m + \\alpha^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term ensures there are no non-zero eigenvalues, and that the inverse of $A^TA$ exists. You can also see that added the bias term adds a 'ridge' along the diagonal of the eigenvalue matrix, giving this method one of its names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian Interpretation\n",
    "\n",
    "Another way to view l2 regularization is using a Bayesian formulation of the problem. Let's start with the posterior distribution of the weight vector $W$ given the features $X$ and labels $Y$. Using Bayes rule we can write this posterior distribution as:\n",
    "\n",
    "$$p( W\\ |\\ \\{X,Y\\} ) = \\frac{p(W)\\ p(\\{X,Y\\}\\  |\\  W\\ )}{p( \\{X,Y\\})}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$p(W) = $ the prior distribution of $W$.   \n",
    "$p(\\{X,Y\\}\\  |\\  W\\ ) = $ the likelihood of $\\{X,Y\\}$ given $W$.\n",
    "\n",
    "We want the **maximum a posteriori** or **MAP** of the weights, $W$. Taking the log of both sides:\n",
    "\n",
    "$$max_W log \\big( p( W\\ |\\ \\{X,Y\\} ) \\big) \\propto max_W\\ \\Big[ log \\big( p(W) \\big)\\  + log \\big( p(\\{X,Y\\}\\  |\\  W\\ ) \\big) \\Big]\\\\\n",
    "= max_W \\Big[ log\\big(prior(W)\\big) + log \\big( likelihood(\\{X,Y\\}\\  |\\  W\\ ) \\big) \\Big]$$\n",
    "\n",
    "For a Gaussian process with l2 loss and l2 regularization we formulate the problem:\n",
    "\n",
    "$$max_W log \\big( p( W\\ |\\ \\{X,Y\\} ) \\big) = max_W \\Big[\\frac{1}{n} \\sum_{i=1}^n (f_W(x_i) - y_i)^2 + \\lambda || W ||^2 \\big) \\Big]$$  \n",
    "\n",
    "where,   \n",
    "$\\frac{1}{n} \\sum_{i=1}^n (f_W(x_i) - y_i)^2 = $ the likelihood.    \n",
    "$\\lambda || W ||^2 = $ the prior acting as a regularization penalty. \n",
    "\n",
    "\n",
    "So how are we to interpret this prior? It is useful to view the prior as constraining the norm of the weight vector close to zero. In other words, we think of the weights as **shrinking** toward zero. Thus, the shrinkage process prevents the weights from reaching extreme values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example    \n",
    "\n",
    "How can we use the eigenvalue decomposition of the covariance matrix to better understand how stable a linear model is? One way to summarize stability is to compute the **condition number**, which is the ratio of the largest to the smallest eigenvalue:   \n",
    "\n",
    "$$condition\\ number = \\frac{largest\\ eigenvalue}{smallest\\ eigenvalue}$$   \n",
    "\n",
    "A linear model with a covariance matrix having a large condition number is unstable and the model coefficients will be poorly determined. In other words, when the condition number is large, we can expect high variance and poor generalization. As a rule of thumb, the condition number of a **well-posed** linear model should be less than about 100. Otherwise, we say the model is **ill-posed**.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-1:**  You will now compute the compute the covariance matrix for the model specified above and evaluate its eigenvalues.     \n",
    "> 1. Construct the design (model) matrix using the training data subset. You can do this with the [patsy.dematrix](https://patsy.readthedocs.io/en/latest/API-reference.html) function.      \n",
    "> 2. Use [numpy.transpose](https://numpy.org/devdocs/reference/generated/numpy.transpose.html) and [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) functions to compute the covariance matrix of the design matrix. Make sure you normalize by dividing by the dimension of the covariance matrix!     \n",
    "> 3. Compute the [numpy.linalg.eigvals](https://numpy.org/devdocs/reference/generated/numpy.linalg.eigvals.html) function to compute the eigenvalues. Save the real part of these complex numbers using [numpy.real](https://numpy.org/devdocs/reference/generated/numpy.real.html) and print the result.   \n",
    "> 4. Compute and print the condition number.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does this condition number tell you about the stability of the coefficient estimates for this model? Is this consistent with what you learned from the model summary?   \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of ridge regression   \n",
    "\n",
    "When L2 regularization is applied to regression the resulting algorithm is often referred to as **ridge regression**. For this algorithm, the regularization parameter, $\\alpha$, must be selected. For a given problem, there is are limited theory-based options to find the best value of $\\alpha$, Therefore, one typically resorts to a **hyperparameter** search. \n",
    "\n",
    "There are several possible approaches to hyperparameter search. An error metric, such as MSE or MAE is used to determine the optimal value.    \n",
    "\n",
    "The simplest approach is to search of grid (or line) of regularly spaced hyperparameter values. At each gird point the model performance metric is computed. The best model is considered the one with the best metric. We will use this simpler approach here.       \n",
    "\n",
    "An alternative is to randomly sample the space of hyperparameter values. This later approach is more efficient.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-2:** You will now perform a search to find a best regularization hyperparameter by the following steps:      \n",
    "> 1. Complete the code in the `regularized_coefs` function to compute a regularized OLS model using the value of alpha and L1_wt. For each value of alpha searched, this model will be computed and evaluated.      \n",
    "> 2. In the space provided below the functions create an array of alpha values from 0.0 to 0.005 in steps of 0.00005.  \n",
    "> 3. Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_coefs(df_train, df_test, alphas, L1_wt=0.0, n_coefs=15,\n",
    "                      formula = formula, label='city_mpg'):\n",
    "    '''Function that computes a linear model for each value of the regualarization \n",
    "    parameter alpha and returns an array of the coefficient values. The L1_wt \n",
    "    determines the trade-off between L1 and L2 regualarization'''\n",
    "    coefs = np.zeros((len(alphas),n_coefs + 1))\n",
    "    MSE_train = []\n",
    "    MSE_test = []\n",
    "    for i,alpha in enumerate(alphas):\n",
    "        ## First compute the training MSE\n",
    "        #### Complete the line of code below\n",
    "        temp_mod = \n",
    "        #############\n",
    "        \n",
    "        coefs[i,:] = temp_mod.params\n",
    "        MSE_train.append(sqrt(np.mean(np.square(df_train[label] - temp_mod.predict(df_train)))))\n",
    "        ## Then compute the test MSE\n",
    "        MSE_test.append(sqrt(np.mean(np.square(df_test[label] - temp_mod.predict(df_test)))))\n",
    "        \n",
    "    return coefs, MSE_train, MSE_test\n",
    "\n",
    "\n",
    "def plot_coefs(coefs, alphas, MSE_train, MSE_test, ylim=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis\n",
    "    for i in range(coefs.shape[1]): # Iterate over coefficients\n",
    "        ax[0].plot(alphas, coefs[:,i])\n",
    "    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)\n",
    "    ax[0].set_ylabel('Partial slope values')\n",
    "    ax[0].set_xlabel('alpha')\n",
    "    ax[0].set_title('Parial slope values vs. regularization parameter number')\n",
    "    if ylim is not None: ax[0].set_ylim(ylim)\n",
    "    \n",
    "    ax[1].plot(alphas, MSE_train, label='Training error')\n",
    "    ax[1].plot(alphas, MSE_test, label='Test error')\n",
    "    ax[1].set_ylabel('Root mean squared error')\n",
    "    ax[1].set_xlabel('alpha')\n",
    "    ax[1].set_title('RMSE vs. regularization parameter number')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these plots. Notice how the training error increases and the model coefficients decrease in value with increasing regularization hyperparameter. This is expected, since as the coefficient values of the model are forced toward 0, the training bias increases. Notice however, the behavior of the MSE for the test data. Approximately at which value is the test MSE minimized?             \n",
    "> **End of exercise.**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-3:** Now you will evaluate the L2 regularized model using a value of $\\alpha^2 = 0.0005$. This is value of $\\alpha$ provides only mild regularization and will provide a basis for comparison with subsequent models.       \n",
    "> 1. Compute a regularized OLS model using the training data.     \n",
    "> 2. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 3. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions.   \n",
    "> 1. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse? Keeping in mind that the model is the product of a least squares fit, do the results make sense?   \n",
    "> 2. How does the distribution of the residuals compare to those of the unregularized models in terms of changes of skewness and the outlier?\n",
    "> 3. Do the residuals still appear approximately homoskedastic? \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-4:** You will now compare the condition number of the regularized covariance matrix to the unregularized covariance matrix you computed earlier.     \n",
    "> 1. Add a matrix with the value of the square root of the optimal $alpha$ value estimated along the diagonal (0 elsewhere) to the covariance matrix you computed in Exercise 25-2. Use [numpy.diag]() to instantiate the diagonal matrix.    \n",
    "> 2. Compute and display the eigenvalues of the regularized covariance matrix.  \n",
    "> 3. Compute and display the condition number of the regularized covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the condition numbers you have computed for the regularized and unregularized condition number. Has regularization made a significant difference? Does the regularized model still appear to have an undesirably large condition number?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply l1 regularizaton\n",
    "\n",
    "Regularization can be performed using norms other than l2. The **l1 regularizaton** or **Lasso**  method limits the sum of the absolute values of the model coefficients. The l1 norm is sometime know as the **Manhattan norm**, since distance are measured as if you were traveling on a rectangular grid of streets. This is in contrast to the l2 norm that measures distance 'as the crow flies'. \n",
    "\n",
    "We can compute the l1 norm of the model coefficients as follows:\n",
    "\n",
    "$$||\\beta||^1 = \\big( |\\beta_1| + |\\beta_2| + \\ldots + |\\beta_n| \\big) = \\Big( \\sum_{i=1}^n |\\beta_i| \\Big)^1$$\n",
    "\n",
    "where $|\\beta_i|$ is the absolute value of $\\beta_i$. \n",
    "\n",
    "Notice that to compute the l1 norm, we raise the sum of the absolute values to the first power.\n",
    "\n",
    "As with l2 regularization, for l1 regularization, a penalty term is multiplied by the l1 norm of the model coefficients. A penalty multiplier, $\\lambda$, determines how much the norm of the coefficient vector constrains values of the weights. The complete loss function is the sum of the squared errors plus the penalty term which becomes: \n",
    "\n",
    "$$J(\\beta) = ||A \\beta - b||^2 + \\lambda ||\\beta||^1$$\n",
    "\n",
    "You can see a geometric interpretation of the l1 norm penalty in the figure below.  \n",
    "\n",
    "<img src=\"../images/L1.jpg\" alt=\"Drawing\" style=\"width:700px; height:400px\"/>\n",
    "<center> Geometric view of L1 regularization\n",
    "\n",
    "The l1 norm is constrained by the sum of the absolute values of the coefficients. This fact means that values of one parameter highly constrain another parameter. The dotted line in the figure above looks as though someone has pulled a rope or lasso around pegs on the axes. This behavior leads the name lasso for l1 regularization.  \n",
    "\n",
    "Notice that in the figure above that if $B_1 = 0$ then $B_2$ has a value at the limit, or vice versa. In other words, using a l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values. Thus, you can think of the l1 norm constraint **knocking out** some weights free the model altogether. In contrast to l2 regularization, l1 regularization does drive some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-5:** Continuing with the running example you will now apply L1 regularization to the model.    \n",
    "> 1. Create an array with values of $\\alpha$ from 0.0 to 0.02 in steps of 0.0002.   \n",
    "> 2. Using the hyperparameter search function you created for exercise 25-2 compute the model performance metrics for each value of $\\alpha$ and with `L1_wt` set to 1.0; all weight on L1 regularization. It will help your understanding to set `ylim=[-0.3,0.3]` for the `plot_coefs` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these plots. Notice how the training error increases and the model coefficients decrease in value with increasing regularization hyperparameter. This is expected, since as the coefficient values of the model are forced toward 0, the training bias increases. Notice the behavior of the MSE for the test data. Where is the minimum? What does this tell you about the bias-variance trade-off.             \n",
    "> **End of exercise.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-6:** Now you will evaluate the L1 regularized model using the optimal a value $\\alpha$ where there is a minimum in the test error curve.      \n",
    "> 1. Compute a regularized OLS model using the training data and your estimate of the optimal value of $\\alpha = 0.003$ and `L1_wt=1.0`.     \n",
    "> 2. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 3. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.   \n",
    "> 4. Print the model coefficients. These coefficients are the `params` attribute of the model object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions.   \n",
    "> 1. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse? \n",
    "> 2. How does the distribution of the residuals compare to those of the unregularized models in terms of changes of skewness, kurtosis and the outlier?\n",
    "> 3. Do the residuals still appear approximately homoskedastic?    \n",
    "> 4. Examine the model coefficients, noticing that some are 0.0 as expected with L1 regularization. What does this tell you about the usefulness of some of the model features, particularly the numeric features?    \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regularization    \n",
    "\n",
    "We have now examined a bit of theory and examples of L2 and L1 regularization. We can compare the characteristics of these methods as follows:  \n",
    "\n",
    "- L2 regularization works well for **colinear features**    \n",
    "   - Down-weights colinear features   \n",
    "   - But soft constraint so poor model selection \n",
    "\n",
    "- L1 regularization provides **good model selection** by hard constraint    \n",
    "   - But poor selection for colinear features     \n",
    "\n",
    "But, we do not always have to choose between soft constraint of L2 and hard constraint of L1. The **elastic net regularization** combines the behavior of both methods. The loss function for elastic net is expressed:         \n",
    "\n",
    "$$min \\Big[ \\parallel A \\cdot x - b \\parallel +\\ \\lambda\\ \\alpha \\parallel b\\parallel^1 +\\ (1- \\lambda)\\ \\alpha \\parallel b\\parallel^2 \\Big]$$     \n",
    "\n",
    "This model has two hyperparameters:    \n",
    "- $\\lambda$ weights L1 vs. L2 regularization.      \n",
    "- $\\alpha$ sets strength of regularization.   \n",
    "\n",
    "Tuning this model, requires a 2-dimensional hyperparameter search. This search can be done on a grid or by random sampling, as was discussed previously.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-7:**  Continuing with the running example you will now apply elastic net regularization to the model.    \n",
    "> 1. Create an array with values of $\\alpha$ from 0.0 to 0.02 in steps of 0.0002.   \n",
    "> 2. Using the hyperparameter search function you created for exercise 25-2 compute the model performance metrics for each value of $\\alpha$ and with `L1_wt` set to 0.5. It will help your understanding to set `ylim=[-0.5,0.5]` for the `plot_coefs` function.       \n",
    "> In this case, we equal weight L2 and L1 regularization in order to simplify the hyperparameter search. Performance could possibly improved if a 2-hyperparameter search was performed.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One again, the curve of training error does not have a well defined minimum, except at $\\alpha = 0$. Is there any well defined minimum for the test error? Based on this behavior, do you expect that elastic net regularization to improve model generalization?      \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 25-8:** Now you will evaluate the elastic net regularized model using an arbitrary value of $\\alpha$.      \n",
    "> 1. Compute a regularized OLS model using the training data an estimate of the optimal value of $\\alpha = 0.0015$ and `L1_wt=0.5`, putting equal weight on L2 and L1.     \n",
    "> 2. Compute and print the MSE, RMSE and MAE for the model, using the test data.   \n",
    "> 3. Compute the residuals, using the test data, and display distribution plots and the plot of residuals vs. predicted values.   \n",
    "> 4. Print the model coefficients. These coefficients are the `params` attribute of the model object.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions.   \n",
    "> 1. Compare the RMSE and MAE of the regularized model to the same metrics for the unregularized model. In terms of which of these metrics is the regularized model better and worse? \n",
    "> 2. Do the residuals still appear approximately homoskedastic?    \n",
    "> 3. Examine the model coefficients? Are any of the coefficients 0? Is this behavior expected from the soft constraint of L2 regularization or the hard constraint of L1 regularization.        \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have explored the basics of regularization. Regularization can prevent machine learning models from being overfit. Regularization is required to help machine learning models generalize when placed in production. Selection of regularization strength involves consideration of the bias-variance trade-off. \n",
    "\n",
    "L2 and l1 regularization constrain model coefficients to prevent overfitting. L2 regularization constrains model coefficients using a Euclidian norm. L2 regularization can drive some coefficients toward zero, usually not to zero. On the other hand, l1 regularization can drive model coefficients to zero.    \n",
    "\n",
    "The elastic net algorithm provides weighted behavior between the L1 and L2 methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021 Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
