---
title: "Review of Probability"
author: "Steve Elston"
date: "09/18/2023"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```


------------------------------------------------------

## Importance of Probability Theory

- Probability theory is the basis of statistics, machine learning, and much AI     

- An understanding of probability theory is an important foundation to understand these methods    

- In this lesson we will review some basic concepts    
  - Properties of probability distributions   
  - Some commonly used probability distributions - focus on difficult to understand properties   
  
- Many texts provide comprehensive introductions to probability theory   


-----------------------------------------------------------------

## Probability Theory Has a Long History     

- First probability textbook by Jacob Bernoulli, published posthumously in 1713 

```{r Brenoulli_Book, out.width = '25%', fig.cap='First probability textbook\n Credit, Wikipedia commons', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Ars.jpg"))
```

- In the 21st Century, probability theory is an active area of research       


-------------------------------------------


## Probability Distributions     

**Probability distributions** are models for uncertainty of **random variables**     

- A random variable is any **mapping**, $X$, from from some outcome of a random event, $\omega$, to a real number, $\mathbb{R}$:

$$X(\omega) \rightarrow \mathbb{R}$$

- **Example:** The mapping can be a count   

- **Example:** The function maps $\omega$ to a real number, $\mathbb{R}$   

- This concept appears abstract at first glance, but is fundamental to the theory of probability    
- We will see many examples in this course
      
  
  
-----------------------------------------------

## Two Types of Probability Distributions   

- **Discrete**   
  - Model countable events     
  - Examples: count of people making a purchase, number of patients with disease,....
  - Characterized by a **probability mass function (PMF)**  

- **Continuous**    
  - Examples: temperature, velocity, price,.......
  - Characterized by a **probability density function (PDF)**  


-----------------------------------------

## Axioms of Probability         

For discrete distributions, we can speak of a **set of events** within the **sample space** of all possible events       



1. Probability for any set of events, A, is greater than 0 and less than or equal to 1    

$$0 \le P(A) \le 1 $$


2. The sum of the probability mass functions over the sample space must add to 1

$$P(S) = \sum_{a_i \in A}P(a_i) = 1 $$


3. If sets of events A and B are mutually exclusive, then the probability of either A and B is the probability of A plus the probability of B

$$P(A\ \cup B) = P(A) + P(B)\\ if\ A \perp B$$

------------------------------------

## Axioms of Probability   

From these three axioms we can draw some useful conclusions     

- Events which cannot occur have probability 0     

- Events that must occur have probability 1    

- Events must have a probability mass function between 0 and 1     


----------------------------------------------------------

## What do you expect: discrete distributions

What value we should expect to find when we sample a random variable?     

- This is the **expected value** or simply the **expectation**   

- For $n$ samples, $\mathbf{X} = x_1, x_2, \ldots, x_n$, of a random variable probability mass function $p(x_i)$ the expected value is:  

$$\mathrm{E}[\mathbf{X}] = \sum_{i=1}^n x_i\ p(x_i)$$

How can we interpret expectation?   

- Expectation is a probability weighted sum of the sample of the random variable $\mathbf{X}$   

- By the second axiom of probability the weights must sum to 1.0  


-----------------------------------------------

## Properties of Expectation     

Useful properties of expectation   

1. The relationship is linear in probability   

2. The expectation of the sum of two random variables, $X$ and $Y$, is the sum of the expectations:

$$\mathrm{E}[\mathbf{X + Y}] = \mathrm{E}[\mathbf{X}] + \mathrm{E}[\mathbf{Y}]$$

3. The expectation of an **affine transformation** of a random variable, $X$, is an **affine transformation** of the expectation:

$$\mathrm{E}[\mathbf{a\ X + b}] = a\ \mathrm{E}[\mathbf{X}] + b$$

----------------------------------------------------------------------

## Axioms of probability for continuous distributions

**Axioms of probability** for continuous probability density function, $f(x)$

1. On the interval, $\{ x_1, x_2 \}$, $P(x)$, must be bounded by 0 and 1:

$$0 \le \int_{x_1}^{x_2} f(x) dx\ \le 1$$

Note: if $x_1 = x_2$ the integral is 0

2. The area under the entire PDF over the limits must be equal to 1:   

$$\int_{lower}^{upper} f(x) dx = 1$$    

Note: many distributions lower = $0$ or $-\infty$ and upper = $\infty$

3. If events A and B are mutually exclusive:

$$P(A\ \cup B) = P(A) + P(B)\ \\ if\ A \perp B$$


-----------------------------------------------

## What do you expect: continuous distributions

**Expected value** with PDF $f(x)$, over the interval, $\{ a, b \}$:  

$$\mathrm{E}[\mathbf{X}] = \int_{a}^b x\ f(x)\ dx$$
 
- Values $x$ are weighted by the PDF     
- By the second axiom of probability presented above, PDF must equal 1.0 integrated over the entire range of $x$   
- Transformation of expectation is same as for discrete random variables    


-------------------------------------------------------

## Bernoulli and Binomial Distributions

Bernoulli distributions model the results of a **single trial** or **single realization** with a binary outcome

- For outcomes $\{ 0,1 \}$, or $\{ failure, success \}$, with probability $p$ of success:

\begin{align}
P(x\ |\ p) &= \bigg\{ 
\begin{matrix}
p \rightarrow x = 1\\
(1 - p) \rightarrow x = 0
\end{matrix}\\
or\\
P(x\ |\ p) &= p^x(1 - p)^{(1-x)}\ x \in {0, 1}
\end{align}

- Model the number of successful outcome in $N$ trials with the **Binomial distribution**  

- Binomial distribution is product of multiple Bernoulli trials:

$$P(k\ |\ N, p) = \binom{N}{k} p^k(1 - p)^{(N-k)}$$

- Product of Bernoulli trials is normalized by the [**Binomial coefficient**](https://en.wikipedia.org/wiki/Binomial_coefficient#Factorial_formula)
  

----------------------------------------

## Distributions for Multiple Outomes; the Categorical and Multinomial Distribution    

Many real-world cases have many possible outcomes   

- In these cases need a probability distribution for multiple outcomes     

- **Categorical distribution** models multiple outcomes   

- Categorical Distribution is the multiple-outcome extension of the Bernoulli distribution, and is sometimes call the **Multinoulli distribution**.   

--------------------------------------------------------

## The Categorical distribution  

Sample space of $k$ possible outcomes, $\mathcal{X} = (e_1,e_2, \ldots, e_k)$.   

- For each trial, there can only be one outcome    

- For outcome $i$ we can encode the results as:    

$$\mathbf{e_i} = (0, 0, \ldots, 1, \ldots, 0)$$

- Only $1$ value $e_i$ has a value of $1$; **one hot encoding** 

- For a single trial the probabilities of the $k$ possible outcomes are expressed:    

\begin{align}
\Pi &= (\pi_1, \pi_2, \ldots, \pi_k) \\ \\
with\ \sum_{i}\pi_i &= 1
\end{align}


--------------------------------------------------------

## The Categorical distribution  

And consequently, we can write the simple probability mass function as:   

$$f(x_i| \Pi) = \pi_i$$


For a series $k$ of trials we can estimate each of the probabilities of the possible outcomes, $(\pi_1, \pi_2, \ldots, \pi_k)$:      

$$\pi_i = \frac{\#\ e_i}{k}$$   

Where $\#\ e_i$ is the count of outcome $e_i$.   

--------------------------------------------------------

## The Categorical distribution  

For the case of $k=3$ you can visualize the possible outcomes of a single Categorical trial   

- Each discrete outcome must fall at one of the corners of a **simplex**  

- The probabilities of of each outcome is $(\pi_1, \pi_2, \pi_3)$. 

 

```{r Simplex, out.width = '40%', fig.cap='Simplex for $Mult_3$', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Simplex.png"))
```


------------------------------------------------

## Normal distribution

The **Normal distribution** or **Gaussian distribution** is one of the most widely used probability distributions

- The distribution of mean estimates of observations of a random variable drawn from any distribution converge to a Normal distribution by the **central limit theorem (CLT)**   

- Many physical processes produce Normal measurement values    

- Normal distribution has tractable mathematical properties   


------------------------------------------------

## Normal distribution    

For a univariate Normal distribution we can write the density function as:

$$P(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\frac{-(x - \mu)^2}{2 \sigma^2}}$$

The parameters can be interpreted as:

\begin{align}   
\mu &= location\ parameter = mean \\
\sigma &= scale = standard\ deviation \\
\sigma^2 &= Variance 
\end{align}

------------------------------------------------

## Multivariate Normal

Many practical applications have an $n$-dimensional parameter vector in $\mathbb{R}^n$, requiring **multivariate distributions**     

- **Multivariate Normal distribution**, parameterized by:  
- **n-dimensional vector of locations**, $\vec{\mathbf{\mu}}$  
  - The vector(multi) valued version of univariate location
- $n$ x $n$ dimensional **covariance matrix**, $\mathbf{\Sigma}$   
  - The multi-dimensional version of univariate variance $\sigma^2$

$$f(\vec{\mathbf{x}}) = \frac{1}{{\sqrt{(2 \pi)^n |\mathbf{\Sigma}|}}}exp \big(\frac{1}{2} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})^T \mathbf{\Sigma} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})\big)$$      

- $|\mathbf{\Sigma}|$ is the determinant of the covariance matrix. 

- Along the diagonal the values are the $n$ variances of each dimension, $\sigma_{i,i}$    

- Off-diagonal terms describe the **dependency** between the $n$ dimensions of the distribution. 

------------------------------------------------

## Multivariate Normal

We can write the covariance matrix:  

$$
\mathbf{\Sigma} = 
\begin{bmatrix}
\sigma_{1,1} &  \sigma_{1,2} & \ldots & \sigma_{1,n} \\
\sigma_{2,1} &  \sigma_{2,2} & \ldots & \sigma_{2,n} \\
\vdots & \vdots & \vdots & \vdots \\
\sigma_{n,1} &  \sigma_{n,2} & \ldots & \sigma_{n,n} \\   
\end{bmatrix} 
$$

For a Normally distributed n-dimensional multivariate random variable, $\sigma_{i,j}$ computed from the sample, $\mathbf{X}$:

\begin{align}
\sigma_{i,j} &= \mathrm{E} \big[ (\vec{x}_i - \mathrm{E}[\vec{x}_i]) \cdot (\vec{x}_j - \mathrm{E}[\vec{x}_j]) \big] \\
&= \mathrm{E} \big[ (\vec{x}_i - \bar{x}_i) \cdot (\vec{x}_j - \bar{x}_j) \big] \\
&= \frac{1}{k}(\vec{x}_i - \bar{x}_i) \cdot (\vec{x}_j - \bar{x}_j)
\end{align}

Where $\cdot$ is the inner product operator and $\bar{x_i}$ is the mean of $\vec{x_i}$.

## Multivariate Normal   

2-dimensional Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.0 \\ 0.0 & 1.0 \end{bmatrix}$

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import scipy.stats as ss
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python, echo=FALSE}
def plot_bi_variate(x, title='1000 draws from a bivariate Normal distribution'):
    ## Plot bi-variable points
    fig, ax = plt.subplots(figsize=(6,6)) 
    plt.rcParams.update({'font.size':6})
    ax.scatter(x[:, 0], x[:, 1], alpha=.1)
    ax.set_xlim(-4.0,4.0)
    ax.set_ylim(-4.0,4.0)
    _=ax.set_title(title)
    plt.show()


## Define the covariance and mean of the bivariate Normal. 
sigma = np.array([[1,0.0], [0.0, 1]])
mu = np.array([0.0, 0.0])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```


## Multivariate Normal   

2-dimensional Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.0 \\ 0.0 & 0.5 \end{bmatrix}$

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,0.0], [0.0, 0.5]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```


## Multivariate Normal   

2-dimensional Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.5 \\ 0.5 & 1.0 \end{bmatrix}$

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,0.5], [0.5, 1.0]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```

## Multivariate Normal   

2-dimensional Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & -0.5 \\ -0.5 & 1.0 \end{bmatrix}$

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,-0.5], [-0.5, 1.0]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```

## Multivariate Normal   

2-dimensional Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.9 \\ 0.9 & 1.0 \end{bmatrix}$

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,0.9], [0.9, 1.0]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```

--------------------------------------------------

## Log-Normal distribution

Log Normal distribution is defined for continuous random variables in the range $0 < x \le \infty$     
- Examples price, weight, length, and volume    

- The Log-Normal distribution is based on a log-transformation of the random variable:    

$$P(x) = \frac{1}{x} \frac{1}{\sigma \sqrt{2 \pi}} \exp{\frac{-(log(x) - \mu)^2}{2 \sigma^2}}$$

--------------------------------------------

## Student t-distribution

[**Student t-distribution**](https://en.wikipedia.org/wiki/Student%27s_t-distribution), or simply the t-distribution, is of importance in statistics since it is the distribution of the difference of means of two Normally distributed random variables  

- t-distribution has one parameter, the **degrees of freedom**, denoted as $\nu$        

- The PDF of the t-distribution is a rather complex looking result:

$$
P(x\ |\ \nu) = \frac{\Gamma(\frac{\nu + 1}{2})}{\sqrt{\nu \pi} \Gamma(\frac{\nu}{2})} \bigg(1 + \frac{x^2}{\nu} \bigg)^{- \frac{\nu + 1}{2}}\\
where\\
\Gamma(x) = Gamma\ function
$$

--------------------------------------------

## Student t-distribution      

Dispursion of student-t distribution determined by DOF, $\nu$    
- Low DOF has heavy tails compared to Normal    
- Student-t $\rightarrow$ standard Normal as $DOF \rightarrow \infty$


```{r student_t, out.width = '70%', fig.cap='Heat map of ', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/student_t.PNG"))
```
---------------------------------------------

## The Gamma and $\chi^2$ distributions

**Gamma family of distributions** includes several members which are important in statistics  

- Gamma distributions are a two-parameter exponential family     

- PDF is defined in the range $0 \le x \le \infty$       

- Gamma family are used in problems, ranging from measurements of physical systems to hypothesis testing  

---------------------------------------------

## The Gamma and $\chi^2$ distributions

Gamma family can be parameterized in several ways; we will use:    
- A shape parameter, $\nu$, the degrees of freedom  
- A scale parameter, $\sigma$    

$$
Gam(\nu,\sigma)=\frac{x^{\nu-1}\ e^{-x/\sigma}}{\sigma^\nu\ \Gamma(\nu)}\\
where\\
x \ge 0,\ \nu > 0,\ \sigma > 0\\
and\\
\Gamma(\nu) = Gamma\ function
$$
 
- Alternatively, use an inverse scale parameter, $\beta = 1/\sigma$.   

---------------------------------------------

## The Gamma and $\chi^2$ distributions

Two useful special cases of the Gamma distribution are:    

- $Gam(1,1/\lambda)$ is the **exponential distribution** with decay constant $\lambda$, and PDF:  
$$exp(\lambda) = \lambda e^{-\lambda x}$$   

- $Gam(\nu/2,2) = \chi^2_\nu$ is the **Chi-squared distribution** with $\nu$ degrees of freedom The $\chi^2_\nu$ distribution has many uses in statistics.   
   - Used for estimates of the variance of the Normal distribution   
   - PDF of the $\chi^2_\nu$ distribution:    
$$\chi^2_\nu=\frac{x^{\nu/2-1}\ e^{-x}}{\sigma^{\nu/2}\ \Gamma(\nu/2)}\\ for\ \nu\ degrees\ of\ freedom$$


-----------------------------------------------------     

## The $\chi^2$ Distribution      

The $\chi^2$ distribution is used to construct parametric hypothesis tests of differences in counts between groups and also:       

- Constructing tests for the significance of fits of observed values to probability distributions.   

- The likelihood ratio test for the significance of differences between nested models.    

- Computing confidence intervals for empirical (as opposed to theoretical) variance estimates of observed values. 

-----------------------------------------------------     

## The $\chi^2$ Distribution  

The $\chi^2$ distribution is a parametric distribution with a single parameter, the degrees of freedom, k = number of possible outcomes - 1. 

- For $n$ iid  Normal random variables, $Z_1, Z_2, ..., Z_n$, define a statistic, $Q$, as the sum of squares:    

$$Q = \sum_{i=1}^n Z_i^2$$

- $Q$ is said to be **$\chi^2_k$ distributed with $k=n-1$ degrees of freedom**   

$$Q = \chi^2_k = \chi^2(k)$$

-----------------------------------------------------     

## The $\chi^2$ Distribution  

The shape of the $\chi^2$ distribution changes character with the DoF: For $k = 1\ or\ 2$ the $\chi^2$ distribution has an exponential decay with the maximum value at $x=0$      

```{r Chi2_DOF_12, out.width = '90%', fig.cap='Heat map of ', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Chi2_DOF_12.PNG"))
```


-----------------------------------------------------     

## The $\chi^2$ Distribution  

The shape of the $\chi^2$ distribution changes character with the DoF: For a middle range of DoF values the density starts at 0 and rises to a maximum or mode and then decay back toward 0         

```{r Chi2_DOF_Med, out.width = '90%', fig.cap='Heat map of ', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Chi2_DOF_Med.PNG"))
``` 


-----------------------------------------------------     

## The $\chi^2$ Distribution  

The shape of the $\chi^2$ distribution changes character with the DoF: For large values of DoF the $\chi^2$ distribution converges toward a normal distribution with location parameter DoF       

```{r Chi2_DOF_Lrg, out.width = '90%', fig.cap='Heat map of ', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Chi2_DOF_Lrg.PNG"))
```



------------------------------------------------

## Odds 

**Odds** are the ratio of the number of ways an event occurs to the number of ways it does not occur  

- Can say that **odds** are the count of events in favor of an event vs. the count against the event

- **Example:** Flip a fair coin, odds of getting heads are $1:1$ (1 in 1)   

- **Example:** Roll a single fair die your odds of rolling a 6 are $1:5$ (1 in 5), or 0.2 

------------------------------------------------

## Odds 

What is the relationship between odds and probability of an event?   

- For some event with count $A$ in a set of all outcomes with count $S$, and count of negative outcomes $B = S - A$:

$$P(A) = \frac{A}{S} = \frac{A}{A + (S - A)} = \frac{A}{A + B} = \frac{count\ in\ favor}{count\ in\ favor\ + count\ not\ in\ favor}\ \\
which\ implies\\
odds = A:(S-A)$$

- **Example:** For the fair coin flip, the odds are $1:1$. So we can compute the probability of heads as:

$$P(H) = \frac{1}{1 + 1} = \frac{1}{2}$$

- **Example** In statistics the [**odds ratio**](https://en.wikipedia.org/wiki/Odds_ratio), $\frac{p}{1-p}$, used to predict the response variable in logistic regression


----------------------------------------------------

## Summary      



- Axioms of probability; for discrete distribution           

$$0 < P(A) \le 1 $$
$$P(S) = \sum_{a_i \in A}P(a_i) = 1 $$
$$P(A\ \cup B) = P(A) + P(B)\\ if\ A \perp B$$

- Expectation    

$$\mathrm{E}[\mathbf{X}] = \sum_{i=1}^n x_i\ p(x_i)$$

## Summary     

- The Categorical distribution  
  - For outcome $i$ we **one hot encode** the results as:    
$$\mathbf{e_i} = (0, 0, \ldots, 1, \ldots, 0)$$
  - For a single trial the probabilities of the $k$ possible outcomes are expressed:    
$$\Pi = (\pi_1, \pi_2, \ldots, \pi_k)$$
  - probability mass function as:   

$$f(x_i| \Pi) = \pi_i$$
   
- Multivariate Normal distribution, parameterized by **n-dimensional vector of locations**, $\vec{\mathbf{\mu}}$ and $n$ x $n$ dimensional **covariance matrix**
$$f(\vec{\mathbf{x}}) = \frac{1}{{\sqrt{(2 \pi)^k |\mathbf{\Sigma}|}}}exp \big(\frac{1}{2} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})^T \mathbf{\Sigma} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})\big)$$      
   
