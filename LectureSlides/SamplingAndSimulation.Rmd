---
title: "Sampling and Simulation"
author: "Steve Elston"
date: "09/22/2022"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Review      



- Axioms of probability; for discrete distribution           

$$0 \le P(A) \le 1 $$
$$P(S) = \sum_{a_i \in A}P(a_i) = 1 $$
$$P(A\ \cup B) = P(A) + P(B)\\ if\ A \perp B$$

- Expectation    

$$\mathrm{E}[\mathbf{X}] = \sum_{i=1}^n x_i\ p(x_i)$$

## Review     

- The Categorical distribution - Discrete multi-variate distribution  
  - For outcome $i$ we **one hot encode** the results as:    
$$\mathbf{e_i} = (0, 0, \ldots, 1, \ldots, 0)$$
  - For a single trial the probabilities of the $k$ possible outcomes are expressed:    
$$\Pi = (\pi_1, \pi_2, \ldots, \pi_k)$$
  - Probability mass function as:   

$$f(x_i| \Pi) = \pi_i$$
   
- Multivariate Normal distribution, parameterized by **n-dimensional vector of locations**, $\vec{\mathbf{\mu}}$ and $n$ x $n$ dimensional **covariance matrix**
$$f(\vec{\mathbf{x}}) = \frac{1}{{\sqrt{(2 \pi)^k |\mathbf{\Sigma}|}}}exp \big(\frac{1}{2} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})^T \mathbf{\Sigma} (\vec{\mathbf{x}} - \vec{\mathbf{\mu}})\big)$$      
   



## Review      

- Conditional probability     
  - One random variable depends on another    
  - But not commutable  
$$P(A|B) = P(A) \nLeftrightarrow P(B|A) = P(B)$$  

- Mutually exclusivity   
$$P(A|B) = P(A) + P(B)$$

- Independence      
$$P(A|B) = P(A)$$


## Review   

- Bayes' theorem     
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
- Marginal distribution   

- For continuous distributon
$$p(\theta_1) = \int_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)\ d\theta2, \ldots, d \theta_n$$

- For discrete distribution 
$$p(\theta_1) = \sum_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)$$





## Introduction      

Sampling is a fundamental process in the collection and analysis of data    

- Sampling is important because we almost never have data on a whole population   

- Sampling must be randomized to preclude biases     

- As sample size increases, the standard error decreases by the law of large numbers   

- Key points to keep in mind:   
  - Understanding sampling is essential to ensure data is representative of the entire population   
  - Use inferences on the sample to say something about the population   
  - The sample must be randomly drawn from the population  

- Sampling from distribution is the building block of simulation     

- We will take up the topic of resampling later    


## Sampling Example


| Use Case | Sample | Population |
|---|---|---|
| A/B Testing | The users we show either web sites A or B | All possible users, past present and future|   
|World Cup Soccer | 32 teams which qualify in one season | All national teams in past, present and future years|   
|Average height of data science students | Students in a data science class | All students taking data science classes world wide|   
|Tolerances of a manufactured part | Samples taken from production lines | All parts manufactured in the past, present and future |   
|Numbers of a species in a habitat |Population counts from sampled habitats |All possible habitats in the past, present and future |   

- In several cases it is not only impractical, but impossible to collect data from the entire population  

- We nearly always work with samples, rather than the entire population.    



## Importance of Random Sampling   

All statistical methods rely on the use of **randomized unbiased samples**    

- Failure to randomized samples violates many key assumptions of statistical models     

- An understanding of proper use of sampling methods is essential to statistical inference      

- Most commonly used machine learning algorithms assume that training data are **unbiased** and **independent and identically distributed (iid)**   
  - These conditions are only met if training data sample is randomized   
  - Otherwise, the training data will be biased and not represent the underlying process distribution
  


## Sampling Distributions   

Sampling of a population is done from an unknown **population distribution**, $\mathcal{F}$      

- Any statistic, $s$, we compute for the generating process is based on a sample, $\hat{\mathcal{F}}$     
- The statistic is an approximation, 
of a **population parameter**   
   - For example, the mean of the population is $\mu$   
   - But, sample estimate is $\bar{x}$   

- If we continue to take random samples from the population and compute estimates of a statistic, we generate a  **sampling distribution**   
   - Hypothetical concept of the sampling distribution is a foundation of **frequentist statistics**
   - Example, if we continue generating samples and computing the sample means, $\bar{x}_i$ for the ith sample   

- **Frequentist statistics** built on the idea of randomly resampling the population distribution and recomputing a statistic         
   - In the frequentist world, statistical inferences are performed on the sampling distribution    
   - Sampling process must not bias the estimates of the statistic

 
 
## Sampling Distributions   

Sampling of a population is done from an unknown **population distribution**, $\mathcal{F}$      

- Any statistic, $s$, we compute for the generating process is an approximation for the population, $s(\hat{\mathcal{F}})$  

```{r Sampling, out.width = '60%', fig.cap='Sampling distribution of unknown population parameter', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/SamplingDistribuion.png"))
```
 

## Sampling and the Law of Large Numbers

The **weak law of large numbers** is a theorem that states that **statistics of independent random samples converge to the population values as more samples are used**      

- Example, for a population distribution, $\mathcal{N}(\mu,\sigma)$, the sample mean is:

$$Let\ \bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i$$

Then by the weak law of large numbers:

$$ \bar{X} \rightarrow E(X) = \mu\\
as\\
n \rightarrow \infty$$

- This result is reassuring, the larger the sample the more the statistic converges to the population parameter 


## Sampling and the Law of Large Numbers

The law of large numbers is foundational to statistics    

- We rely on the law of large numbers whenever we work with samples   

- Assume that **larger samples are more representatives of the population we are sampling**     

- Is foundation of sampling theory, plus modern computational methods;  simulation, bootstrap resampling, and Monte Carlo methods   

- If the real world did not follow this theorem, then much of statistics (along with much of science and technology) would have to be rethought


-------------------------------------   

## Sampling and the Law of Large Numbers   

The weak law of large numbers has a long history     

- Jacob Bernoulli posthumously published the first proof for the Binomial distribution in 1713   

- Law of large numbers is sometimes referred to as **Bernoulli's theorem**     
  
- A more general proof was published by Poisson in 1837.  

-------------------------------------   

## Sampling and the Law of Large Numbers

A simple example     

- The mean of fair coin flips (0,1) = (T,H) converges to the expected value with more flips      

- The mean converges to the expected value of 0.5 for $n = 5, 50, 500, 5000$  


```{python, echo = FALSE}
import numpy as np
import numpy.random as nr
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

import math
```

```{python, echo = FALSE}
nr.seed(3457)
n = 1
p = 0.5
size = 1000000
# Create a large binomial distributed population. 
pop = pd.DataFrame({'var':nr.binomial(n, p, size)}) 
# Sample the population for different sizes and compute the mean
sample_size = [5, 50, 500, 5000]
out = [pop.sample(n = x).mean(axis = 0) for x in sample_size] 
for n,x in zip(sample_size,out): print("%.0f  %.2f" %(n, x))
```

## Sampling and the Law of Large Numbers

A simple example; mean of fair coin flips (0,1) = (T,H) converges to the expected value with more flips      


```{r LLN_Bernoulli, out.width = '60%', fig.cap='Convergance of mean estimates for fair coin', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/LLN_Bernoulli.png"))
```


## the Central Limit Theorem (CLT)

- Law of large number is almost too obvious, but the CLT is more tricky!     

- Law of large number applied to any statistic, but the CLT applies only to the **mean**     

- Let $X$ be a random variable representing the population     
   - $X$ is allowed to have **any distribution** (not limited to normal), and let $\mu$ be your **true population mean** and $\sigma$ the **true population standard deviation**
   - Given sample size $n$ thee sampling distribution of $\bar X$is 

$$\bar X \sim N(\mu, \frac{\sigma}{\sqrt n})$$

## importance of CLT

CLT is a sort of guarantee    

- Sampling distribution of mean estimates do not depend on the population the sample was drawn from   
- **Standard deviation** $s$ of the sampling distribution of $\bar{x}$ converges as $1/\sqrt n$


- Only depends on the population's mean and variance, and on the sample size     

- CLT is the basis for hypothesis testing    


## Example of CLT

Start with a mixture of Normal distributions   

```{python, echo=FALSE}
x = np.concatenate([
        nr.normal(loc=0, scale=1, size=1000),
        nr.normal(loc=3, scale=.5, size=1000)])
        

fig, ax = plt.subplots(figsize=(8, 5), )          
sns.kdeplot(x, ax=ax)
```


## Example CLT    

Sample distribution of the mean of mixture of Normals is Normally distributed!    

- Repetitively random sample the population, $size = 50$

- Compute the mean estimate, $\bar{x}$ for each sample   

```{python, eval=FALSE}
x_means = np.array([
        nr.choice(x, size=50, replace=True).mean()
        for i in range(500)])

breaks = np.linspace(x_means.min(), x_means.max(), num=40)
fig, ax = plt.subplots(1,2, figsize=(12, 5) ) 
_ = ax[0].hist(x_means, bins=breaks)
_ = sm.qqplot(x_means, line='s', ax=ax[1])
```


## Example CLT    

Sample distribution of the mean of mixture of Normals is Normally distributed!    

```{python, echo=FALSE}
x_means = np.array([
        nr.choice(x, size=50, replace=True).mean()
        for i in range(500)])

breaks = np.linspace(x_means.min(), x_means.max(), num=40)
fig, ax = plt.subplots(1,2, figsize=(12, 5) ) 
_ = ax[0].hist(x_means, bins=breaks)
_ = sm.qqplot(x_means, line='s', ax=ax[1])
```

## Standard Error and Convergence for a Normal Distribution

As we sampled from a Normal distribution, the sample means converges to the population mean    

- What can we say about the expected error of the mean estimate as the number of samples increases?   
   - Population has standard deviation $\sigma$
   - This measure is known as the **standard error** of the sample mean    
   - By the CLT the standard error is defined:

$$se = \pm \frac{\sigma}{\sqrt{(n)}}$$

- Standard error decreases as the square root of $n$   
   - Example, if you wish to halve the error, you will need to sample four times as many values.   

- For the mean estimate, $\bar{x}$, define the uncertainty in terms of **confidence intervals**    
   - For 95% confidence interval:

$$CI_{95} =\bar{x} \pm 1.96\ se$$ 


## Convergence and Standard Errors for a Normal Distribution

Mean estimates for realizations of standard Normal distribution with 95% confidence intervals


```{r MeanConvergenceSEs, out.width = '40%', fig.cap='Convergance of mean estimates with standard errors', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/MeanConvergenceSEs.png"))
```


## Sampling Strategies

There are a great number of possible sampling methods. 

- Some of the most commonly used methods

- **Bernoulli sampling**, a foundation of random sampling   

- **Stratified sampling**, when groups with different characteristics must be sampled   

- **Cluster sampling**, to reduce cost of sampling     

- **Systematic sampling and convenience sampling**, a slippery slope


---------------------------------------------------------

## Bernoulli Sampling

**Bernoulli sampling** is a widely used foundational random sampling strategy    

- Bernoulli sampling has the following properties:    

- A **single random sample** of the population is created    

- A particular value in the population is sampled based on the outcome of a Bernoulli trial with fixed probability of success, $p$     

- Example, a company sells a product by weight     
  - To ensure the quality of a packaging process so few packages are underweight   
  - Impractical to empty and weight the contents of every package   
  - Bernoulli randomly sampled packages from the production line and weigh contents   
  - Statistical inferences are made from sample 


--------------------------------------------------

## Bernoulli Sampling

An example with synthetic data. 
- Generate population of 10000 samples from the standard Normal distribution    
- The realizations are randomly divided into 4 groups with $p = [0.1,0.3,0.4,0.2]$     
- The probability of a sample being in a group is not uniform, and sums to 1.0.  


```{python}
nr.seed(345)
population_size = 10000
data = pd.DataFrame({"var":nr.normal(size = population_size), 
                     "group":nr.choice(range(4), size= population_size, p = [0.1,0.3,0.4,0.2])})
data.head(10)
```

--------------------------------------------------

## Bernoulli Sampling

The population of 10000 samples from the standard Normal distribution    

The mean of each group should be close to 0.0:    
1. The sample is divided between 4 groups   
2. Probability of sample from given group, $p=0.1$      
3. Summary statistics are computed for each group      

```{python}
def count_mean(dat, p=1.0):
    import numpy as np
    import pandas as pd
    groups = dat.groupby('group') # Create the groups
    n_samples = np.int64(p * groups.size())
    se = np.sqrt(np.divide(groups.aggregate(np.var).loc[:, 'var'], n_samples))
    means = groups.aggregate(np.mean).loc[:, 'var']
    ## Create a data frame with the counts and the means of the groups
    return pd.DataFrame({'Count': n_samples, 
                        'Mean': means,
                        'SE': se,
                        'Upper_CI': np.add(means, 1.96 * se),
                        'Lower_CI': np.add(means, -1.96 * se)})
p = 0.1
count_mean(data, p)
```


--------------------------------------------------

## Sampling Grouped Data

Group data is quite common in application

A few examples include:     

1. Pooling opinion by county and income group, where income groups and counties have significant differences in population        

2. Testing a drug which may have different effectiveness by sex and ethnic group    

3. Spectral characteristics of stars by type  


-----------------------------------------------

## Stratified Sampling     

What is a sampling strategy for grouped or stratified data?    

- **Stratified sampling** strategies are used when data are organized in **strata**    

- **Simple Idea:** independently sample an equal numbers of cases from each strata   

- The simplest version of stratified sampling creates an **equal-size Bernoulli sample** from each strata

- In many cases, nested samples are required      
  - For example, a top level sample can be grouped by zip code, a geographic strata     
  - Within each zip code, people are then sampled by income bracket strata    
  - Equal sized Bernoulli samples are collected at the lowest level     
  
  
-----------------------------------------------------

## Example    

Bernoulli sample 100 from each group and compute summary statistics

```{python}
def stratify(dat, p):
    groups = dat.groupby('group') # Create the groups
    nums = min(groups.size()) # Find the size of the smallest group
    num = int(p * dat.shape[0]) # Compute the desired number of samples per group
    if num <= nums: 
        ## If sufficient group size, sample each group.
        ## We drop the unneeded index level and return, 
        ## which leaves a data frame with just the original row index. 
        return groups.apply(lambda x: x.sample(n=num)).droplevel('group')
    else: # Oops. p is to large and our groups cannot accommodate the choice of p.
        pmax = nums / dat.shape[0]
        print('The maximum value of p = ' + str(pmax))

p = 0.01
stratified = stratify(data, p)
```

```{python}
count_mean(stratified)
```


--------------------------------------

## Cluster Sampling

When sampling is expensive, a strategy is required to reduce the cost  

- Examples of expensive to collect data:   
  - Surveys of customers at a chain of stores   
  - Door to door survey of homeowners   
  - Sampling wildlife populations in a dispersed habitat     

Population can be divided into randomly selected clusters:        
  - Define the clusters for the population    
  - Randomly select the required number of clusters   
  - Sample from selected clusters    
  - Optionally, stratify the sample within each cluster   


--------------------------------------

## Cluster Sampling

As an example, select a few store locations and Bernoulli sample customers at these locations.


```{python}
## First compute the clusters
num_clusters = 10
num_vals = 1000
## Create a data frame with randomly sampled cluster numbers
clusters = pd.DataFrame({'group': range(num_clusters)}).sample(n = num_vals, replace = True)
## Add a column to the data frame with Normally distributed values
clusters.loc[:, 'var'] = nr.normal(size = num_vals)
```

```{python, echo=FALSE}
count_mean(clusters)
```


--------------------------------------

## Cluster Sampling

Randomly select 3 clusters

```{python}
## Randomly sample the group numbers, making sure we sample from 
## unique values of the group numbers. 
clusters_samples = nr.choice(clusters.loc[:, 'group'].unique(), 
                             size = 3, replace = False)
## Now sample all rows with the selected cluster numbers
clus_samples = clusters.loc[clusters.loc[:, 'group'].isin(clusters_samples), :]
```


The sampled clusters   
```{python, echo=FALSE}
print('cluster sampled are: ')
for x in clusters_samples:
    print(x)
```

Display summary statistics 

```{python, echo=FALSE}
count_mean(clus_samples)
```

------------------------------

## Systematic Sampling

**Convenience and systematic sampling** are a slippery slope toward biased inferences      

- Systematic sampling **lacks randomization**    

- Convenience sampling selects the cases that are easiest to obtain     
  - Commonly cited example known as **database sampling**      
  - Example, the first N rows resulting from a database query     
  - Example, every k-th case of the population   
  
  
-------------------------------------------------

## A Few More Thoughts on Sampling

There are many practical aspects of sampling.

- Random sampling is essential to the underlying assumptions of statistical inference    

- Whenever you are planning to sample data, make sure you have a clear sampling plan     

- Know the number of clusters, strata, samples in advance    

- Donâ€™t just stop sampling when your desired result is achieved: e.g. error measure!     

## Introduction to Simulation    

Simulation enables data scientists to study the behavior of stochastic processes with complex probability distributions    

- Most real-world processes have complex behavior, resulting in complex distributions of output values   
   - Simulation is a practical approach to understanding these complex processes     
 
- Two main purposes of simulation can be summarized as:           
   - **Testing models:** If data simulated from the model do not resemble the original data, something is likely wrong  
   - **Understand processes with complex probability distributions:** In these cases, simulation provides a powerful and flexible computational technique to understand behavior   

## Introduction to Simulation    

- As cheap computational power has become ubiquitous, simulation has become a widely used technique      

- Simulations compute a large number of cases, or realizations    
   - The computing cost of each realization must be low in any practical simulation  
   - Realizations are drawn from complex probability distributions of the process model    

- In many cases, realizations are computed using conditional probability distributions    
  - The final or posterior distribution of the process is comprised of these realizations     


## Representation as a Directed Acyclic Graphical Model

When creating a simulation with multiple conditionally dependent variables it is useful to draw a directed graph; a **directed acyclic graphical model or DAG**    

- The graph is a communications device showing which variables are independent and which are conditionally dependent on others with the shapes used representing the type of nodes      
- **Probability distributions** of the variables are shown as **ellipses**       
   - Distributions have parameters which must be estimated          
- **Decision variables** are deterministic and are shown as **rectangles**       
  - Decisions are determined by variables    
  - Setting decision variables can be performed either manually or automatically             
- **Utility nodes**, profit in this case, are shown as **diamonds**       
  - Nodes represent a **utility function** given the dependencies in the graph   
  - Utility calculations are deterministic given the input values           
- **Directed edges** show the dependency structure of the distributions   
  - Arrows point to **child nodes** which are dependent on **parent nodes**  
  - Child node **conditional** on parent nodes    

## Sandwich Shop Simulation   

The sandwich shop simulation can be represented by a DAG

```{r DAG, out.width = '60%', fig.cap='Directed graph of the distributions for profit simulation', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Simulation_Directed_Graph.jpg"))
```

## Sandwich Shop Simulation   

Interpreting the DAG   

- The DAG is a shorthand description of the simulation model   

- Leaves of the DAG are independent distributions   
   - Parameters must be known or estimated     
   - Can be useful to vary the parameters       

- **Child** distributions are conditional on their **parents**       
   - Parameters must be known or estimated        
   - Resulting distribution can be quite complex    

- **Decision variables** deterministicly change the model parameters   

- **Utility node** uses a fixed deterministic formula to compute the value for each realization of the simulaiton    


## Tips on Building Simulations     

Creating, testing and debugging simulation software can be tricky given the stochastic nature of simulation       

- Build your simulation as a series of small, easily tested, chunks      
- Test each small functional unit individually, including at least testing some typical cases, as well as boundary or extreme cases     
- Test your overall simulation each time you add a new functional component - **avoid big bang integration!**    
- Simulations are inherently stochastic, set a seed before you begin tests so they are repeatable      


## Summary

Sampling is a fundamental process in the collection and analysis of data    

- Sampling is important because we almost never have data on a whole population   

- Sampling must be randomized to preclude biases    

- As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers

- Key points to keep in mind:   
  - Understanding sampling is essential to ensure data is representative of the entire population   
  - Use inferences on the sample to say something about the population   
  - The sample must be randomly drawn from the population  

- Sampling from distribution is the building block of simulation     


