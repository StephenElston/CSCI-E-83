---
title: "Models Categorical Variables and Nonlinear Response"
author: "Steve Elston"
date: "10/30/2023"
output:
  slidy_presentation: default
  
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```



## Review    

Linear models are a flexible and widely used class of models   

- Fit model coefficients by **least squares** estimation   

- Least squares is a maximum likelihood estimate, given Normally distributed residuals   

- Can use many types of predictor variables   

- We prefer the simplest model that does a reasonable job   
   - The principle of **Occam's razor**  

- Must consider the **bias-variance trade-off**  

- Outliers can have significant effect on linear model parameter estimates   


## Review   

When evaluating any machine learning model consider **all evaluation methods available**    

- No one method best all of the time    

- Homoskedastic Normally distributed residuals   

\begin{align}
\epsilon_i &\sim \mathbb{N}(0, \sigma^2)\\
\mathbb{E}(\epsilon_i) &= 0
\end{align}

- Want reasonable values $R^2_{adj}$, RMSE, etc

- Is model and its coefficients significant? 
   - F test and Omnibus test on model    
   - t-test on model parameter estimates   

- **Different methods highlight different problems** with your model     

- Don't forget to check that the **model must make sense** for your application! 


## Review      

 Representation of machine learning models      
 
 - The key representation is the model matrix      
    - Column of 1s for intercept      
    - Columns of feature or predictor values   
 
 $$
A = 
\begin{bmatrix}
1, x_{1,1}, x_{1,2}, \ldots, x_{1,p}\\
1, x_{2,1}, x_{2,2}, \ldots, x_{2,p}\\
1, x_{3,1}, x_{3,2}, \ldots, x_{3,p}\\
\vdots,\ \vdots,\ \vdots,\ \vdots,\ \vdots\\
1, x_{n,1}, x_{n,2}, \ldots, x_{n,p}
\end{bmatrix}
$$
 
 - There are two standards for signatures of ML functions    
    - A model matrix $X$ (exogenous-features) and label array $Y$ (dependent-endogenous) - Scikit-learn and base Statsmodels   
    - A data frame with all features (predictors) and label (dependent) columns plus a model formula - Statsmodels formula and R   



## Review    

There are a number of assumptions in linear models that you overlook at your peril! 

- The feature or predictor variables should be **independent** of one another      
   - This is rarely true in practice   
   - **Multi-collinearity** between features makes the model **under-determined**   

- We assume that numeric features or predictors have **zero mean** and about the **same scale**      
   - Do not want to bias the estimation of regression coefficients with predictors that do not have a 0 mean   
   - Do not want to have predictors with a large numeric range dominate training   
   
- Values of each predictor or feature should be iid      
   - If variance changes with sample, the optimal value of the coefficient is not constant    
   - If there **serial correlation** in the predictor values, the iid assumption is violated - use time series models   
   


## Review       

Scaling of features is required for many machine learning models      

- Several commonly used approaches      
   - **Z-score** scaling results in features with zero mean and unit variance    
   - Use Z-score scaling for features approximately normally distributed   
   - **Min-max** scaling transforms feature values to range $\{0,1\}$ or $\{-1,1\}$    
   - Use min-max scaling for features with truncated range of values   

- Effect on model coefficients   
   - Scaling changes model coefficients by the scale factor applied   
   - Can re-scale (unscale) model coefficients before processing unknown cases    
   - Or use **same scaling** for unknown feature values and scale response    
   
- When coding categorical variables as binary dummy variables no need to scale - already in range [0-1]    


## Introduction  

- Working with categorical variables    
   - **One-hot encoding**    
   - Working with **contrasts**    
   - **Effects** and **adjustments**    

- Building models with nonlinear or non-Normal response   
  - Use **generalized linear model (GLM)** for nonlinear response          
  - **Link function** transforms nonlinear model to linear model       
  - Evaluating Binomial response models   
  - Compare model performance with **deviance**   
  - Poisson regression   


## Working with Categorical Variables 

Linear models, like nearly all machine learning models, use numeric features    

- How can categorical variables be used in linear models?    

- Need to transform categories to numeric variables with **one hot encoding**      
   - Each category becomes a binary **dummy variable**, encoded [0,1]  
   - Only one dummy variable has nonzero value - encodes the category   
   - n categories represented by n-1 dummy variables; all 0s encodes one level    
   
- Binary variables are an exception    
   - Represent with a single binary variable. [0,1] values


## Working with Categorical Variables  

Example: Consider a data set with categorical variables

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv')#, index_col=0)
test_scores.head(10)
```

## Working with Categorical Variables  

Example: split the data into train and test subsets    

```{python}
nr.seed(2345)
msk = nr.choice(test_scores.index, size = 120, replace=False)
test_scores_train = test_scores.iloc[msk,:]
print(test_scores_train.shape)
test_scores_test = test_scores.drop(msk, axis=0) 
print(test_scores_test.shape)
```




## Working with Categorical Variables  

Example: We can encode a categorical variable with the Python Patsy package to get the X (model) and Y(label) arrays:     

```{python}
from patsy import dmatrices
Y, X = dmatrices("socst ~ C(ses, levels=[1,2,3])", data=test_scores)
print(X[:5])
print(Y[:5])
```


## Working with Categorical Variables  

Example: A simple linear model with one categorical variable       

```{python}
import statsmodels.formula.api as smf 
linear_model = smf.ols("socst ~ C(ses)", data=test_scores_train).fit()
linear_model.summary()
```


## Working with Categorical Variables  

Wait! What happened to the coefficient for the first level of ses?  

- The intercept is the **mean response** of the first level     

- The other coefficients are **contrasts** with respect to the mean of the first level.       
- Consider the following possible ways we can encode responses to a categorical variable - often called a **treatment**      
   - For $n$ treatments, there are mean responses $[ \mu_1, \mu_2, \ldots, \mu_n ]$   
   - The alternative encoding is a treatment with intercept, $I$, and $n-1$ **contrasts**, $[1, c_2, \ldots, c_{n-1}]$    
   
- The means and contrasts are related:    
   
$$\begin{bmatrix}
I\\
c_2\\
\vdots\\
c_n
\end{bmatrix} = 
\begin{bmatrix}
\mu_1\\
\mu_2 - \mu_1\\
\vdots\\
\mu_n - \mu_1
\end{bmatrix}$$   

## Working with Categorical Variables    

In a linear model we can sometimes relate the coefficient values to an effect size    

- Start with $n$ treatments $[t_1, t_2, \ldots, t_n ]$ with effect sizes, $[e_1, e_2, \ldots, e_n ]$     

- With **no intercept term** the means represent the effect sizes:   

$$\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_n
\end{bmatrix} = 
\begin{bmatrix}
\mu_1\\
\mu_2\\
\vdots\\
\mu_n
\end{bmatrix}$$  

- With intercept term compute effect sizes using contrasts, $[1, c_2, c_3, \ldots, c_n ]$:  

$$\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_n
\end{bmatrix} = 
\begin{bmatrix}
I\\
I + c_2\\
\vdots\\
I + c_{n}
\end{bmatrix}$$   
 
 
## Working with Categorical Variables    

In a linear model we can sometimes relate the coefficient values to an effect size    

- Assumes the treatments are orthogonal    
   - In other words, applied one at a time    
   - e.g. a case can only be in one category     
   
- Assumes that the model coefficients are statistically independent    
   - Coefficients are dependent in overfit model    
   
- Often need to **adjust** for other effects    
   - Other treatments   
   - Levels of other categorical variables    
   - Use **partial slope** of continuous variables  
   
- In other words **apply** with care!    
   - Don't over-interpret your model   
   - Conditions in real world hard to verify, particularly for observational data   
 

## Models with Nonlinear Response    

How do we deal with models that do not have nonlinear response variables?   

- Example: binary response variable, $[0,1]$, $Bin(\theta)$ distributed     
   - Probability parameter $\theta$
   - A binary classifier      
   
- Example: Intensity of an arrival process, $poisson(\lambda)$ response   
   - $\lambda$ is the average rate or **intensity** of a point process 
   - Estimate the parameter $\lambda$    
   
- Example: Categorical response variable for $n$ categories, $Multi(\pi_1, \pi_2, \ldots, \pi_n)$:    
   - $n$ category classifier
   - Response is probability probability for each category, $\Pi = [\pi_1, \pi_2, \ldots, \pi_n]$ 



## Models with Nonlinear Response    

The **generalized linear model (GLM)** is a framework for nonlinear response models   

- Nonlinear response is non-Normally distributed  
   
- For each distribution use a **link function** to transform to a linear model    
   - Linear model has Normally distributed response   
   - Link function transform nonlinear response to Normal distribution  

- To compute the nonlinear response  
   - Start with a linear model, OLS    
   - Transform response with **inverse link function**    
   - Works for all exponential family response distributions      
   
   
## The Generalized Linear Model   

Link functions are available for many distributions      

- Supported in [statsmodels](https://www.statsmodels.org/stable/glm.html)        

- Supported in [Scikit-Learn](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)     

- Examples: 
   - Gaussian, identity function    
   - Inverse Gaussian   
   - Binomial, logit function    
   - Multinomial  
   - Poisson    
   - Negative Binomial   
   - Gamma   
   - Tweedie   
   
   
## The Generalized Linear Model   

General form for link function, $g()$, mapping response variable, $y$, observation vector, $x$, to linear model      

- Given linear model $\hat{\lambda} = \beta_0 + \beta_1\ x$:

$$g \big( \mathtt{E}[y_i|x_i] \big)  = \hat{\lambda} = \beta_0 + \beta_1\ x$$

- To find the value of the response variable we apply the inverse link function:        

$$\mathtt{E}[y_i|x_i]   = g^{-1} \big( \hat{\lambda} \big) = g^{-1} \big( \beta_0 + \beta_1\ x \big) $$


## The Generalized Linear Model  

OLS has Normal response    

- What is the link function    

- Link function for OLS is just unity, or $1$   

- Output of linear model directly maps to Normally distributed response    

$$\beta_0 + \beta_1\ x \sim N(\beta_0 + \beta_1\ x, \sigma)$$     


   
## The Logistic Regression Model   

Construct a generalized linear model using a **Binomial distribution**    

- Commonly known as [**logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression)      

- Logistic regression widely used as a classification model     

- Logistic regression is linear model, with a binary response or label values, `{False, True}` or `{0, 1}`      

- Response computed as a log likelihood, leading to a Binomial distributed response     

- Categorical response is simple extension to categorical distribution     
    - One Binomial to many     
    - Many Binomial to many    


## The Logistic Regression Model    

Construct logistic regression as a GLM       

- Start with a model for the log-odds of response $1$ vs. $0$      
  - Probability of success, or $y_i = 1$, $p_i$      
  - Independent variable vector $x_i$   
  - Model parameter vector, $\mathbf{\beta}$     
  - Binary response, $y_i = [0,1] \sim Bin(p_i)$       
  
- Define the link function, know as the or **logit function**:   

$$logit(\mathtt{E}[y_i|x_i]) = logit(p_i(x_i)) = ln \Big(\frac{p_i(x_i)}{1-p_i(x_i)} \Big) =\beta_0 + \mathbf{\beta} ùë•_i$$

## The Logistic Regression Model      

Response of linear model is transformed to the binomially distributed random variable through the **inverse link function**      

- Known as the **inverse logit function**, or **logistic function**, $f(x_i) = g^{-1}(x_i)$       

- After some algebra we can arrive at:   

\begin{align}
\lambda_i &= \beta_0 + \mathbf{\beta} ùë•_i\\
p_i(x_i) = \mathtt{E}[y_i|x_i] &= f(\lambda_i) = logit^{-1}(\lambda_i)  \\
p_i(x_i) = f(x_i) &= logit^{-1}(\lambda_i) = \frac{1}{1 + e^{-\lambda_i}} = \frac{1}{1 + e^{-(\beta_0 + \mathbf{\beta} ùë•_i)}}
\end{align}



## Logistic Regression Model

What does the transformation function look like?    

- Consider a simple 1-dimensional case

$$y_i = \frac{1}{1 + e^{-x_i}}$$

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv', index_col=0)
#test_scores.head(10)
```


```{python, echo=FALSE}
# Plot the logistic transformation function (f(x) above)
x_seq = np.linspace(-7, 7, 100)

def log_fun(x, center=0, scale=1):
    e = np.exp(-scale*(x-center))
    log_out = 1./(1. + e)
    return log_out

log_fun_vectorized = np.vectorize(log_fun)

log_y = log_fun_vectorized(x_seq)

fig, ax = plt.subplots(figsize=(6,4))
ax.plot(x_seq, log_y)
ax.set_title('Standard Logistic Function')
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.grid()
plt.show()
```

- The response is bound in the range $[0,1]$

- We say the logistic transformation **squashes** the linear response $\lambda =A \vec{b}$ to binary, $[0,1]$   

- Can set a **decision threshold** for binary response    
   - Default $= 0.5$


## Evaluation of Classifiers   

How can we evaluate a classifier's accuracy? 

- Determine proportions of test cases which are classified as:     
   - True Positives (TP): Are positive and should be positive      
   - True Negatives (TN): Are negative and should be negative    
   - False Positives (FP): Classified as positive but are actually negative; **Type I errors**   
   - False Negatives (FN): Classified as negative but are actually positive; **Type II errors**   

- Organize these metrics into a **confusion matrix**      

|  | Classified Negative | Classified Positive |
| ---- | :---: | :---: |
|Negative | TN | FP |  
|Positive | FN | TP |



## Evaluation of Classifiers   


The other metrics are defined as follows:

- Accuracy = (TP + TN) / (TP + FP + TN + FN)      

- Selectivity or Precision = TP / (TP + FP)       
   - Precision is the fraction of the relevant class predictions which are correct     
   
- Sensitivity or Recall = TP / (TP + FN)
   - Recall is the fraction of the relevant class were we able to predict    


- Is a trade-off between precision and recall     
   - Consider changing the decision threshold    
   - High threshold $\rightarrow$ lower recall, more false negative    
   - Low threshold $\rightarrow$ lower precision, more false positives       

## Example of Logistic Regression    

How well can we predict the type of school given the test scores?     

```{python}
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1)

## Fit the model
formula = 'schtyp ~ math'
logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()

print(logistic_reg_model.summary())
```


## Example of Logistic Regression    

The data frame now looks like this with the predicted probability and the binary scores:

```{python}
## score the results 
threshold=0.18
test_scores['predicted'] = logistic_reg_model.predict()
test_scores['score'] = [1 if x>threshold else 0 for x in test_scores['predicted']]

test_scores.loc[:,['schtyp','math','predicted','score']].head(20)
```


## Example of Logistic Regression    

Now, evaluate the model - the classifier is almost useless - **no Kagle awards!**:   

```{python}
import sklearn.metrics as sklm  
def print_metrics(labels, scores):
    metrics = sklm.precision_recall_fscore_support(labels, scores)
    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')
    print('                 Score negative    Score positive')
    print('Actual negative    %6d' % conf[0,0] + '             %5d' % conf[0,1])
    print('Actual postitive    %6d' % conf[1,0] + '             %5d' % conf[1,1])
    print('')
    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))
    print(' ')
    print('           Negative      Positive')
    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])
    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])
    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])
    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])
    
print_metrics(test_scores['schtyp'], test_scores['score'])    
```


## Example of Logistic Regression     

How can we understand the cut-off value in terms of the CDF of the positive and negative cases?    

```{python, echo=FALSE}
fig, ax = plt.subplots(figsize=(4, 3)) # define axis
sns.ecdfplot(x='predicted', hue='schtyp', data=test_scores, ax=ax)
ax.set_ylabel('Cummulative probability')
ax.axvline(0.18, color='red', linestyle='dotted')
```

- Both CDFs are at 1.0 by about probability = 0.25 - this is a very skewed situation!   
- CDF curves nearly the same = poor model    
- Positive cases to the left of cut-off are Type II errors     
- Negative cases to the right of cut-off are Type I errors   


## What is Deviance?

The significance of the GLM is expressed in terms of a statistic called **deviance**      

- It can be challenging to understand what deviance really means      

- To further complicate the problem there are several commonly used forms of deviance

 - OLS regression models are often evaluated based on variance ratios, such as the $R^2$ metric, or error metrics like RMSE       
 
 - Given a nonlinear mapping between the linear model and the response, these methods are not suitable      
 

## What is Deviance?     

The significance of the GLM is expressed in terms of a statistic called **deviance**   

- It can be a bit of a challenge to wrap your head around what deviance really means    

- Deviance is the difference between the log likelihood of a reference model and some other model
    - For model with parameters $\phi$, $f(\phi)$
    - Reference model with parameters $\phi_R$, $f(\phi_R)$,
    
$$D \big( f(\mathbf{X},\phi) \big) = 2 \big( l(f(\mathbf{X}, \phi_R)) - l( f(\mathbf{X}, \phi)\big))$$   

- Where:   
    - $\mathbf{X} =$ the array of observations    
    - $l \big( f(\mathbf{X}, \phi) \big) =$ the log-likelihood of the model   


## What is Deviance?     

There are several commonly used forms of deviance:     

- **Residual deviance** uses a **saturated model** as a reference   
   - Saturated model has a degree of freedom (parameter) for each observation used to fit    
   - Model has a perfect fit to training data   
   - But poor **generalization** or accuracy for new observations 
   
- **Residual deviance** uses a **null model** as a reference    
    - Null model explains none of the variance of the data     
    - Is just an informed guess  
    - Example, for Normally distributed response null model is the mean 
    - Example, for Binomially distributed response null model random guesses $[0,1]$ based on mean of observations  


## What is Deviance? 

The deviance statistic is $\chi^2$ distributed    

- Can apply a significance test on a model  

- A model with small deviance is little better that informed guessing    
    - Has small $\chi^2$ and is not considered significant     
    
- A model with large deviance has a large $\chi^2$   
    - Exhibits a significant improvement in accuracy  
    - Improves on reference model   
 

## Residual Deviance   

**Residual deviance** is 2 times the difference between the log likelihood of a staturated model, $f(\mathbf{X}, \phi_S)$, and some other model:      

$$D_R \big( f(\mathbf{X},\phi) \big) = 2 \Big( l \big(f(\mathbf{X}, \phi_S) \big) - l \big( f(\mathbf{X}, \phi) \big) \Big)$$    

Residual deviance has several important properties.    

- Log likelihood of saturated model is 0 for some some common distributions    
     - Example, Normal distribution   
     - Example, Binomial distribution    
     - In these cases deviance $2\ l \big( f(\mathbf{X}, \phi)\big)$

- In other cases log likelihood of staturated model is not 0:     
    - Example, Poisson distribution 

## Residual Deviance  

**Normal saturated model:**

We can construct a saturated model for normally distributed values by having a free mean parameter, $u_i$ for each observation, $x_i$.

$$
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\vdots\\
x_n
\end{bmatrix} = 
\begin{bmatrix}
1, 0, 0, \ldots, 0\\
0, 1, 0, \ldots, 0\\
0, 0, 1, \ldots, 0\\
\vdots,\ \vdots,\ \vdots,\ \ddots,\ \vdots\\
0, 0, 0, \ldots, 1
\end{bmatrix}
\begin{bmatrix}
\mu_1\\
\mu_2\\
\mu_3\\
\vdots\\
\mu_n
\end{bmatrix}
$$

Now, the normal likelihood for this models is   

$$\mathcal{L}(\mathbf{X} |\ \mathbf{\mu}) = -\frac{1}{(2 \pi \sigma^2)^{n/2}} exp\Big[ - \frac{1}{2 \sigma^2}  \sum_{i=1}^n(x_i - \mu_i)^2 \Big]$$

Since $\mu_i = x_i$, the Normal likelihood is $1.0$ and the log likelihood is $0.0$.     

## Residual Deviance  

**Binomial staturated model:**

A saturated Binomial model can be constructed if for each outcome, $y_i$, the probability parameter is:  

$$ 
\pi_i = 
\begin{cases}
  1 & \text{if $y_i=1$} \\
  0 & \text{if $y_i=0$}
\end{cases}
$$   

The likelihood can be writen:     

$$\prod_{i=1}^{n} \hat{\pi}_i^{y_i} (1 - \hat{\pi}_i)^{1-y_i} = 1^n = 1$$

Since $log(1) = 0$, the log likelihood of the staturated model.     



## Null Deviance
 
 **Null deviance** is measured with respect to a null model          
      
 - Intuitively, the null model is informed guessing     
 
 - Null deviance is a measure of how much the model improves accuracy beyond guessing
 

## Null Deviance 

To understand binomial null deviance, start with the expected value of the binomial log-likelihood:  

$$\hat{l}(k, n | p) = log \binom{n}{k} + k\ log(p) + (n-k)\ log(1-p)$$

- Calculate the binomial probability:  

$$p_\phi = \bar{y} = \frac{k}{n}$$

For the null model we set $\beta_0 = p_\phi$, so the logistic function for a null model is:    

$$f_{\phi}(\hat{y}) = \frac{1}{1 + e^{-p_{\phi}}} = \frac{1}{1 + e^{-\beta_0}}$$

And with expected log-likelihood:   

$$\hat{l}_{phi}(k, n | p_{\phi}) = log \binom{n}{k} + k\ log(p_{\phi}) + (n-k)\ log(1-p_{\phi})$$


## Null Deviance 


Gain intuitive understand of the behavior of the null model by example     

- Consider the case where half the values of the response, $y$, are 1s     

- $k = n/2$, so $p_\phi = n/2 = 0.5$       

- For each value of $x_i$, model randomly selects a 1 or a 0 response with probability of 0.5     
- This model is random guessing with accuracy of 0.5    

- In other words, the null model is no better in terms of predictive power than just saying that all values of $y$ are either 0 or all values are 1.    


## Null Deviance 

Form of null deviance of a linear model

$$logit_{\phi}(p(x_i)) = \beta_0$$  

- Deviance is log of the square expected likelihood ratio:    

\begin{align}
D(\beta_0 + \mathbf{\beta} ùë•_i) &= ln \Big( \frac{\hat{L}^2}{\hat{L}_{\phi}^2} \Big) \\
&= 2 \big( \hat{\mathcal{l}}  -  \hat{\mathcal{l}}_{\phi}  \big)
\end{align}

- Expected null log-likelihood, $\hat{\mathcal{l}}_{\phi}$, is fixed by the observed response values     

-Therefore, the better the model, and higher the likelihood, $\hat{\mathcal{l}}$    
  - Higher deviance and therefore the value of $\chi^2$    
  - Model with large $\chi^2$ has greater significance and accuracy of predictions    


## Properties of Deviance 

What are some key properties of deviance?    

- Recall that log-likelihood is a negative number   
  - Higher log-likelihood has smaller negative magnitude    
  - Log-likelihood of reference model has large negative magnitude       
  
- Deviance $\ge 0$ always     
  - If model is no better than the reference model, $\mathcal{l}_R = \mathcal{l}_{\phi} \rightarrow D = 0$   
  - For model with greater predictive power, $\mathcal{l} \gt \mathcal{l}_{\phi},\ D \gt 0$


## Applying Deviance 

Deviance concepts discussed here can be applied to any GLM   

- In some cases, deviance can be used directly   
    - Is explained variance significant?    

- In other cases, log-likelihood ratio is used directly for model evaluation       
    - Compare performance of different models      


## Solving Maximum Likelihood Problem   

We have investigated methods for finding maximum likelihood solutions at large scale     

- ML algorithms are employed routinely to logistic regression problems on a massive scale    

- Variations of the stochastic gradient descent (SGD) algorithms     

- Quasi-Newton's methods like the limited memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno (l-BFGS) algorithm  


## Poisson Regression as GLM     

Poisson regression is example of a GLM     

- Poisson regression is example of nonlinear response model   

- Recall, the Poisson distribution has an exponential form with a single parameter, $\lambda$   

- Parameter, $\lambda$, is the expected arrival rate of the process    

- Predictions, $y_i$, given the observations, $x_i$ and the model parameter, $\lambda_i$

- Link function $\Longleftrightarrow$ Inverse link function:   

$$log \big[ \mathtt{E}(y_i |x_i) \big] = x_i \lambda_i \Longleftrightarrow \mathtt{E}(y_i | x_i) = e^{x_i \lambda_i }$$


 

## Poisson Regression as GLM     

Extend relationship using a linear model for $\lambda$      

- Expected arrival rate changes with the independent variable    

- Example, linear model with intercept     
  - $\beta_0$, and $p$ dimensional slope parameter vector, $\vec{\beta}$     
  - Estimate of $\lambda_i$, for a $p$ dimensional observation vector $\mathbf{x}_i$

$$log(\lambda_i) = \beta_0 + \mathbf{x}_i \vec{\beta} \Longleftrightarrow \lambda_i = e^{(\beta_0 + \mathbf{x}_i \vec{\beta})}$$



## Poisson Regression Example

Data for number of awards for students by program and math score   

- Count data - suitable for Poisson model       

- Note exponential decrease in counts with math score and program      

```{python, echo=FALSE}
program_codes = {1:'General', 2:'Academic', 3:'Vocational'}
award_scores = pd.read_csv("https://stats.idre.ucla.edu/stat/data/poisson_sim.csv")
award_scores.loc[:,'prog'] = award_scores.loc[:,'prog'].replace(program_codes)

fig,ax = plt.subplots(figsize=(8,6))
sns.countplot(data=award_scores, x='num_awards',hue='prog', ax=ax);
plt.show();
```



## Poisson Regression Example

Fit a model and examine result   

```{python, echo=FALSE}
formula = 'num_awards ~ math + C(prog)'
poisson_glm = smf.poisson(formula, data=award_scores).fit()
poisson_glm.summary()
```

## Poisson Regression Example

Maximum likelihood regression lines for counts with Poisson model       

```{python, echo=FALSE}
award_scores['predicted'] = poisson_glm.predict(award_scores)

fig,ax = plt.subplots(figsize=(8,8))
sns.scatterplot(x='math', y='num_awards', hue='prog', data=award_scores, ax=ax);
sns.lineplot(x='math', y='predicted', hue='prog', data=award_scores, ax=ax, legend=False);
ax.set_title('Actual vs. predicted awards by program');
ax.set_xlabel('Math score');
ax.set_ylabel('Number of awards');
plt.show();
```



## Summary  

- Models with nonlinear response have non-Normal distributions  

- The generalized linear model accommodates nonlinear response distributions          

- Link function transforms to linear model       
   - Inverse link function transforms from Normal distribution to response distribution   

- Evaluating Binomial response models    
   - Confusion matrix organizes 
   - Compute metrics from elements of confusion matrix
   - Use multiple evaluation criteria 

- Compare model performance with deviance   

