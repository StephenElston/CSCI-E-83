---
title: "Regularization and Sparse Models"
author: "Steve Elston"
date: "11/07/2023"
output:
  powerpoint_presentation: default
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```


## Review    

There are a number of assumptions in linear models that you overlook at your peril! 

- The feature or predictor variables should be **independent** of one another      
   - This is rarely true in practice   
   - **Multi-collinearity** between features makes the model **under-determined**   

- We assume that numeric features or predictors have **zero mean** and about the **same scale**      
   - Do not want to bias the estimation of regression coefficients with predictors that do not have a 0 mean   
   - Do not want to have predictors with a large numeric range dominate training   
   
- Values of each predictor or feature should be iid      
   - If variance changes with sample, the optimal value of the coefficient is not constant    
   - If there **serial correlation** in the predictor values, the iid assumption is violated - use time series models   
   

## Review      

 Representation of machine learning models      
 
 - The key representation is the model matrix      
    - Column of 1s for intercept      
    - Columns of feature or predictor values   
 
$$
A = 
\begin{bmatrix}
1, x_{1,1}, x_{1,2}, \ldots, x_{1,p}\\
1, x_{2,1}, x_{2,2}, \ldots, x_{2,p}\\
1, x_{3,1}, x_{3,2}, \ldots, x_{3,p}\\
\vdots,\ \vdots,\ \vdots,\ \vdots,\ \vdots\\
1, x_{n,1}, x_{n,2}, \ldots, x_{n,p}
\end{bmatrix}
$$
 
 - There are two standards for signatures of ML functions    
    - A model matrix $X$ (exogenous-features) and label array $Y$ (dependent-endogenous) - Scikit-learn and base Statsmodels   
    - A data frame with all features (predictors) and label (dependent) columns plus a model formula - Statsmodels formula and R   

   
## Review  

- Models with nonlinear response have non-Normal distributions  

- The generalized linear model accommodates nonlinear response distributions          

- Link function transforms to linear model       
   - Inverse link function transforms from Normal distribution to response distribution   

- Evaluating Binomial response models    
   - Confusion matrix organizes 
   - Compute metrics from elements of confusion matrix
   - Use multiple evaluation criteria 

- Compare model performance with deviance  


 
## Dealing with Overfit Models

Example: We suspect that the terms, ses and prog are significant in predicting social science test scores:  

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf 
import scipy.stats as ss  
from patsy import dmatrices
from sklearn.model_selection import train_test_split
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv')
np.random.seed(3344)
test_scores = test_scores.sample(frac=1.0)
size = int(0.7 * test_scores.shape[0])
test_scores_train = test_scores.iloc[:size,:].reset_index(drop=True)
test_scores_test = test_scores.iloc[size:, :].reset_index(drop=True)
```


```{python}
formula = 'socst ~ C(ses)*C(prog)'
#linear_model = smf.ols("socst ~ C(ses)*C(prog)", data=test_scores_train).fit()
linear_model = smf.ols(formula, data=test_scores_train).fit()
linear_model.summary()
```


## Dealing with Overfit Models

Example: After 3 or 4 rounds of *guess and cut* feature pruning, we arrive at a model with only significant coefficients:    

```{python}
linear_model = smf.ols("socst ~ - 1 + C(ses):C(prog)", data=test_scores_train).fit()
linear_model.summary()
```


## Dealing with Overfit Models   

Let's compare the results of the unpruned and pruned models     

- The metrics indicate the fit is exactly the same   

- Why prefer the sparse (pruned) model?    
   
- What are the consequences of the over-fit model?    
   - Several predictors (features) are included that are not needed     
   - Including non-significant predictors can only increase noise and reduce generalization of a model    
   - Colinear features confound model fiting - change of coefficient values correlated
   - For model with linear response, consider the effect of an unexpected value of a non-significant predictor      
   
- But manually pruning a model with a great many features is a doomed task!   
   - What if we included all the interactions with type of school, race and sex?   
   - We now have up to 5th order interaction - 45 model coefficients with none significant!!    

## Dealing with Overfit Models  

We want our models to be **sparse**   

- A sparse model has the minimum complexity required to explain the data      

- The sparse model is a manifestation of **Occam's Razor**     
   - A scientific principle that the simplest of competing theories is the preferred one     
   
- Sparse models use the minimum number of independent variables (features)       
   - Are considered **parsimonious**      
   - Generalize well     
   
- Use regularization methods to identify minimum coefficient set    


## Dealing with Overfit Models   

**Idea!:** Try systematically pruning the model using some metric    

- Leads to the the **step-wise regression algorithm**     
   - Forward step-wise regression adds most explanatory variable one at a time    
   - Backward step-wise regression removes least explanatory variable one at a time    
   - Can go both directions - see the R documentation  
   - Hard to find a good metric    

- But, making multiple hypothesis tests is a fraught undertaking    
   - $H_0$ is null or insignificant predictor
   - High probability of Type 1 or Type 2 error     
   - Type 1 error, fail to accept $H_0$, $\rightarrow$ include insignificant predictor       
   - Type 2 error, fail to reject $H_0$, $\rightarrow$ drop significant predictor 
  


## Regularization - The Bias-Variance Trade-Off      

Regularization is a systematic approach to preventing over-fitting     

- To understand regularization need to understand the bias-variance trade-off     

- To better understand this trade-off decompose mean square error:    

$$\Delta Y^2 = \mathbb{E} \Big[ \big( Y - \hat{f}(X) \big)^2 \Big]$$
$Y =$ the label vector     
$X =$ the feature matrix      
$\hat{f}(X) = \hat{Y}$ estimate from fitted model      

Expanding this relation gives us:    

\begin{align}
\Delta Y^2 &= \Big( \mathbb{E} [ \hat{f}(X)] - \hat{f}(X) \Big)^2 + \mathbb{E} \Big[ \big( \hat{f}(X) - \mathbb{E} [ \hat{f}(X)] \big)^2 \Big] + \sigma^2\\
&= Bias^2 + Variance + Irreducible\ Error
\end{align}



## Regularization - The Bias-Variance Trade-Off      

How do we interpret the bias-variance trade-off relationship:    
$$\Delta Y^2 = \Big( \mathbb{E} [ \hat{f}(X)] - \hat{f}(X) \Big)^2 + \mathbb{E} \Big[ \big( \hat{f}(X) - \mathbb{E}[ \hat{f}(X)] \big)^2 \Big] + \sigma^2$$
- $\mathbb{E}  \Big[ \big( \hat{f}(X) - \mathbb{E} [ \hat{f}(X)] \big)^2 \Big]$, the expected squared difference between the model output and the expected model output is the **variance** of the model     
   - For low variance model, $\mathbb{E}  \Big[ \big( \hat{f}(X) - \mathbb{E} [ \hat{f}(X)] \big)^2 \Big] \rightarrow 0$    
   - Model **generalizes** since variance is low for each prediction, $\hat{f}(X)$     

- $\Big( \mathbb{E}[ \hat{f}(X)] - \hat{f}(X) \Big)^2$, the expected value of the difference between the model output and the expected model output is the **bias** of the model    
  - For **unbiased model**, $\mathbb{E}\Big[ \hat{f}(X)] - \hat{f}(X) \Big] = 0$
  - Example: OLS model with $residuals \sim N(0, \sigma^2)$ is **unbiased**     
  
- $\sigma^2$ is inherent or irreducable error in data   



## Regularization - The Bias-Variance Trade-Off

There is a trade-off between bias and variance      

- Need to find the optimal trade-off point  

```{r BiasVariance, out.width = '50%', fig.cap='The trade-off between bias and variance', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BiasVariance.png"))
```


## Eigendecomposition - Review

For OLS model need to find  $\big(A^TA \big)^{-1}A \propto Cov(A)^{-1}A$ for $p \times p$ design matrix $A$

- Decompose the $p \times p$ covariance matrix $cov(A) = \frac{1}{p}A^TA$ into **eigenvalues** and **eigenvectors**: 

$$cov(A) = Q \Lambda Q^{-1}$$

- $Q$ is the $p \times p$ matrix of $p$ **orthonormal eigenvectors**   

- For real-valued $cov(A)$ the eigenvectors $Q$ are real valued so:   

$$Q^{-1} = Q^T$$

## Eigendecomposition - Review

Decompose the $p \times p$ covariance matrix $cov(A) = \frac{1}{p}A^TA$ into **eigenvalues** and **eigenvectors**: 

$$cov(A) = Q \Lambda Q^{-1}$$


And, the $p$ eigenvalues are represented as diagonal matrix: 

$$
\Lambda = 
\begin{bmatrix}
\lambda_1, 0, \ldots, 0\\
0, \lambda_2, \ldots, 0 \\
\vdots,\ \vdots,\ \ddots,\  \vdots\\
0, 0, \ldots, \lambda_{p}
\end{bmatrix}
$$




##  Eigendecomposition - Review

The inverse of the covariance can be computed from its eigendecomposition   

$$cov(A)^{-1} = Q^T \Lambda^{-1} Q$$

Where $\Lambda^{-1}$ is:       

$$
\Lambda^{-1} = 
\begin{bmatrix}
\frac{1}{\lambda_1}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2}, \ldots, 0 \\
\vdots,\ \vdots,\ \ddots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p}
\end{bmatrix}
$$


##  Eigendecomposition - Review    

At first look eigen-decomposition seems a bit mysterious

- The eigenvalues are the **roots** of the covariance matrix    
   - Similar to the familiar roots of a polynomial  

- For square matrix, $A$, and some Euclidean norm 1 vector, $x$, we can find a root, $\lambda$: 

\begin{align}
Ax &=  \lambda x \\
Ax - \lambda x &= 0 \\
(A - I \lambda) x &= 0
\end{align}

- For $p \times p$ dimensional matrix there are $p$ eigenvalues and orthogonal eigenvectors    

- But, there is no guarantee that the $p$ values of $\lambda$ are unique     
   - If columns of $A$ are colinear, there are $< p$ unique eigenvalues        
   - With colinear independent variables some $\lambda_i \approx 0$

## Regularization - Ill-Posed Problems 

If columns of $A$ are not linearly independent, the inverse of the covariance matrix is **ill-posed**       

$$
cov(A) = Q \Lambda Q^T\\
cov(A)^{-1} = Q^T \Lambda^{-1} Q
$$

- The eigenvalues are ordered, $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$

- For **ill-posed** covariance matrix    
   - The smallest $\lambda_i \rightarrow 0$   
   - So, $\frac{1}{\lambda_i} \rightarrow \infty$
  
- Uh oh! The inverse covariance matrix **does not exist!**    
  - With colinear features $\lambda_i \rightarrow 0\ \rightarrow$ confounded fitting    
  - With unifromative features $\lambda_i \rightarrow 0\ \rightarrow$ projecting random noise



## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- Recall:   

$$x = A b + \epsilon$$

- The **normal equations provide a solution**:

$$b = (A^TA)^{-1}A^Tx$$

- This solution requires finding the inverse of the covariance matrix, $(A^TA)^{-1}$       
   - But this inverse may be unstable since $\lambda_p \approx 0$   
   - In mathematical terminology we say the problem is **ill-posed**     
   
   
## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- We can add a **bias term** to the diagonal of the covariance matrix     

the **L2 or Euclidean norm** minimization problem):

$$min [\parallel A \cdot x - b \parallel +\ \alpha^2 \parallel b\parallel]\\  or \\
b = (A^TA + \alpha^2 I)^{-1}A^Tx$$

Where the L2 norm of the coefficient vector is:
$$||b|| = \big( \beta_0^2 + \beta_1^2 + \ldots + \beta_p^2 \big)^{\frac{1}{2}} = \Big( \sum_{i=0}^p \beta_i^2 \Big)^{\frac{1}{2}}$$


## L2 Regularization    

How can we understand this relationship?     

$$b = (A^TA + \alpha^2I)^{-1}A^Tx$$

- Adds values along the diagonal of the covariance matrix     
   - This creates a so called **ridge** in the covariance, $cov_{regularized} = A^TA + \alpha^2$     
   - Leads to the term **ridge regression**   

- Constrain the L2 norm values of the model coefficients using the **penalty term** $\alpha$     
   - Larger $\alpha$ is more bias but lover variance   
   - Larger $\alpha$ makes the inverse of the covariance more stable   
   
- L2 regularization is a **soft constraint** on the model coefficients    
   - Even smallest coefficients are not driven to 0   
   - Coefficients can grow in value, but under the constraint   


## L2 Regularization   

Eigen-decomposition of the regularized covariance matrix:    

$$A^TA + \alpha^2I = Q 
\begin{bmatrix}
\lambda_1 + \alpha^2, 0, \ldots, 0\\
0, \lambda_2 + \alpha^2, \ldots, 0 \\
\vdots,\ \vdots,\ \ddots,\  \vdots\\
0, 0, \ldots, \lambda_p + \alpha^2
\end{bmatrix}
Q^T$$


## L2 Regularization   

The inverse regularized covariance matrix is: 

$$\Big( A^TA + \alpha^2I \Big)^{-1} = Q^T 
\begin{bmatrix}
\frac{1}{\lambda_1 + \alpha^2}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2 + \alpha^2}, \ldots \\
\vdots,\ \vdots,\ \ddots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p + \alpha^2}
\end{bmatrix}
Q$$

With any $\alpha > 0$, the inverse eigenvalues of the inverse covariance matrix are bounded     

Increasing $\alpha$ increases bias, but increases the stability of the inverse   

## L2 Regularization     

**Example:** compute the eigenvalues of a covariance matrix     

```{python, echo=FALSE}
test_scores_train['socst_zero_mean'] = test_scores_train['socst'] - np.mean(test_scores_train['socst'])
test_scores_test['socst_zero_mean'] = test_scores_test['socst'] - np.mean(test_scores_test['socst'])
```

```{python}
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
Y, X = dmatrices("socst_zero_mean ~ C(ses, levels=[1,2,3])*C(prog, levels=[1,2,3])", data=test_scores)
cov_X = np.matmul(np.transpose(X),X)
cov_X = np.divide(cov_X, float(cov_X.shape[0]))
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 90$

**Add regularization** with $\alpha^2=0.1$ and compute the eigenvalues

```{python}
alpha_sqr = 0.1 
alpha_sqr = np.diag([alpha_sqr] * cov_X.shape[0])
#alpha_sqr = np.diag([alpha] * cov_X.shape[0])
#cov_X = np.divide(np.matmul(np.transpose(X),X), float(cov_X.shape[0]))
cov_X = np.add(cov_X, alpha_sqr)
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 74$    

- Notice that the largest and most influential eigenvalues hardly change    

- Limited bias in this case   


## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$    

- The norm of the coefficient vector, $\vec{b}$, is constrained


```{r L2, out.width = '60%', fig.cap='L2 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L2.jpg"))
```   

- Notice that L2 regularization is a **soft constraint** on parameter values  

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python}
def regularized_coefs(df_train, df_test, alphas, L1_wt=0.0, n_coefs=8,
                      formula = 'socst_zero_mean ~ C(ses)*C(prog)', 
                      label='socst_zero_mean'):
    '''Function that computes a linear model for each value of the regularization 
    parameter alpha and returns an array of the coefficient values. The L1_wt 
    determines the trade-off between L1 and L2 regularization'''
    coefs = np.zeros((len(alphas),n_coefs + 1))
    MSE_train = []
    MSE_test = []
    for i,alpha in enumerate(alphas):
        ## First compute the training MSE
        temp_mod = smf.ols(formula, data=df_train).fit_regularized(alpha=alpha,L1_wt=L1_wt)
        coefs[i,:] = temp_mod.params
        MSE_train.append(np.mean(np.square(df_train[label] - temp_mod.predict(df_train))))
        ## Then compute the test MSE
        MSE_test.append(np.mean(np.square(df_test[label] - temp_mod.predict(df_test))))
        
    return coefs, MSE_train, MSE_test

alphas = np.arange(0.0, 0.3, step = 0.003)   
Betas, MSE_test, MSE_train = regularized_coefs(test_scores_train, test_scores_test, alphas) 
```

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python, echo=FALSE}
def plot_coefs(coefs, alphas, MSE_train, MSE_test, ylim=None):
    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis
    for i in range(coefs.shape[1]): # Iterate over coefficients
        ax[0].plot(alphas, coefs[:,i])
    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)
    ax[0].set_ylabel('Partial slope values')
    ax[0].set_xlabel('alpha')
    ax[0].set_title('Parial slope values vs. regularization parameter number')
    if ylim is not None: ax[0].set_ylim(ylim)
    
    ax[1].plot(alphas, MSE_train, label='Training error')
    ax[1].plot(alphas, MSE_test, label='Test error')
    ax[1].set_ylabel('Mean squared error')
    ax[1].set_xlabel('alpha')
    ax[1].set_title('MSE vs. regularization parameter number')
    plt.legend()
    plt.show()
    
    
plot_coefs(Betas, alphas, MSE_test, MSE_train)#, ylim=(-5.0,5.0))
```

## L1 Regularization    

The L1 norm provides regularization with different properties     

- Constrains the model parameters using the L1 norm: 

$$min [\parallel A \cdot x - b \parallel +\ \alpha^2 \parallel b\parallel^1]$$

- This form looks a lot like the L2 regularization formulation   

- $\parallel b\parallel^1$ is the L1 norm       

- Compute the L1 norm of the $p$ model parameters:

$$||b||^1 = \big( |\beta_0| + |\beta_1| + \ldots + |\beta_p| \big) = \Big( \sum_{i=0}^m |\beta_i| \Big)^1$$

$|\beta_i|$ is the absolute value of $\beta_i$. 


## L1 Regularization 

What are the properties of the L1 regularization   

- L1 norm is a **hard constraint**     

- L1 regularization drives coefficients to zero   

- The hard constraint property leads to the term **lasso regularization**    

- Lasso regression is a method of **feature selection**   


## L1 Regularization   

The Lasso regularization is a strong constraint on coefficient values   

- Some coefficients are forced to zero    

- The constraint curve is like a *lasso*


```{r L1, out.width = '60%', fig.cap='L1 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L1.jpg"))
```   


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.0, 0.6, step = 0.02)
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=1.0) #, formula=formula)
Betas[:5]
```


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE_train, MSE_test) #, ylim=(-10.0,10.0))
```



## Elastic Net Regularization    

Do we always have to choose between the soft constraint of L2 and the hard constraint of L1?   

- L2 regularization works well for **colinear features** as soft constraint       
   - Down-weights colinear features   
   - But soft constraint so poor model selection 

- L1 regularization provides **good model selection** as hard constraint    
   - Drives coefficients of non-informative variables to 0
   - But poor selection for colinear features     
   
- The **Elastic Net** weights L1 and L2 regularization    
   - Hyperparameter $\lambda$ weights L1 vs. L2 regularization   
   - Hyperparameter $\alpha$ sets strength of regularization  

$$min \Big[ \parallel A \cdot x - b \parallel +\ \lambda\ \alpha \parallel b\parallel^1 +\ (1- \lambda)\ \alpha \parallel b\parallel^2 \Big]$$

## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.0, 0.5, step = 0.02)
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=0.5)
```


## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE_train, MSE_test) 
```


## Elastic Net Regularization 

Check the model summary for $\lambda=0.5$, $\alpha=0.10$

```{python}
lm_elastic = smf.ols("socst_zero_mean ~ C(ses)*C(prog)", data=test_scores_train).fit_regularized(alpha=0.1, L1_wt=0.5)
lm_elastic.params
```


```{python, echo=FALSE}
test_scores_train['predicted'] = lm_elastic.predict(test_scores_train)
test_scores_train['resids'] = np.subtract(test_scores_train.predicted, test_scores_train.socst_zero_mean)

def residual_plot(df):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(figsize=(8, 3), ) 
    RMSE = np.std(df.resids)
    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);
    plt.axhline(0.0, color='red', linewidth=1.0);
    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.title('PLot of residuals vs. predicted');
    plt.xlabel('Predicted values');
    plt.ylabel('Residuals');
    plt.show()
    
residual_plot(test_scores_train)  
```



## Summary  


Over-fit models and regularization    

- **Bias variance trade-off** between fit to training data (bias) and generalization error (vaiance)   

- Prefer minimal or **sparse models**    

- L2 regularization is a soft constraint      

- L1 regularization is a hard constraint  

- ElasticNet trade-off between L1 and L2  
