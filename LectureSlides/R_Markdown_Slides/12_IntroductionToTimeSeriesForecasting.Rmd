---
title: "Forecasting with Time Series Data"
author: "Steve Elston"
date: "11/20/2023"
output:
  powerpoint_presentation: default
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
#py_install("pmdarima")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```


## Why Are Time Series Useful?

- Data are often time-ordered 
- Estimates 30% of data science problems include time series data
- Must use specific time series models


## Why Are Time Series Useful?


<center> "It's tough to make predictions, especially about the future!"</center>    

<center>Karl Kristian Steincke, Danish politician, ca 1937</center>   

- **Demand forecasting:** Electricity production, Internet bandwidth, Traffic management, Inventory management, sales forecasting     
- **Medicine:** Time dependent treatment effects, EKG, EEG    
- **Engineering and Science:** Signal analysis, Analysis of physical processes   
- **Capital markets and economics:** Seasonal unemployment, Price/return series, Risk analysis 


## Why Are Time Series Data Different? 

Models must account for time series behavior      

- Most statistical and machine learning assume data samples are **independent identically distributed (iid)**             

- But, this is not the case for time series data       

- Time series values are correlated in time       

- Time series data exhibit **Serial correlation**      
   * Serial correlation of values      
   * Serial correlation of errors     
   * Violate iid assumptions of many statistical and ML Models    
`

## Properties of Time Series

Fundamental elements of time series  

- Fundamental components which cannot be predicted   
  * White noise    
  * Random walks    
- Autocorrelation and partial autocorrelation
- Trend    
- Seasonal components    
- Differencing to transform to stationarity     
    - Seasonal differencing    
    - Non-seasonal differencing  
- Stationarity properties
    - augmented Dicky-Fuller test
    - KPSS test    



## Time Series Forecasting Models    

**Forecasting** is the goal of much of time series analysis    

 
- ARIMA and SARIMAX models; time series linear models    
- For comprehensive introduction see [Forecasting: Principles and Practice, Hyndman and Athanaosopoulos, 3rd edition, 2018](https://otexts.com/fpp2/), available as book or free online    
- [Rob Hyndman's blog](https://robjhyndman.com/hyndsight/) is a source of many interesting ideas and example in time series analysis    




## Tine Series Models   

The **ARIMA model** is composed three components:    

- **Autoregressive component (AR)** accounts for partial autocorrelation   
   * Serial correlation of observatons     
- **Integrative component (I)** accounts random walks and trend   
- **Moving Average (MA)** accounts for autocorrelation   
   * Serial correlation of model error    
- **SARIMAX** model adds:   
  * **Seasonal components (S)**   
  * **Exogenous variables (X)**   




## Time Series Models

Many types of time series models   


- **SARIMA**: seasonal autoregressive integrative moving average, adds seasonal components     
  * **seasonal AR** accounts for serial correlation of seasonal values      
  * **seasonal I** component accounts for random walk and trend of seasonal components     
  * **seasonal MA** accounts for seasonal serial correlation of errors    

- **SARIMAX:** seasonal autoregressive integrative moving average with **exogenous variables**       
   * Add effect of external factors    
   * Example; effect of specific holiday    
   * Example; effect arising from factors not incorporated in endogenous variable   



## The Autoregressive Model   

**Autoregressive model** relates past observed values to the current value    

- An autoregressive model of order $p$, $AR(p)$, uses the last p observations:   

\begin{align}
y_t &= \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + w_t\\
&where\\
\phi_k &= model\ coefficient\ at\ lag\ k\\
w_t &= white\ noise\ error\ at\ time\ t;\ \sim \mathtt{N}(0,\sigma^2)\\
y_t &= observation\ at\ time\ t\\
\end{align}

- An AR process has the following properties:   
   * $\rho_0 = 1$ always    
   *  $p_k = \phi_k$   
   * Number of nonzero PACF values $= p$   
   * A shock at any time will affect the result as $t \rightarrow \infty$   
- AR model assume stationary time series       
 

## The Autoregressive Model    

How can we understand the AR model?   

- Consider an AR(2) model   
- The value of $y_t$ is a weighted sum of $k$ previous values plus an error term    


```{r AR_Model, out.width = '75%', fig.cap='Illustration of the AR(2) model', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/AR_Model.png"))
``` 

## The Autoregressive Model    

How can we understand the AR model?   

- Model matrix of AR(2) model

$$
A = 
\begin{bmatrix}
y_t,\ y_{t-1},\ y_{t-2}\\
y_{t-1},\ y_{t-2},\ y_{t-3}\\
y_{t-2},\ y_{t-3},\ y_{t-4}\\
\vdots,\ \ \ \ \ \vdots,\ \ \ \ \ \vdots\\
y_{2},\ \ \ \ y_{1},\ \ \ \ y_{0}\\
y_{1},\ \ \ \ y_{0}\ \ \ \, 0\\
y_0,\ \ \ \ 0,\ \ \ \ 0
\end{bmatrix}
$$

- AR model is a **linear model!**    

- For coefficient vector, $\Phi = [\phi_1, \phi_2, \ldots, \phi_p]$, solve linear system:

$$Y = A \Phi$$

## The Autoregressive Model   

We can rewrite the AR(1) model in terms of exceptions:    

\begin{align}
\mathbb{E}(y_t) &= \phi \mathbb{E}(y_{t-1}) + \mathbb{E}(\epsilon_t) \\
&or \\
\mu &= c + \phi \mu + 0 \\
&therefore \\
\mu &= \frac{c}{1 - \phi}
\end{align}

- The AR model is unstable for the roots of the polynomial $1 - \phi$   
- Is a stable AR process if $\phi \lt 1$    
- Violation of this condition leads to an unstable model!    
- AR(1) model with unit root is a random walk with the $\phi_1=0$:    

$$y_t = \sum_{i=1}^t w_i$$    

## The Autoregressive Model

Example of AR(2) time series with coefficients $= (1.0, 0.75, 0.25)$:     
  
- Time series looks a bit random     
- But, notice the statistical properties; ACF, PACF    
- PACF has 2 non-zero lag values, so $p=2$    


```{python, echo=FALSE}
from math import sin
import pandas as pd
import numpy as np
import numpy.random as nr
from math import pi
from scipy.stats import zscore
import sklearn.linear_model as lm
import statsmodels.tsa.seasonal as sts
import scipy.stats as ss
import statsmodels.tsa.arima_process as arima
from statsmodels.tsa.arima.model import ARIMA, ARIMAResults
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing
import pmdarima as pm
import statsmodels.graphics.tsaplots as splt
import matplotlib.pyplot as plt

def plot_ts(ts, lab = ''):
    fig, ax = plt.subplots(figsize=(8,4))
    ts.plot(ax=ax)
    ax.set_title('Time series plot of ' + lab)
    ax.set_ylabel('Value')
    ax.set_xlabel('Date')
    plt.show()

def acf_pacf_plot(ts):
    fig, ax = plt.subplots(1,2,figsize=(12,3))
    _=splt.plot_acf(ts, lags = 40, ax=ax[0])
    _=splt.plot_pacf(ts, lags = 40, method='yw', ax=ax[1])
    plt.show()
    
def dist_ts(ts, lab = '', bins = 40):
    ## Setup a figure with two subplots side by side
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
    ## Plot the histogram with labels
    ts.hist(ax = ax1, bins = bins, alpha = 0.5)
    ax1.set_xlabel('Value')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Histogram of ' + lab)
    ## Plot the q-q plot on the other axes
    ss.probplot(ts, plot = ax2)
    plt.show()
    
def decomp_ts(ts, freq = 'M', model = 'additive'):
    res = sts.seasonal_decompose(ts, model=model, period=12) 
    res.plot()
    plt.show()
    return(pd.DataFrame({'resid': res.resid, 
                         'trend': res.trend, 
                         'seasonal': res.seasonal},
                       index = ts.index) )

nr.seed(4477)
def ARMA_model(ar_coef, ma_coef, start = '1-2005', end = '1-2015'):
    dates = pd.date_range(start = start, end = end, freq = 'M')
    ts = arima.ArmaProcess(ar_coef, ma_coef)
    print('Is the time series stationary? ' + str(ts.isstationary))
    print('Is the time series invertable? ' + str(ts.isinvertible))
    return(pd.Series(ts.generate_sample(120), index = dates))
ts_series_ar2 = ARMA_model(ar_coef = [1, .75, .25], ma_coef = [1])

plot_ts(ts_series_ar2, lab = 'AR(2) process')
acf_pacf_plot(ts_series_ar2)
```


## The Autoregressive Model   

Example model summary for $AR(0.75, 0.25)$ model:   

- Both AR coefficients are statistically significant    
- Variance term is statistically significant   
  

```{python, echo=FALSE}
def model_ARIMA(ts, order):
    model = ARIMA(ts, order = order)
    model_fit = model.fit()
    print(model_fit.summary())
    return(model_fit)
ar2_model = model_ARIMA(ts_series_ar2, order = (2,0,0))
```


## The Moving Average Model    

A **moving average** model of order $q$, $MA(q)$, uses the last q error terms or shocks:    

\begin{align}
y_t &= \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}\\
&where\\
\theta_k &= model\ coefficient\ at\ lag\ k\\
y_t &= observation\ at\ time\ t\\
\epsilon_t &= innovation\ or\ error\ at\ time\ t;\ \sim \mathtt{N}(0,\sigma^2)
\end{align}     

- An MA process has the following properties:   
   * For autocorrelation, $\rho_0 = 1$ always    
   * Number of $\rho_k \ne 0$, $= q$    
   * Shocks die off quickly in MA processes    
- MA model assumes stationary time series    


## The Moving Average Model    

A **moving average** model of order $q$, $MA(q)$, uses the last q error terms or shocks:  

$$y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$$

We can also write the MA model in terms of estimated value, $\hat{y}_t$: 

\begin{align}
\hat{y}_t &= \mu + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}\\
& Therefore,\\
\epsilon_t &= y_t - \hat{y}_t \sim \mathtt{N}(0,\sigma^2);\ error\ at\ time\ t \\
\end{align}


## The Moving Average Model  

How can we understand the MA model?   

- Model matrix of MA(2) model

$$
A = 
\begin{bmatrix}
y_t,\ \epsilon_{t-1},\ \epsilon_{t-2}\\
y_{t-1},\ \epsilon_{t-2},\ \epsilon_{t-3}\\
y_{t-2},\ \epsilon_{t-3},\ \epsilon_{t-4}\\
\vdots,\ \ \ \ \ \vdots,\ \ \ \ \ \vdots\\
y_{2},\ \ \ \ \epsilon_{1},\ \ \ \ \epsilon_{0}\\
y_{1},\ \ \ \ \epsilon_{0}\ \ \ \, 0\\
y_0,\ \ \ \ 0,\ \ \ \ 0
\end{bmatrix}
$$


- MA model is a **nonlinear model!**; must compute $\epsilon_{t}$ at each time step   

- The value of $\epsilon_t$ dependents on $[ \epsilon_{t-1}, \epsilon_{t-2}, ..., \epsilon_{t-q}]$ 

- The $\epsilon_k$s are **unobservable**!  

- So, fitting requires **nonlinear iteratively rewieighted least squares**


## The Moving Average Model   

Example of an MA(1) model with coefficients $(1, -0.75)$    

- The time series looks fairly random    
- The ACF has 1 statistically significant nonzero lag value   


```{python, echo=FALSE}
ts_series_ma1 = ARMA_model(ar_coef = [1], ma_coef = [1, -0.75])
plot_ts(ts_series_ma1, lab = 'MA(1) process')
acf_pacf_plot(ts_series_ma1)
```


## The Moving Average Model    

Example model summary for $MA(-0.75)$ model:    

- The MA coefficient is statistically significant     
- Notice that true value is within the confidence interval 
- Confidence interval is wide   


```{python, echo=FALSE}
ma1_model = model_ARIMA(ts_series_ma1, order = (0,0,1))
```

## Autoregressive Moving Average Model    

We can combine AR and MA terms to create the **autoregressive moving average (ARMA)** model of order $(p,q)$:  

$$y_t =  \phi_1 y_{t-1} + \phi_2 y_{t-2} ,\ldots, + \phi_p y_{t-p} +
 \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$$


- Fit ARMA model by solving a nonlinear equatioin:    

$$y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} ,\ldots, - \phi_p y_{t-p} = 
\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$$

- Can write as polynomial equation in terms of coefficient vectors $\Phi = [1, \phi_1, \phi_2, \ldots, \phi_p ]$, $\Theta = [1, \theta_1, \theta_2, \cdots, \theta_q]$:     
$$(1-\Phi)Y = \Theta \epsilon$$

- ARMA model assumes stationary time series    

## The ARIMA Model

The integrative model addresses certain non-stationary components of a time series   

- Random walks   
- Trends   
- Based on difference operator   
  * Typically first order difference   
  * Seasonal and non-seasonal differences      
  * Is deterministic, no model coefficient to estimate      



## The ARIMA Model   

The **autoregressive integrative moving average (ARIMA)** model includes AR, integrative and MA terms  

- The order of an ARIMA is specified as (p,d,q)    
  * p is the AR order   
  * d is the order of differencing   
  * q is the MA order    
- The integrative term helps transforms trend and random walks to stationary process      
- Does not account for seasonal effect     


## The ARIMA Model   

The **autoregressive integrative moving average (ARIMA)** model includes AR, integrative and MA terms  

- Formulate an $ARIMA(1,1,1)$ model     
- Start with $ARMA(1,1) = ARIMA(1,0,1)$ model:

$$y_t -  c - \phi_1 y_{t-1} =  \epsilon_t + \theta_1 \epsilon_{t-1}$$
 
- Take the first difference of the observations, $y_i$, to find the formulation of the $ARIMA(1,1,1)$ model.

$$y_t -  y_{t-1} - \phi_1( y_{t-1} - y_{t-2}) =  \epsilon_t + \theta_1 \epsilon_{t-1}$$
 
 
- Applied same algebra to finding polynomial formulations for higher order ARIMA models.


## Seasonal Models   

Several possible seasonal models   

- Seasonal effects can be periodic or single event (e.g. holiday, game day, etc.)   
- Linear regression model to find effect for each time step in period    
- STL decomposition    
- Fourier decomposition    
    - Flexible  
    - Accommodates multiple periods of seasonality    
    - Used by PROFIT model, [Statsmodels](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_seasonal.html) and others 
- SARIMAX, the S term   
- Each model requires:    
   * Known period of the cycle or time of seasonal event   
   * Additive or logarithmic transformation   


## SARIMAX Model  

The SARIMAX model adds seasonal and exogenous terms   

- ARIMA terms are same, (p,d,q)       
- Seasonal terms:    
   * ARIMA seasonal model, order (P,D,Q,S)    
   * Must specify period, S, seasonal difference order, D
- Order of SARIMAX model is specified as (p,d,q)(P,D,Q,S)   
- See [Statsmodels State Space User Guide](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html) for more details and examples    


## SARIMAX Model       

SARIMA model can be formulated as an additive or a multiplicative model of the components of a non-seasonal ARIMA model and the seasonal ARIMA model    

- Additive model can be expressed: $SARIMA \big[ (p,d,q) + (P,D,Q,S) \big]$    

- Multiplicative model can be expressed: $SARIMA \big[ (p,d,q)(P,D,Q,S) \big]$   

- Example, a multiplicative $SARIMA(1,1,1)(1,1,1,S)$ model:   

$$(y_t - \phi_1 y_{t-1})(y_t - \Phi_1 y_{t-S})(1-y_{t-1})(y_t - y_{t-S}) = (\epsilon_t - \theta_1 \epsilon_{t-1})(\epsilon_t - \Theta_1 \epsilon_{t-S})$$    

- Continue same algebra to find the polynomial of any $(p,d,q)(P,D,Q,S)$ order    



## SARIMAX Model

The SARIMAX model (with no exogenous variables) is formulated    

$$
\phi_p(Y) \tilde{\phi}_P(Y^*) \nabla^d \nabla^D y_t =  A(t) + \theta_q(Y) \tilde{\theta}_Q(Y^*)
$$


- $\phi_p(Y)$ and $\tilde{\phi}_P(Y^*)$ are the AR polynomials non-seasonal and seasonal terms   
- $\theta_q(Y)$ and $\tilde{\theta}_Q(Y^*)$ are the MA polynomials non-seasonal and seasonal terms  
- $\nabla^d$ and $\nabla^D$ are the non-seasonal and seasonal differencing operators      
- $A(t)$ is the trend term    


## SARIMAX Model


SARIMAX model can include exogenous variables, $x$, leading to a new system of equations":   

$$
y_t = \beta_t x_t + \mu_t \\
\phi_p(Y) \tilde{\phi}_P(Y^*) \nabla^d \nabla^D \mu_t =  A(t) + \theta_p(Y) \tilde{\theta}_P(Y^*)
$$

- Time series model for latent variable, $\mu_t$    
- $\mu_t$ acts as the intercept term for the regression model for $x_t$    
- The coefficient vector, $\beta$, contains the **effect sizes** for the exogenous variables    


## Forecasting with the ARIMA model

Goal of forecasting is to compute a point estimates, $\hat{y}_{t+H}$, $H$ time steps in the future     

- Use the time series history, $[y_t, y_{t-1}, y_{t-2}, \ldots]$    

- Example, compute a forecast for a stationary time series with an ARIMA$(p,d,q)$ model    
- Recursively apply a one step ahead forecast     

- For the forecasted value $\hat{y}_{t+H}$ we have no estimated error:   

$$\epsilon_{t+H} = y_{t+H} - \hat{y}_{t+H}$$    


## Forecasting with the ARIMA model

Goal of forecasting is to compute a point estimates, $\hat{y}_{t+H}$, $H$ time steps in the future     
 
- Example, make a forecast with an ARIMA(2,1,1) model:    

$$y_t -  y_{t-1} - \phi_1( y_{t-1} - y_{t-2}) - \phi_2 ( y_{t-2} - y_{t-3}) =  \epsilon_t + \theta_1 \epsilon_{t-1}$$

- Model the one step ahead forecast as:     

$$\hat{y}_{t+1}= y_{t} + \phi_1( y_{t} + y_{t-1}) + \phi_1( y_{t-1} - y_{t-2}) + \theta_1 \epsilon_{t}$$

- Two step ahead forecast is computed using a recursive relationship    



## Forecasting with the ARIMA model     

Two step ahead forecast is computed using a recursive relationship   

- But, no way to compute the error terms beyond the current observation at time $t$    

- Replace $\epsilon_{t+1}$ with the last known residual as $e_{t+2} = e_t$:    

$$\hat{y}_{t+2}= \hat{y}_{t+1} + \phi_1( \hat{y}_{t+1} - y_{t}) + \phi_1( y_{t} - y_{t-1}) + \theta_1 e_{t}$$

- Take recursion one more step, set error terms, $\epsilon_i = 0$:    

$$\hat{y}_{t+3}= \hat{y}_{t+2} + \phi_1( \hat{y}_{t+2} - \hat{y}_{t+1}) + \phi_1( \hat{y}_{t+1} - y_{t})$$

- Continue this recursion for as many time steps as desired   

- Same algebra used to work out the forecasting axolynomial of higher order ARIMA model  


## Evaluating and Comparing Time Series Models   

How can we evaluate time series models?      

- **Confidence intervals**    
    - Fit to observations   
    - Forecasts  
- RMSE; compare forecast to actual values    
    - Fit to observations  
    - Forecasts  
- Could use log-likelihood; $log \big(p(X|\theta) \big)$     
  * $\theta = model\ parameters$    
  * Use **score function** $= -2\ log(likelihood) = -2\ log \big(p(X|\theta) \big)$   
  * But, score decreases with model complexity    
- Need to adjust for number of model parameters   
  * We always prefer simpler models; fewer parameters to learn    
  * **Akaki Information Criteria (AIC)**     
  * **Bayes Information Criteria (BIC)**   

## Forecast Confidence Intervals    

When presented with any forecast, the first question should be 'what are the errors'?      

- Forecast are a **extrapolations** of the model into the future    

- Cannot use the residuals, $\epsilon_i$      
    - Errors themselves must reflect the uncertainty beyond the range of available observations     

- The forecast is a **point estimate**, which has a **confidence interval**. There are several ways which are commoinly used to compute confidence intervals:   

## Forecast Confidence Intervals   

Several ways which are commonly used to compute confidence intervals:     

- **Theoretical sampling distribution:** 
   - Compute confidence intervals from a theoretical sampling distribution     
   - Deriving a suitable distribution can be difficult    
   - Actual sampling distribution is invariably different 
   
- **Bootstrap resampling:**     
    - Use bootstrap resampling to compute a non-parametric sampling distribution  
    - Bootstrap resampling of time series requires specific sampling methods    
    - Confidence intervals are computed from emperical sampling distribution   
    
- **Back-testing:**    
    - Time series model is trained on a training portion of the data    
    - Forecasts are made some time steps ahead, and errors calculated    
    - Move the training and forecast windows    
    - Sampling distribution and the confidence intervals are computed    
    - Compute RMSE, MAE, etc. from the back test         


## Evaluating and Comparing Time Series Models   

Akaki Information criteria, AIC    

\begin{align}
AIC &= 2\ k - 2\ ln(\hat{L})\\
&where\\
\hat{L} &= p(x| \hat\theta) = the\ likelihood\ given\ the\ fitted\ model\ parmaters\ \hat\theta \\
x &= observed\ data\\
k &= number\ of\ model\ parameters
\end{align}

- AIC penalizes the score function for the complexity of the model by $2\ k$    
- Model with lowest AIC is best    


## Evaluating and Comparing Time Series Models   

Bayes Information criteria, BIC    

\begin{align}
BIC &= ln(n)\ k- 2\ ln(\hat{L})\\
&where\\
\hat{L} &= p(x| \hat\theta) = the\ likelihood\ given\ the\ fitted\ model\ parmaters\ \hat\theta\\
x &= observed\ data\\
k &= number\ of\ model\ parameters\\
n &= number\ of\ observations   
\end{align}

- BIC penalizes the score function for the complexity of the model, $k$    
- BIC adjusts for number of samples used to learn the $k$ model parameters     
- Model with lowest BIC is best    
- BIC is often preferred to AIC for time series models    


## Evaluating and Comparing Time Series Models   

Can compare and select models using BIC or AIC     

- Backwards step-wise model selection    
  1. Start with initial order of the model; e.g. $(p,d,q)(P,D,Q,S)$   
  2. Fit (learn) the model parameters   
  3. compute the BIC, and if reduced consider this a better model   
  4. Reduce the order of one of the model components    
  5. Repeat steps 2, 3 and 4 until no further improvement    
- Tips for comparing models:
  * BIC and AIC are approximations; small changes (3rd or 4th decimal) are not important     
  * If close tie for best model pick the simpler (lower order) case   
  * Often best to consider integrative terms, $d$ and $D$, separately  
  
## SARIMAX Example

Example: 3 time series of Australian production   

```{python, echo=FALSE}
CBE = pd.read_csv('../data/cbe.csv')
CBE.index = pd.date_range(start = '1-1-1958', end = '12-31-1990', freq = 'M')

f, (ax1, ax2, ax3) = plt.subplots(3, 1);
CBE.choc.plot(ax = ax1);
CBE.beer.plot(ax = ax2);
CBE.elec.plot(ax = ax3);
ax1.set_ylabel('Choclate');
ax2.set_ylabel('Beer');
ax3.set_ylabel('Electric');
ax3.set_xlabel('Date');
ax1.set_title('Three Australian production time series');
plt.show()
```


```{python, echo=FALSE}
CBE['elec_log'] = np.log(CBE.elec)
elect_decomp = decomp_ts(CBE.elec_log)

```


## SARIMAX Example

Use the SARIMAX model to find the best ARIMA fit of log(electric production)   

```{python}
Log_electric = CBE.elec_log[:'1989-12-31']
best_model = pm.auto_arima(Log_electric, start_p=1, start_q=1,
                             max_p=3, max_q=3, m=12,
                             start_P=0, seasonal=True,
                             d=1, D=1, trace=True,
                             information_criterion = 'bic',
                             error_action='ignore',  # don't want to know if an order does not work
                             suppress_warnings=True,  # don't want convergence warnings
                             stepwise=True)  # set to stepwise
```


## SARIMAX Example

Example of SARIMAX model of order (0.1.1)(0,1,2,12) for monthly electric production series    

- Model selected by backwards step-wise method    
- First order model integrative term and MA(1)     
- First order model integrative term and MA(1) for period 12 seasonality   


```{python, echo=FALSE}
best_model.summary()
```

## SARIMAX Example

Predictions for the last 12 months of the time series   

```{python}
prediction = pd.Series(best_model.predict(n_periods=12), 
                       index = pd.date_range(start = '1990-01-31', end = '1990-12-31', freq = 'M'))
```

```{python, echo=FALSE}
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4));
CBE.elec_log.plot(ax=ax[0]);
prediction.plot(ax=ax[0]);
ax[0].set_title('Full log electric use with predicted values');
ax[0].set_ylabel('Log electric power use');
ax[0].set_xlabel('Date');

CBE.elec_log['1990-01-31':].plot(ax=ax[1]);
prediction.plot(ax=ax[1]);
ax[1].set_title('Log electric use for 12 months \nwith predicted values');
ax[1].set_xlabel('Date');
plt.show()
```


## SARIMAX Example

Residuals of the predictions    

```{python}
residuals = CBE.elec_log['1990-01-31':] - prediction

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))
_=ss.probplot(residuals, plot = ax);
plt.show()
```


## PROFIT Model   

The PROFIT model from Meta Research is a **deterministic model** well suited for some business forecasting    

- Uses a sophisticated piece-wise linear trend model    
   - Trend computed between breakpoints     
   - Breakpoints found with Bayesian model     
   - Complex trend model **confounded by random walks**   
   
- Multi-seasonal component modeled modeled by Fourier decomposition    
   - Multiple harmonics per seasonal period    
   - Flexible modeling of complex seasonal patterns    
   
- Supports exogenous variables    

- PROFIT model assumes residual is non-informative     

## PROFIT Model   

The PROFIT model from Meta Research is a **deterministic model** well suited for some business forecasting       

- PROFIT model **assumes residual is non-informative**      
   - ACF and PACF must have no significant nonzero lags     

- If significant ACF and PACF use SARIMAX    
   - Uses information in stationary residual   
   - Gives **superior results for time series with stochastic components**   
   


## Summary

Time series models must account for serial correlation   

- e.g. ARIMA and SARIMAX     
- AR components for serial correlation of values       
- MA components for serial correlation of errors      
- Integrative components for random walk and trend, I 
- Seasonal, (P,D,Q,S)    
- Exogenous variables, X    

## Summary

Evaluation and model comparison    

- RMSE   
- AIC and BIC, penalize score function for model complexity    
- Use BIC (or AIC) to perform backwards step-wise model selection    


## Exponential Smoothing Models    

[Exponential smoothing models](https://en.wikipedia.org/wiki/Exponential_smoothing) are simple and widely used    

- Consider the simple first order model   
- Set initial conditions:    

$$ s_0 = y_0 $$

- The smoothed update is:     

$$ s_t = \alpha y_t + (1-\alpha) s_{t-1}\\ 
= s_{t-1} \alpha(y_t - s_{t-1}),\\ 
t \gt 0 $$

- And, the smoothing coefficient is, $0 \le \alpha \le 1$    
- But, model only works if no trend     


## Exponential Smoothing Models   

Decay and exponential smoothing    

- We can understand the smoothing parameter $\alpha$ in terms of a **decay constant**, $\tau$     

$$\alpha = 1 - e^{\big( \frac{\Delta T}{\tau} \big)}$$

- An innovation or shock has an effect for all future time   
- Effect decays exponentially with time, $\Delta T$   


## Exponential Smoothing Models   

Can extend exponential smoothing model to accommodate trend     

- Algorithm known as **double exponential smoothing** or **Holt-Winters double exponential smoothing**      
- Update smoothed values and slope at each time step   
- Start with initial values     

$$s_1 = y_1\\
b_1 = y_2 - y_1$$

- Update relationships for both smoothed value and slope    

$$s_t = \alpha y_t + (1-\alpha) (s_{t-1} + b_{t-1})\\
b_t = \beta(s_t - s_{t-1}) + (1 - \beta)b_{t-1}$$

- Additional slope smoothing hyperparameter, $0 \le \beta \le 1$    
- Use **third order** update includes seasonality in **Holt-Winters smoother**   


## Exponential Smoothing Models   

Exponential smoothing models are useful for forecasting    

- Forecast dependent on the choice of smoothing parameters     
- Can forecast with first, second, third order models    
- For second order model (with trend) the forecast $m$ steps ahead is:   


$$F_{t+m} = s_t + m b_t $$

- Third order update include seasonal terms   

- Holt-Winters smoother is a **linear model!**  


## Exponential Smoothing Models   

Example of smoothing trend plus white noise series    

- Decreasing the smoothing parameter, $\alpha$, increases smoothing     
- Additionally, smooth trend    
- Additional examples in [Statsmodels user documentation](https://www.statsmodels.org/stable/examples/notebooks/generated/exponential_smoothing.html)   


```{python, echo=FALSE}
import warnings

nr.seed(6677)
def trend(start = '1-1990', end = '1-2015', freq = 'M', slope = 0.02, sd = 0.5, mean = 0):
    dates = pd.date_range(start = start, end = end, freq = freq)
    trend = pd.Series([slope*x for x in range(len(dates))],
                    index = dates)
    trend = trend + nr.normal(loc = mean, scale = sd, size = len(dates))
    return(trend)
                              
trends = trend()   

fig, ax = plt.subplots(figsize=(7,4))
_=ax.plot(trends, label='original', linewidth=2)

Holt_model = ExponentialSmoothing(trends, trend='add', seasonal=None)

for smoothing in [0.5,0.2,0.05]:
    warnings.filterwarnings("ignore")
    Holt_model_fit = Holt_model.fit(smoothing_level=smoothing, smoothing_slope=0.05)
    _=label = 'smoothing = ' + str(smoothing)
    _=ax.plot(Holt_model_fit.fittedvalues, label = label, linewidth=1)
_=ax.set_xlabel('Date')
_=ax.set_ylabel('Value')
_=ax.set_title('Smoothing of trend series with white noise')
_=plt.legend()    
plt.show()
```

 



  