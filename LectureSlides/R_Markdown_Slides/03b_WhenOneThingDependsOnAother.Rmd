---
title: "When One Thing Depends on Another; Conditional Probability"
author: "Steve Elston"
date: "09/18/2023"
output:
  powerpoint_presentation: default
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```


-----------------------------------------------------------------

## Introduction      

Many real-world random variables depend on other random variables    

- Statistical models of complex processes invariably require the use of **conditional probability distributions**     

- **Conditional probability** is the probability that event A occurs given that event B has occurred 
- Write the conditional probability of A given B as:    

$$P(A|B)$$

- **Example:** Model of the probability of contracting the infectious disease, depends on other variables
  - In more technical terms, the probability of contracting the disease is **conditional** on other random variables.   
  - Age, contact with people carrying the disease, immunity, etc. 


--------------------------------------------------

## Properties of Conditional Probability

Example: 

```{r ConditionalSets, out.width = '30%', fig.cap='Example of conditional probability of discrete events; credit, Wikipedia commons', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Prob1.png"))
```

- **Sample space** is the space of all possible events in the set $S$     

- Sample space is divided into several **subspaces** or **subsets**, $A$, $B$ and $C$    

- **Intersection** is where the two sets overlap occur in both $A$ and $B$ $\rightarrow A \cap B$  



--------------------------------------------------

## Properties of Conditional Probability

Example: 

```{r ConditionalSets2, out.width = '20%', fig.cap='Example of conditional probability of discrete events; credit, Wikipedia commons', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/Prob1.png"))
```

- **Intersection**, $A \cap B$ , is where the two sets overlap   
  - The probability of the intersection is the product of two probabilities:    
  1. $P(B)$ since B must be true to be in this intersection.    
  2. $P(A|B)$ since A must also occur when B is occurring
  - The result is:   

$$P(A \cap B) = P(A|B) P(B)$$


-------------------------------------------------

## Properties of Conditional Probability

$$P(A \cap B) = P(A|B) P(B)$$

Rearranging terms we get the following: 

\begin{align}
P(A|B) &= \frac{P(A \cap B)}{P(B)} \\
& = \frac{\frac{2}{10}}{\frac{4}{10}} = \frac{2}{4} = \frac{1}{2}
\end{align}

We could have, just as well, written the last equation as: 

$$P(B \cap A) = P(B|A)P(A)$$

Now, the probability of an identical event in the same intersection:   

$$P(A \cap B) = P(A|B) P(B) = P(B|A)P(A) = P(B \cap A)$$

**Factorization** of a probability function is a key tool: notice that the factorization of a conditional probability distribution in not unique  


----------------------------------------------------

## Set Operations and Probability

Set operations are applied to probability problems    

1. **Intersection:**   

$$P(A \cap B)  = P(A|B)P(B) = P(B|A)P(A)$$

2. **Union:** is the sum of the probabilities of the sets minus the intersection between the sets:

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

3. **Negation:** Example, compute the probability of an event being in subset $A$ but not in $B$:   

$$P(A\ and\ \neg B) = P(A) - P(B \cap A)$$


**Example:** We can apply **[De Morgan's Laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws)**:

\begin{align}
P(\neg (A \cup B)) &= P(\neg A\ \cap \neg B)\\
P(\neg (A \cap B)) &= P(\neg A\ \cup \neg B)
\end{align}


--------------------------------------------------------

## Independence and Mutual Exclusivity

The factorization of probability distributions can be simplified if events are either **independent** or **mutually exclusive**    

- At first glance, these concepts may seem similar
  - Are quite different
  - Very different implications

- **Independence** of sets $A$ and $B$ means the occurrence of an event in $A$, does not have any dependency on an event in $B$   

- **Mutual exclusivity** means events cannot occur in both the sets $A$ and $B$

-----------------------------------------

## Independence

Express independence of random variables mathematically:   

\begin{align}
P(A|B) &= P(A)\\
P(A| \neg B) &= P(A)\\
P(A\ \cap B) &= P(A|B)P(B) = P(A)P(B)\\ 
P(A\ \cup B) &= P(A) + P(B) - P(A)P(B)\\
\end{align}
 
But independence of A given B does not imply independence of B given A:

$$P(A|B) = P(A) \nLeftrightarrow P(B|A) = P(B)$$

In other words, we need to pay attention to if A is independent of B or B is independent of A  
- One or the other could be true    
- Both could be true

---------------------------------------------

## Mutual Exclusivity     

If the intersection between events is an empty set:  

$$A \cap B = \emptyset$$     

Then, events in A are **mutually exclusive** of events in B:

\begin{align}
P(A \cup B) &= P(A) + P(B)\\
P(A|B) &= P(B|A) = 0\\
P(A| \neg B) &= \frac{P(A)}{1 - P(B)}
\end{align}

And,     
$$A \perp B \iff B \perp A$$

----------------------------------------------------


## Conditional Distributions and Bayes' Theorem

**Bayes' theorem**, also known as **Bayes' rule**, is a powerful tool to think about and analyze conditional probabilities        

- We can derive Bayes Theorem starting with the following relationships: 

$$P(A \cap B) = P(A|B)P(B)\\
P(B \cap A) = P(B|A)P(A)$$

- Now:

$$P(A \cap B) = P(B \cap A)$$

- Which leads to:

\begin{align}
P(A|B)P(B) &= P(B|A)P(A)\\
P(A|B) &= \frac{P(B|A)P(A)}{P(B)}
\end{align}

**Which is Bayes' theorem!** 


----------------------------------------------------------

## Interpreting Bayes Theorem

How can we interpret Bayes' theorem in a useful way?  

- Consider an example using Bayes Theorem for an hypothesis test given some data or **evidence**    

- We must make an assertion of our prior probability that the hypothesis is true   

$$prior(hypothesis)$$     

- We also must choose a likelihood function of the evidence given the hypothesis   

$$Likelihood(evidence\ |\ hypothesis)$$   

----------------------------------------------------------

## Interpreting Bayes Theorem

Now, we can think of Bayes' theorem in the following terms:

$$Posterior(hypothesis\ |\ evidence) =\\ \frac{Likelihood(evidence\ |\ hypothesis)\ prior(hypothesis)}{P(evidence)}$$

- We discuss selection of prior probability distributions and likelihood functions in subsequent lectures    

- The denominator $P(evidence)$ or **partition function** is problematic        

- Required to normalize the posterior distribution to range:     
$$0 \le Posterior(hypothesis\ |\ evidence) \le 1$$  

## Interpreting Bayes Theorem

Denominator must account for all possible outcomes, or alternative hypotheses, $h'$:   

$$Posterior(hypothesis\ |\ evidence) =\\ \frac{Likelihood(evidence\ |\ hypothesis)\ prior(hypothesis)}{\sum_{ h' \in\ All\ possible\ hypotheses}Likelihood(evidence\ |\ h')\ prior(h')}$$

This is a formidable problem!    


## Bayes Theorem Example

Hemophilia is a serious genetic condition expressed on any X chromosome    

- Women have two X chromosomes and are unlikely to exhibit hemophilia   
  - One X chromosome inherited from each parent    
  - Must inherit hemophilia from both parents   

- Men have one X chromosome and one Y chromosome   
  - Inherit Y chromosome from the father   
  - Inherit X chromosome, and possibly hemophilia, from the mother    
  
- Say a woman has a brother who exhibits hemophilia   
  - X chromosome expression is $\theta = 1.0 \rightarrow$ brother has hemophilia with $P = 1.0$
  - Woman's father does not exhibit hemophilia, $\theta = 0 \rightarrow$ father has hemophilia with $P = 0.0$  
  - Our prior probability that she carries the genetic marker for hemophilia $\theta = 0.5 \rightarrow$ Woman's X chromosome has P = 0.5 that it is from mother, who carried the marker with $P = 1.0$


## Bayes Theorem Example  

As evidence the woman has two sons (not identical twins) with no expression of hemophilia      

- What is the likelihood for the two sons $X = (x_1,x_2)$ not having hemophilia?   

- Two possible cases here
  - Case where woman caries one X chromosome with hemophilia expression, $\theta = 1$, and probability of passing to son = 0.5
  - Case where woman does not carry an X chromosome with hemophilia expression, $\theta = 0$   
 
$$
p(x_1=0, x_2=0 | \theta = 1) = 0.5 \times 0.5 = 0.25  \\
p(x_1=0, x_2=0 | \theta = 0) = 1.0 \times 1.0 = 1.0 
$$
 
Note: we are neglecting the possibility of a mutations in one of the sons 


## Bayes Theorem Example  

Use Bayes theorem to compute probability woman carries an X chromosome with hemophilia expression, $\theta = 1$

$$
p(\theta=1 | X) = \frac{p(X|\theta=1) p(\theta=1)}{p(X|\theta=1) p(\theta=1) + p(X|\theta=0) p(\theta=0)} \\
= \frac{0.25 * 0.5}{0.25 * 0.5 + 1.0 * 0.5} = 0.20
$$
Where:   
\begin{align}
Likelihood &= p(X|\theta=1) = 0.5 \times 0.5 = 0.25\\
Prior\ probability &= p(\theta=1) = 0.5\\
Probability\ of\ hypothesis\ no\ marker &= p(\theta=0) = 1 - p(\theta=1) = 0.5 
\end{align}

The **evidence** of two sons without hemophilia causes us to update our **belief** that the probability of the woman carrying the disease


----------------------------------------------------------

## Marginal Distributions   

In many cases we are interested in the **marginal distribution**      

- Example, it is often the case that only one or a few parameters of a joint distribution will be of interest     
  - In other words, we are interested in the marginal distribution of these parameters   
  - The denominator of Bayes theorem, $P(data)$, can  be computed as a marginal distribution   

- Consider a multivariate probability density function with $n$ variables, $p(\theta_1, \theta_2, \ldots, \theta_n)$     
  - **Marginal distribution** is the distribution of one variable with the others integrated out.  
  - Integrate over all other variables $\{ \theta_2, \ldots, \theta_n \}$ the result is the marginal distribution, $p(\theta_1)$:       

$$p(\theta_1) = \int_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)\ d\theta_2, \ldots, d \theta_n$$

- For discrete distribution the above is a summation 
$$p(\theta_1) = \sum_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)$$

-----------------------------------------

## Example: Marginal Distribution   

**Marginal distributions** of multivariate Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.0 \\ 0.0 & 1.0 \end{bmatrix}$

Marginal distributions **displayed on margins** of scatter plot

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import scipy.stats as ss
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python, echo=FALSE}
def plot_bi_variate(x, title='1000 draws from a bivariate Normal distribution'):
    ## print the marginal mean and variance    
    dims = ['x','y']
    for i,dim in enumerate(dims):     
      print(f"For {dim} mean = {np.mean(random_points[:,i])}  variance = {np.var(random_points[:,i])}")
    ## Plot bi-variable points
    g = sns.jointplot(x=random_points[:,0], y=random_points[:,1], kind="reg")
    plt.show()


## Define the covariance and mean of the bivariate Normal. 
sigma = np.array([[1,0.0], [0.0, 1]])
mu = np.array([0.0, 0.0])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```


-----------------------------------------

## Example: Marginal Distribution   

**Marginal distributions** of multivariate Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.5 \\ 0.5 & 1.0 \end{bmatrix}$

Marginal distributions **displayed on margins** of scatter plot

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,0.5], [0.5, 1.0]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```


-----------------------------------------

## Example: Marginal Distribution   

**Marginal distributions** of multivariate Normal with $\mu = [0,0]$ and $\sigma = \begin{bmatrix} 1.0 & 0.9 \\ 0.9 & 1.0 \end{bmatrix}$

Marginal distributions **displayed on margins** of scatter plot

```{python, echo=FALSE}
## Define the covariance of the bivariate Normal. 
sigma = np.array([[1,0.9], [0.9, 1.0]])
## Sample 500 realizations from the bivariate Normal
random_points = nr.multivariate_normal(mean=mu, cov=sigma,  size=1000)

## Plot the result
plot_bi_variate(random_points)
```



------------------------------------------

##  Conditional Probability Example

A simple and widely used example of using conditional probabilities to work out the chance of having a rare disease.

- Sickle Cell Anemia is a serious, but fairly rare, disease    

- The probability that a given patient, drawn at random from the population of all people in the United States, has the disease is $P(S) = \frac{1}{3200} = 0.0003125$. We can describe the possible events in diagnosing this condition as: 
  - $S \Rightarrow$ a patient has the disease. 
  - $S' \Rightarrow$ a patient does not have the disease.
  - $\oplus \Rightarrow$ patient tests positive.
  - $- \Rightarrow$ a patient tests negative.
  
------------------------------------------

##  Conditional Probability Example

What if a medical company claims that it has developed a test that is 99% accurate?   

- We can write:
  - $P(S |\oplus) = 0.99$
  - $P(S'|-) = 0.99$
  
- On the surface, it seems that a 99% reliable test is rather good   
  - On average, 99 people out of 100 who have the disease will be identified and treated  
  - But, dig into the conditional probabilities and make sure! 



------------------------------------------

##  Conditional Probability Example

From the root the **directed acyclic graphical model (DAG)** defines a conditional dependency structure

- Goal: Evaluate the medical test as a **decision rule** for treatment    

- Summarize the conditional probabilities for these outcomes:
  - $P(\oplus | S)$: Conditional probability the test correctly identifies patient with disease 
  - $P(- | S)$: Conditional probability of a negative test for a patient with the disease; **Type II Error** or **False Negative**
  -  $P(\oplus | S')$: Conditional probability that a patient with no disease tests positive; **Type I Error** or **False Positive**
   - $P(- | S')$: Conditional probability of a negative test for a patient with no disease  
   
```{r CondTree, out.width = '30%', fig.cap='Graph showing dependency of conditional distribution', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/CondTree.jpg"))
```

------------------------------------------

##  Conditional Probability Example

Four possible outcomes shown using a **confusion matrix** or **truth table**   

- Table shows conditional probabilities of each outcome  

| | Positive Test | Negative Test |
|:---:|:---:|:---:|
|**Disease**| True Positive Rate | False Negative Rate |
|**No Disease**| False Positive Rate | True Negative Rate |

- **Tip:** Make sure the numbers in your confusion matrix sum to 1.0


## Summary      

- Conditional probability     
  - One random variable depends on another    
  - But not commutable  
$$P(A|B) = P(A) \nLeftrightarrow P(B|A) = P(B)$$  

- Mutually exclusivity   
$$P(A|B) = P(A) + P(B)$$

- Independence      
$$P(A|B) = P(A)$$


## Summary   

- Bayes' theorem     
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
- Marginal distribution   


$$
p(\theta_1) = \int_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)\ d\theta2, \ldots, d \theta_n\\ $$

$$
p(\theta_1) = \sum_{\theta_2, \ldots, \theta_n} p(\theta_1, \theta_2, \ldots, \theta_n)
$$
