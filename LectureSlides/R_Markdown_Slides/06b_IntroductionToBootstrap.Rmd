---
title: "Introduction to Nonparametric Bootstrap Methods"
author: "Steve Elston"
date: "10/09/2023"
output:
  powerpoint_presentation: default
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:/Users/steph/anaconda3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```


## Introduction

*“There were others who had forced their way to the top from the lowest rung by the aid of their bootstraps.”     
James Joyce, ‘Ulysses’ 1922*

Bootstrap and re-sampling methods are widely applicable statistical methods   

- Resampling methods are products of the computer age    

- Use computational resources unimaginable in the early 20th Century    

- Repeatedly re-sampling the data with nonparametric model relaxes some assumptions of classical statistical methods

- Re-sampling methods draw heavily on the law of large number and the central limit theorem      


## Types of Resampling Methods    

Commonly used re-sampling methods include:     

- **Randomization or Permutation methods:** aka exact tests     
   - Have a long history; e.g. Fisher’s exact test (1922)     
   - Practical approximate algorithms for larger data sets in computer era      
   
- **Cross validation:** resample into multiple folds without replacement     
   - Leave n out method      
   - Has origins in the 1950s     
   - Widely used to evaluate machine learning (ML) models      
   
- **Jackknife:** leave one out re-sampling       
   - Leave one out method   

   - Early general purpose resampling method   
   - Has origins in the 1950s    
   
- **Nonparametric Bootstrap:** resample with equivalent size and replacement - our focus here    
   - Published by Prof Brad Efron in 1979      
   

## Nonparametric vs. Parametric Statistical Model    

Many familiar statistic models are **parametric**, being based on a assumed **likelihood**      

- Likelihood models based on a parametric distributions     

- Parametric models have low variance estimates for statistics     
  - But susceptible to poor choice of likelihood model       

- Example, least-squares error model uses a Normal likelihood     
  - Parameters which must be estimated     
  - Location and scale in one-dimension   
  - Betas and covariance in higher dimensions    

   

## Nonparametric vs. Parametric Statistical Model    

**Nonparametric model** not based on a parametric likelihood      

- Use an **empirical distribution** estimated from observations     
  - No likelihood model assumptions    

- Statistical properties estimated from this empirical distribution     
  - Potentially high variance estimates    
  - Need sufficient sample size    
  - Example, mean and variance estimates          

- Examples of nonparametric statistical estimators:     
  - Permutation tests     
  - Jackknife estimates    
  - Nonparametric bootstrap   



## General Characteristics of Nonparametric Resampling Methods    

General characteristics of nonparametric resampling methods include     

- Allow computation of statistics from data samples for statistics with continuous derivatives        

- Repeatedly compute statistics from multiple resamples of dataset     

- The result converges to the sample distribution of the statistic being computed    

- Make minimal distributional assumptions, when compared to classical frequentist statistics      

## Pitfalls of Resampling Methods    

Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! 

- If a sample is biased, the resampled statistic estimate based on that sample will be biased    
   - Results can be no better than the sample you start with  
   - Example; the bootstrap estimate of mean is a sample estimate, $\bar{x}$, not the population parameter, $\mu$ 
   
- The sample variance and CIs can be no better than the sample distribution allows    
   - Often higher variance than parametric models  
   - Be suspicious of overly optimistic confidence intervals    
   - CIs can be optimistically biased        

- Are computationally intensive, but often highly parallelizable   

   
##  Point Estimates vs. Distribution Estimates    

The goal of frequentist statistics is to compute a point estimate and confidence interval      

- Point estimate is the single most likely value for a statistic     
   - Confidence interval expresses the uncertainty of the point estimate  

- Parametric confidence interval based on the properties of some assumed probability distribution   
   - Are there alternatives to this classical frequentist approach?    
   - Here we focus on bootstrap methods which do not require explicit probability model    
   
   
## Bootstrap Distribution    

Rather than computing a point estimate directly, bootstrap methods compute a bootstrap distribution of a statistic    

- Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)     

- Computing bootstrap distribution requires **no assumptions about population distribution!**
  - Bootstrap resampling substitutes computer power for paper and pencil statistician power

- Bootstrap resampling estimates the **bootstrap distribution** of a statistic    
  - Compute mostly likely point estimate of the statistic, or bootstrap estimate   
  - The bootstrap confidence interval is computed from the bootstrap distribution    

## Bootstrap Distribution    

Rather than repeatedly resample the population, bootstrapping repeatedly resamples an original sample    

```{r BootstrapDistribution, out.width = '70%', fig.cap='Resampling to estiamte the bootstrap distribution of a statistic', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BootstrapDistribution.png"))
```

## Overview of the Bootstrap Algorithm    

The bootstrap method follows a simple algorithm. Estimates of the point estimate of a statistic are accumulated by these steps:    

  1. Randomly **Bernoulli sample** sample with size n **with replacement** from an original data sample of n values; Resample is the same size as the original data sample   
  2. Re-compute the statistic with each resample       
  3. Repeat steps 1 and 2 to accumulate the required number of bootstrap samples  
  4. Accumulated bootstrap values form the bootstrap distribution; An estimate of the sample distribution of the statistic     
  5. The mean of the computed statistic values is the bootstrap point estimate of the statistic     
  
## Overview of the Bootstrap Algorithm    

Properties of the nonparametric bootstrap  
  
- By law of large numbers, bootstrap point estimate converges    

- Larger number of resamples lower variance of estimate

- Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 200 bootstrap samples for point estimates     

- Other authors recommend a larger number (e.g. 1,000-2,000) of resamples given low computer cost


## Overview of the Bootstrap Algorithm    

```{r BootstrapMean, out.width = '90%', fig.cap='Outline of bootstrap resampling algorithm to compute mean', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BootstrapMean.png"))
```


## Example; One Sample Bootstrap   

Use sample of standardized scores of highschool students from [UCLA Statistical Consulting](https://stats.idre.ucla.edu/)

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv', index_col=0)
test_scores.head(10)
```

## Example; One Sample Bootstrap   

Histogram of the math scores     

```{python, echo=FALSE}
def plot_hist(x, xlab, title, bins=20, height=6):   
       sns.displot(x, bins=bins, kde=True, height=height, aspect=1.4, linewidth=3)
       plt.axvline(x=np.mean(x), color='red', linestyle='dashed', linewidth=2)
       plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.8)
       plt.xlabel(xlab)
       plt.title(title)

math = test_scores.loc[:,'math']
plot_hist(math, 'Math score', 'Histogram of math scores')
plt.show()
```


## Example; One Sample Bootstrap

Function to compute single sample bootstrap estimate of statistic    

```{python, evl=FALSE}
def bootstrap_statistic(x, b, statistic):
    '''
    Function Computes b one-sample bootstrap estimates of data x
    using statistic function.  
    '''
    n_samps = len(x)
    boot_vals = []
    for _ in range(b):
        ## The heavy work is done here. The statistic is computed 
        ## using the bootstrap sample of the observations 
        boot_vals.append(statistic(nr.choice(x, size=n_samps, replace=True)))
    boot_estimate = np.mean(boot_vals)
    print('Bootstrap point estimate = {:6.2f}'.format(boot_estimate))
    return(boot_estimate, boot_vals)      
    
bootstrap_mean_estimate, boot_means = bootstrap_statistic(math, 200, np.mean)
```


## Example; One Sample Bootstrap

Distribution of 200 bootstrap samples of mean estimates    

```{python, echo=FALSE}
def bootstrap_statistic(x, b, statistic):
    '''
    Function Computes b one-sample bootstrap estimates of data x
    using statistic function.  
    '''
    n_samps = len(x)
    boot_vals = []
    for _ in range(b):
        ## The heavy work is done here. The statistic is computed 
        ## using the bootstrap sample of the observations 
        boot_vals.append(statistic(nr.choice(x, size=n_samps, replace=True)))
    boot_estimate = np.mean(boot_vals)
    print('bootstrap point estimate = {:6.2f}'.format(boot_estimate))
    return(boot_estimate, boot_vals)      
    
bootstrap_mean_estimate, boot_means = bootstrap_statistic(math, 200, np.mean)

plot_hist(boot_means, 'Bootstrap mean values', 'Histogram of bootstrap distribution')
plt.show()
```


## Bootstrap Confidence Intervals   

Distribution of 2000 bootstrap bootstrap confidence intervals?

- Two sided CI percentile method:      
   1. Define confidence level, eg. 95% or $\alpha=0.05$       
   2. Order b bootstrap samples, $s_i$, by value       
   3. Lower CI index; $i = b∗α/2$   
   4. Upper CI index; $i = b∗(1−α/2)$    
   
- Percentile method is know to be biased    
   - Bias correction methods available   
   
- Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 2,000 bootstrap samples to estimate confidence intervals

- Other authors recommend a larger number (e.g. 5,000-20,000) of resamples given low computer cost

## Bootstrap Confidence Intervals    

Bootstrap confidence intervals are known to be biased!    

- Often bootstrap CIs are overly optimistic    

- Bias can be significant for asymmetric distributions    

- In practice, bias corrections are applied   


## Example; One Sample Bootstrap

Bootstrap distribution of 2000 of mean estimates with confidence intervals     

```{python, echo=FALSE}
def bootstrap_cis(boot_samples, alpha=0.05):
      n = len(boot_samples)
      sorted = np.sort(boot_samples)
      index_lci = int(n * alpha / 2)
      index_uci = int(n * (1 - alpha / 2))
      print('At alpha = {0:3.2f}, lower and upper bootstrap confidence intervals = {1:6.2f}   {2:6.2f}'.format(alpha, sorted[index_lci], sorted[index_uci]))
      return(sorted[index_lci], sorted[index_uci])
    
bootstrap_mean_estimate, boot_means = bootstrap_statistic(math, 2000, np.mean)
LCI, UCI = bootstrap_cis(boot_means)

plot_hist(boot_means, 'Bootstrap mean values', 'Histogram of bootstrap distribution')
plt.axvline(x=LCI, color='red', linestyle='dotted', linewidth=2)
plt.axvline(x=UCI, color='red', linestyle='dotted', linewidth=2)
plt.show()
```

## Two Sample Bootstrap    

How can we apply the bootstrap algorithm for two-sample statistics?    

- Example, difference of means of two independently sampled populations   

- How to generate bootstrap samples?    

- Can we just sample the concatenation of the two samples?     

- **No!**    
   - There is no guarantee of a correct number of resamples for each group   
   - Imbalanced sampling leads to bias       

- Must **independently sample the two groups** or populations     

- Compute the statistic from the two samples  


## Two Sample Bootstrap     

Example: algorithm to compute difference of means:    

   1. Independently randomly **Bernoulli sample** n data **with replacement** from each original data sample; The number of resamples for each populations is the number of samples for that population     
   2. Compute the two-sample statistic; e.g. difference of means    
   3. Repeat steps 1 and 2 to accumulate the required number of bootstrap samples
Accumulated bootstrap values comprise the bootstrap distribution; an estimate of the sample distribution of the statistic    
   4. The mean of the computed statistic values is the bootstrap point estimate of the statistic; e.g. difference of means      
   5. Compute CIs from bootstrap distribution

## Two Sample Bootstrap   

Example; find the bootstrap distribution of the difference in math scores between low and middle SES students.  

```{python, echo=FALSE}
g=sns.displot(data=test_scores.loc[test_scores.loc[:,'ses']!=3,:], 
        x='math', bins=20,
        row = 'ses', kde=True,
        height=3, aspect=1.5
        )
g.axes[0][0].set_title('Math scores for low SES students', fontsize=20);
g.axes[0][0].axvline(np.mean(test_scores.loc[test_scores.loc[:,'ses']==1,'math']), color='red', linestyle='dashed')
g.axes[1][0].set_title('Math scores for mid SES students', fontsize=20);
g.axes[1][0].axvline(np.mean(test_scores.loc[test_scores.loc[:,'ses']==2,'math']), color='red', linestyle='dashed')
```


## Example, Two Sample Bootstrap   

```{python, eval=FALSE}
def two_boot_two_stat(sample_1, sample_2, b, statistic_1, two_samp_statistic):
    '''
    Function computes two sample and two statistic bootstrap estimate.   
    - sample_1 and sample_2, independent obervation vectors   
    - b, number of bootstrap samples to compute
    - statistic 1, statistic applied to the two independent bootstrap samples
    - two_sample_statistic, statistic applied to the independent bootstrap statistics
    '''
    two_boot_values = []
    n_samps_1 = len(sample_1)
    n_samps_2 = len(sample_2)
    for _ in range(b):  
        ## Heavy lisfting is done here. First, the two independent bootstrap estimates
        ## are computed from independent bootstrap resamples. Second, the statistic 
        ## of the two bootstrap estimates is computed.  
        boot_estimate_1 = statistic_1(nr.choice(sample_1, size=n_samps_1, replace=True))
        boot_estimate_2 = statistic_1(nr.choice(sample_2, size=n_samps_2, replace=True))
        two_boot_values.append(two_samp_statistic(boot_estimate_1, boot_estimate_2))
    boot_estimate = np.mean(two_boot_values)
    print('bootstrap point estimate = {:6.2f}'.format(boot_estimate))
    return(boot_estimate, two_boot_values)    

math_low_ses = test_scores.loc[test_scores.loc[:,'ses']==1,'math'] 
math_mid_ses = test_scores.loc[test_scores.loc[:,'ses']==2,'math']
bootstrap_diff_of_mean, boot_diffs = two_boot_two_stat(math_low_ses, math_mid_ses, 2000, np.mean, lambda x,y: x-y)
```

## Example, Two Sample Bootstrap   

Compute and display the bootstrap distribution of the difference of student scores    

```{python, echo=FALSE}
def two_boot_two_stat(sample_1, sample_2, b, statistic_1, two_samp_statistic):
    '''
    Function computes two sample and two statistic bootstrap estimate.   
    - sample_1 and sample_2, independent observation vectors   
    - b, number of bootstrap samples to compute
    - statistic 1, statistic applied to the two independent bootstrap samples
    - two_sample_statistic, statistic applied to the independent bootstrap statistics
    '''
    two_boot_values = []
    n_samps_1 = len(sample_1)
    n_samps_2 = len(sample_2)
    for _ in range(b):  
        ## Heavy lifting is done here. First, the two independent bootstrap estimates
        ## are computed from independent bootstrap resamples. Second, the statistic 
        ## of the two bootstrap estimates is computed.  
        boot_estimate_1 = statistic_1(nr.choice(sample_1, size=n_samps_1, replace=True))
        boot_estimate_2 = statistic_1(nr.choice(sample_2, size=n_samps_2, replace=True))
        two_boot_values.append(two_samp_statistic(boot_estimate_1, boot_estimate_2))
    boot_estimate = np.mean(two_boot_values)
    print('bootstrap point estimate = {:6.2f}'.format(boot_estimate))
    return(boot_estimate, two_boot_values)    

math_low_ses = test_scores.loc[test_scores.loc[:,'ses']==1,'math'] 
math_mid_ses = test_scores.loc[test_scores.loc[:,'ses']==2,'math']
bootstrap_diff_of_mean, boot_diffs = two_boot_two_stat(math_low_ses, math_mid_ses, 2000, np.mean, lambda x,y: x-y)
LCI, UCI = bootstrap_cis(boot_diffs)

plot_hist(boot_diffs, 'Bootstrap difference of means', 'Histogram of bootstrap distribution of difference of means')
plt.axvline(x=LCI, color='red', linestyle='dotted', linewidth=2)
plt.axvline(x=UCI, color='red', linestyle='dotted', linewidth=2)
plt.show()  
```



## Summary    

Bootstrap estimation is widely useful and requires minimal assumption    

- Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)      

- Computing bootstrap distribution requires **no assumptions about population distribution!**
  - Bootstrap resampling substitutes computer power for paper and pencil statistician power     

- Bootstrap resampling estimates the **bootstrap distribution** of a statistic      
  - Compute mostly likely point estimate of the statistic, or bootstrap estimate     
  - The bootstrap confidence interval is computed from the bootstrap distribution     
  

## Summary   

There are several variations of the basi nonparametric bootstrap algorithm      

- One sample bootstrap    
   - Inference on single populations   

- Two sample bootstrap    
   - Inference on different populations   

- Special cases   
   - Correlation coefficients - part of your assignment     


## Summary    

Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! 

- If a sample is biased, the resampled statistic estimate based on that sample will be biased    
   - Results can be no better than the sample you start with  
   - Example; the bootstrap estimate of mean is the unbiased sample estimate, $\bar{x}$, not the population parameter, $\mu$ 
   
- The sample variance and Cis can be no better than the sample distribution allows    
   - Be suspicious of overly optimistic confidence intervals    
   - CIs can be optimistically biased        

- Are computationally intensive, but often highly parallelizable     

