
---
title: "Parameter Estimation and Likelihood"
author: "Steve Elston"
date: "09/29/2022"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Review   

Sampling is a fundamental process in the collection and analysis of data    

- Sampling is important because we almost never have data on a whole population   

- Sampling must be randomized to preclude biases    

- As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers

- Key points to keep in mind:   
  - Understanding sampling is essential to ensure data is representative of the entire population   
  - Use inferences on the sample to say something about the population   
  - The sample must be randomly drawn from the population  

- Sampling from distribution is the building block of simulation    


## Review   

Sampling of a population is done from an unknown **population distribution**, $\mathcal{F}$      

- Any statistic we compute for the generating process is an approximation for the population, $s(\hat{\mathcal{F}})$  

```{r Sampling, out.width = '60%', fig.cap='Sampling distribution of unknown population parameter', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/SamplingDistribuion.png"))
```
 
## Review   


The **law of large numbers** is a theorem that states that **statistics of independent random samples converge to the population values as more samples are used**      

- The law of large numbers is foundational to statistics and sampling theory    

- Assume that **larger samples are more representatives of the population we are sampling**     

- If the real world did not follow this theorem much of statistics, to say nothing of science and technology, would fail badly. 


## Introduction   

The concept of **likelihood** and **maximum likelihood estimation (MLE)** have been at the core of much of statistical modeling for about 100 years   

- In 21st Century, likelihood and MLE ideas continue to be foundational      

-  Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods     
  - Likelihood is a measure of how like a parametric model is to generate the observed data     
  - MLE is a generic methods for parameter estimation    

- MLE used widely for machine learning models, including some deep learning models      


## Likelihood and Density Functions  

Likelihood is a measure of how like a **parametric model** is to generate the observed data   

- Start with a data sample, $\mathbf{X} = [x_1, x_2, \ldots, x_n]$ 

- Likelihood of sample from a **generating process** with a **parametric density function**, $f(\mathbf{X} | \theta)$       

  - $f(\mathbf{X}\ |\ \mathbf{\theta})$ can be either a probability density function (PDF), for continuous distributions, or a probability mass function (PMF), for discrete distributions   

  - The distribution parameter vector, $\mathbf{\theta}$, is fixed        

- Now, for each observation, $x_i$, in $\mathbf{X} = x_1, x_2, \ldots, x_n$, the density is just $f(x_i |\ \mathbf{\theta})$    


## Likelihood   

Likelihood is a measure of how like a **parametric model** is to generate the observed data     

- Likelihood of sample from a **generating process** with a **parametric probability density**, $f(\mathbf{X} | \theta)$   

- For the set of observations, $\mathbf{X} = [x_1, x_2, \ldots, x_n]$, the likelihood is the product of the densities:   

$$\mathcal{L}(\mathbf{X} |\ \mathbf{\theta}) = \prod_{i=1}^n f(x_i | \mathbf{\theta})$$

- In most practical cases, we work with the **log likelihood**, for observations, $\mathbf{X} = x_1, x_2, \ldots, x_n$, the log likelihood is expressed:    

$$l(\mathbf{X} |\ \mathbf{\theta}) = log\big( \mathcal{L}(\mathbf{X} |\ \mathbf{\theta}) \big) = \sum_{i=1}^n log \Big( f(x_i\ |\ \mathbf{\theta}) \Big)$$   


## Likelihood   

Likelihood is a measure of how likely a **parametric model** is to generate the observed data sample     

- In most practical cases, we work with the **log likelihood**, for observations, $\mathbf{X} = x_1, x_2, \ldots, x_n$, the log likelihood is expressed:    

$$l(\mathbf{X} |\ \mathbf{\theta}) = log\big( \mathcal{L}(\mathbf{X} |\ \mathbf{\theta}) \big) = \sum_{i=1}^n log \Big( f(x_i\ |\ \mathbf{\theta}) \Big)$$   

- Use log-likelihood to work with the sum of log probabilities rather than the product    

  - If the probabilities are small, the sum is numerically stable     
  - The product of many small numbers is a very small number, which can lead to numerical underflow even for 64 or 128 bit floating point arithmetic      


## Likelihood Example    

Likelihood is a measure of how likely a **parametric model** is to generate the observed data sample      

- Binomial likelihood for sample sizes $[25, 50, 100]$ with $p=0.5$    

- Notice the variability in the likelihood curve for smaller samples   

- Likelihood has **stronger curvature for larger samples** - less uncertainty for maximum    


```{r BinomialLikelihood, out.width = '60%', fig.cap='Binomial likelihood at different sample sizes', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BinomialLikelihood.JPG"))
```


## Example: The Normal likelihood

The univariate Normal probability density function with parameter vector $\mathbf{\theta} = (\mu,\sigma)$ for a single observation, $x_i$:     

$$f(x\ |\ \mu,\sigma^2) = -\frac{1}{(2 \pi \sigma^2)^{1/2}} exp\Big[ - \frac{1}{2 \sigma^2}  (x_i - \mu)^2 \Big]$$
For n observations, $\mathbf{X} = x_1, x_2, \ldots, x_n$, the likelihood is the **product of the densities**:  

$$f(x\ |\ \mu,\sigma^2) = -\frac{1}{(2 \pi \sigma^2)^{1/2}} \prod_{i=1}^n  exp\Big[ - \frac{1}{2 \sigma^2}  (x_i - \mu)^2 \Big]$$


The log-likelihood (log of above) is a lot easiter to deal with:  

$$l(\mathbf{X}\ |\ \mu, \sigma) = - \frac{n}{2} log( 2 \pi \sigma^2 ) - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i - \mu)^2$$
- The log-likelihood is a function of the parameters, $(\mu,\sigma)$     


## Example: The Normal likelihood

An example to illustrate the foregoing concepts    

- Plot the likelihood for 5, 10 and 20 samples from a standard Normal distribution   

- Vary the parameter $\mu$, and assume the parameter $\sigma$ is fixed and known. The steps are:     
  - A random sample is drawn from a standard Normal distribution    
  - For the random sample the log-likelihood is computed at each location parameter value    
  
- Notice that as the number of observations increases so does the curvature of the likelihood.


```{r NormalLikelihood, out.width = '60%', fig.cap='Normal likelihood at different sample sizes', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/NormalLikelihood.JPG"))
```


## Example: Binomial Likelihood   

Example of log-likelihood for the Binomial distribution   

- Binomial distribution models **discrete events**      

- Range of the single parameter, $\pi$, restricted to the range $0 \le \pi \le 1$      

- Binomial distribution has the following probability mass function (PMF) for $k$ successes in $n$ trials:        

$$
f(k, n\ |\ \pi) = \binom{n}{y} \pi^k (1 - \pi)^{n-k}
$$

- Log-likelihood is easily found:    

$$l(k, n\ |\ \pi) = log \binom{n}{k} + k\ log(\pi) + (n-k)\ log(1-\pi)$$

- Binomial log-likelihood has a strong dependence on both the sample size, $n$ and the number of successes, $k$         


## The Maximum Likehihood Estimator    

**Maximum likelihood estimator (MLE)** is a foundational tool for much of statistical inference and machine learning      

- Given a log-likelihood function, find the model parameters which maximize it  

- Further, knowing the distribution allows us to quantify the uncertainty of the MLE parameter estimates   

- The model parameter estimates found by MLE is Normal for large samples, a remarkable property 

- The MLE is a **point estimator**    

- An estimate of a single parameter value, or point value, with the highest likelihood  


## The Maximum Likehihood Estimator  

The maximum likelihood for the model parameters is achieved when two conditions are met:  

$$
\frac{\partial\ l(\mathbf{X}\ |\ \mathbf{\theta)})}{\partial \theta} = 0 \\
\frac{\partial^2\ l(\mathbf{X}\ |\ \mathbf{\theta)})}{\partial \theta^2} < 0
$$


Interpret these two conditions:     

- First derivative of log-likelihood function, or slope, is 0 at either maximum or minimum points    
  - In general, $\vec{\theta}$ is a vector of model parameters   
  - Partial derivatives of log-likelihood are a vector - the gradient with respect to the model parameters   
  - Gradient of the log-likelihood are known as the **score function**   
  
- The second derivatives of the log-likelihood indicates the curvature      
  - Maximum has negative curvature   
  - Minimum has positive curvature


## Fisher information and properties of MLE     

The maximum likelihood estimator has useful and desirable properties  

- Start with a matrix of second partial derivatives of the log-likelihood function   

- Matrix is the **observed information matrix** of the model, $\mathcal{J}(\vec{\theta})$.   


$$
\mathcal{J}(\vec{\theta}) = 
\begin{pmatrix}
  \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_1^2} & 
  \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_2\ \partial \theta_1} & 
  \cdots & 
  \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n\ \partial \theta_1}\\
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_1\ \partial \theta_2} &
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_2^2} & 
   \cdots &
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_2\ \partial \theta_n}\\
   \vdots & \vdots & \vdots & \vdots \\
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n\ \partial \theta_1} &
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n\ \partial \theta_2} & 
   \cdots &
   \frac{\partial^2 l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n^2} 
 \end{pmatrix}
$$

Useful properties of the information matrix   

- The more negative the values of the second partial derivatives, the greater the curvature of the log-likelihood    
   - log-likelihood likelihood function with more negative values has limbs    
   - Narrow peak implies that the information on parameter values is high    
   - The matrix is symmetric, or information is symmetric around the maximum likelihood point


## Fisher information and properties of MLE   

Can one consider the information of the MLE before sampling data or performing an experiment?

- Can use the **expected information** or **Fisher information**    

- Fisher information is the expectation over the second derivative of the observed information            
$$\mathcal{J}(\vec{\theta}) = -\mathbf{E}\big\{ \mathcal{I}(\vec{\theta}) \big\} = -\mathbf{E}\big\{ \frac{\partial^2\ l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta^2} \big\}$$


## Fisher information and properties of MLE   

Fisher information leads to an important relationship    

- The MLE parameter estimate $\hat{\theta}$ is a Normally distributed random variable

- Arises from the Taylor expansion of the maximum likelihood estimator   

$$
0 = \frac{\partial\ l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta} = 
\frac{\partial\ l(\theta)_\mathbf{X}}{\partial \theta} + 
\frac{\partial^2\ l(\hat{\theta})_\mathbf{X}}{\partial \theta^2} (\hat{\theta} - \theta)
$$
or     

$$
0 = l'(\theta)_\mathbf{X} + l''(\hat{\theta})_\mathbf{X} (\hat{\theta} - \theta)
$$

- $l'(\theta)_\mathbf{X}$: first partial derivative given observations $\mathbf{X}$  

- $l''(\hat{\theta})_\mathbf{X}$: second partial derivative given observations $\mathbf{X}$ 


## Fisher information and properties of MLE   

Continuing with the simplified notation, and solving for $\hat{\theta}$;    

$$
\hat{\theta} = \theta + \frac{l'(\theta)_\mathbf{X}/n}{-l''(\hat{\theta})_\mathbf{X}/n}
$$

Fisher information relates to the **score function**  as the inverse variance   

$$
\frac{\partial\ l(\theta)_{\mathbf{X}}}{ \partial \theta}\ \dot{\sim}\
\mathcal{N} \big(0, 1/ \mathcal{I}_\theta \big)
$$


For **large sample**, $n \rightarrow \infty$, take the expectation over all samples, $\mathbf{X}$. Assuming first and second derivatives exist and continuous, then by the Central Limit Theorem:    

$$\hat{\theta} \dot{\sim} \mathcal{N}\Big(\theta, \frac{1}{n\mathcal{I}(\theta)} \Big)$$


## Fisher information and properties of MLE   

$$
\frac{\partial\ l(\theta)}{ \partial \theta}\ \dot{\sim}\
\mathcal{N} \big(0, 1/ \mathcal{I}_\theta \big)
$$

Relationship shows several important properties     

- The maximum likelihood estimate of model parameters, $\hat{\theta}$, is Normally distributed      
- The larger the Fisher information, the lower the variance of the parameter estimate    
  - Greater curvature of the log likelihood function gives more certain the parameter estimates       
  - The variance of the parameter estimate is inversely proportional to the number of samples, $n$.   



## Example of MLE for Normal distribution   

MLE for the Normal distribution   

- Find derivatives of the log-likelihood function with respect to the model parameters, $\mu$ and $\sigma2$    

$$
\begin{pmatrix}
\frac{\partial l}{\partial \mu} \\
\frac{\partial l}{\partial \sigma^2}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{\sigma^2} \sum_j (x_j - \mu) \\
-\frac{n}{2 \sigma^2} + 
\frac{1}{2 \sigma^4} \sum_j (x_j - \mu)^2
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
$$

Solving above for the estimate of the mean, $\bar{x}$  


$$
\sum_{j=1}n (x_j - \mu) = 0 
$$
$$
\bar{x} \rightarrow \mu:\ at\ convergence\\
\bar{x} = \frac{1}{n} \sum_{j=1} x_j
$$


## Example of MLE for Normal distribution   

MLE for the Normal distribution   

$$
\begin{pmatrix}
\frac{\partial l}{\partial \mu} \\
\frac{\partial l}{\partial \sigma^2}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{\sigma^2} \sum_j (x_j - \mu) \\
-\frac{n}{2 \sigma^2} + 
\frac{1}{2 \sigma^4} \sum_j (x_j - \mu)^2
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
$$

Find the maximum likelihood estimate of $\sigma^2$    

$$
\frac{1}{s^2} \sum_{j=1} (x_j - \mu)^2 = n
$$
$$
\bar{x} \rightarrow \mu:\ at\ convergence\\
s^2 = \frac{1}{n} \sum_{j=1}^n (x_j - \bar{x})^2 
$$


## Example of MLE for Normal distribution   


\begin{align}
\mathcal{J}(\vec{\theta}) &= 
\begin{pmatrix}
\frac{\partial^2 l}{\partial \mu^2} & \frac{\partial^2 l}{\partial \mu\ \partial \sigma^2}\\
\frac{\partial^2 l}{\partial \mu\ \partial \sigma^2} & \frac{\partial^2 l}{\partial (\sigma^2)^2}
\end{pmatrix} \\
&= 
\begin{pmatrix}
-\frac{n}{\sigma^2} & -\frac{n}{\sigma^4}(\bar{x} - \mu) \\
-\frac{n}{\sigma^4}(\bar{x} - \mu) & -\frac{n}{2\sigma^4} + \frac{1}{\sigma^6} \sum_j (x_j - \mu)^2
\end{pmatrix} \\
&= 
\begin{pmatrix}
-\frac{n}{\sigma^2} & 0 \\
0 & -\frac{n}{2\sigma^4} 
\end{pmatrix}
\end{align}

The simplification results from the fact that $\bar{x} \rightarrow  \mathbf{E}(\mathbf{x_j})  = \mu$ and $s^2 \rightarrow \mathbf{E} \big\{ (x_j - \mu)^2 \big\} = \sigma^2$ in the limit of a large sample from the law of large numbers.   

- There are some aspects of these relationships which make the MLE method attractive: 
  - The curvature of the MLE for both parameters increases with the number of samples $n$   
  - The peak of the log-likelihood function becomes better defined as $n$ increases.  
  - The maximum likelihood estimates of the parameters, $\mu$ and $\sigma^2$ are independent. The off-diagonal terms are $0$.      


## Fisher information and properties of MLE   

Example of Fisher Information for Normal distribution    

- As sample size increases, Fisher Information decreases   

- Variance of MLE decreases with decreasing Fisher Information   


```{r NormalFisher, out.width = '60%', fig.cap='Fisher Information for Normal distribution at different sample sizes', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/NormalFisherInformation.JPG"))
```



## Finding Solutions Without a Closed Form

How do we generalize the MLE method beyond cases with a closed form solution    

- For example, logistic regression has a nonlinear log likelihood function     

- An approximate solution can be found by numerical **optimization methods** or **root finding methods**     

- Two widely used methods   
  - Gradient descent, a first-order method     
  - Newton's method, quadratic approximation    
  
  
## Gradient descent methods

The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill'    

- Start with the gradient of the log-likelihood function with respect to the parameters, $\theta$    

- The gradient is the vector of partial derivatives with respect to each of the parameters      

$$
grad(l(\vec{\theta})) =  \nabla_\theta\ l(\vec{\theta}) = \begin{pmatrix}
  \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_1} \\
   \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_2}\\
   \vdots \\
   \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n} 
 \end{pmatrix}
 $$
 
 - Given a current parameter estimate vector at step n , $\hat{\theta}_n$, the improved parameter estimate vector,  $\hat{\theta}_{n+1}$, is found:    

$$\hat{\theta}_{n+1} = \hat{\theta}_n + \gamma\ \nabla_\theta\ l(\hat{\theta})$$  


## Gradient descent methods    

The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill'    

- Example: contour plot of Normal log-likelihood    

- Gradient descent following negative (minus) gradient to maximum of log-likelihood        

- Maximum gradient direction is perpendicular to coutour lines - steepest uphill path     

- Notice difference between gradients (and information) between locaiton and scale!  



```{r LikelihoodCountours, out.width = '60%', fig.cap='Contours of Normal Log-Likelihood', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/NormalLikelihoodCountours.JPG"))
```




## Gradient descent methods

The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill'  

 - Given a current parameter estimate vector at step n , $\hat{\theta}_n$, the improved parameter estimate vector,  $\hat{\theta}_{n+1}$, is found:    

$$\hat{\theta}_{n+1} = \hat{\theta}_n + \gamma\ \nabla_\theta\ l(\hat{\theta})$$   

- The hyperparameter $\gamma$ is the **learning rate** or **step size**     
   - Determining a learning rate can have a significant effect on the performance of the gradient     
   - This hyperparameter can be chosen manually, often with by a search of the hyperparameter space  

- Using a fixed $\gamma$ is far from optimal      
  - The gradient changes toward the maximum point the optimal step size changes    
  - More sophisticated algorithms use an adaptive method to determine an optimal step at each step      
  - Adaptive algorithm finds step size dynamically using a **line search** procedure

- Algorithm converges when the norm of the gradient is approximately 0   
  - This is the **stopping condition**  
  - Express this condition as     
  
$$
|| \nabla_\theta\ l(\vec{\theta}) ||\le tollerance
$$

## Gradient descent methods   

**Gradient with respect to model parameters** is computed for each dimension of the model parameter vector         

```{eval=FALSE}
proceedure compute_expected_grad(parameters, cases):   
  for each dimension:
      grad[dimension] = grad(parameters, cases)
  return grad
```  

The forgoing procedure can be vectorized or parallelized   





## Gradient descent methods

The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill'  

- The gradient is always perpendicular to the contours 

```{r GradientDescent, out.width = '60%', fig.cap='Gradient descent on countour plot of log-likelihood', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/GradientDescent.png"))
```
 
## Stochastic gadient descent (SGD)

The simple gradient descent algorithm and Newton's method have limited scalability   

- Basic algorithms require computing and summing the entire gradient vector     
   - Calculation must be done as a single batch in memory, or **batch gradient descent**         
   - Computing the full gradient at each step limits scalability   

- **Stochastic gradient decent (SGD)** algorithm computes the approximate expected gradient using a **mini-batch**   
   - The mini-batch is a limited-size Bernoulli sample from the full set of cases    
   - These gradient approximations are inherently noisy or stochastic, giving rise to the method's name     

- Using mini-batches greatly increases scalability     
   - While gradient estimates are less accurate, these estimates can be computed very quickly   
   - SGD is highly scalable and the workhorse of many large-scale statistical methods  
   - Mini-batch optimization is often referred to as **online optimization** since updates as cases arrive. 


## Stochastic gadient descent (SGD)

The basic idea of stochastic optimization is using a Bernoulli random sample of the data to estimate the **expected update** of the model weights     

$$W_{t+1} = W_t + \alpha\ E_{\hat{p}data}\Big[ \nabla_{W} J(W_t) \Big]$$ 

where, $E_{\hat{p}data} \big[ \big]$ is the expected value of the gradient given the Bernoulli sample of the data $\hat{p}data$.

- Choosing batch size can require some tuning    
   - If the batch is too small, the gradient estimate will be and, hardware resources may not be fully utilized     
   - Large batches require significant memory and slow the calculation    

- Empirically, SGD has good convergence properties     
   - This behavior arises since stochastic gradient samples provide a better exploration of the loss function space    
   - For very large data sets, the SGD algorithm often converges before the first pass through the data is completed 

## Stochastic gadient descent (SGD)     

The pseudo code for the SGD algorithm is:       

```{eval=FALSE)`
define learning_rate
proceedure update_weights(weights, grad):  
   weights = weights + learning_rate * grad
   return weights

Random_sort(cases)          
while(grad > stopping_criteria):      
    mini-batch = sample_next_n(cases)     
    grad = compute_expected_grad(weights, mini_batch)      
    weights = update_weights(weights, grad)`   
```    
    
- If the sampling continues for more than one cycle through the cases, the samples are biased    
- In practice, this small bias does not seem to mater much      


## Newton's method

**Newton's method**, and related methods, employ a **quadratic approximation** to optimization. For MLE, Newton's method uses both the first and second derivatives of the log-likelihood function. 

Consider a nonlinear log-likelihood function, $l(\theta\ | \mathbf{X})$. We use a Taylor expansion to find the tangent point of the log-likelihood, $\theta_n = \theta_k + \delta \theta$. The Taylor expansion is:     

$$
l(\theta_k + \delta\theta) = l(\theta_k) + l'(\theta_k)\delta\theta + \frac{1}{2} l''(\theta_k)\delta\theta
$$

Setting this expansion to 0, we have:

\begin{align}
0 &= \frac{d}{d\ \delta \theta} \Big( l(\theta_k) + l'(\theta_k)\delta\theta + \frac{1}{2} l''(\theta_k)\delta\theta^2 \Big)\\
0 &= l'(\theta_k) + l''(\theta_k) \delta\theta 
\end{align}

It is simple to solve for $\delta \theta$: 

$$
\delta\theta = \frac{l'(\theta_k)}{l''(\theta_k)}
$$

## Newton's method  

At each step in the iteration, the update $x_{n+1}$ is given by the following relationship:

\begin{align}
\theta_{n+1} &= \theta_n + \gamma \frac{l'(\theta_k)}{l''(\theta_k)}\\ 
&= \theta_n + \gamma\ \delta\theta
\end{align}

- $\gamma$ is a learning rate of step size. 

- Newton's method has a quadratic form     
   - The quadratic form is not just a mathematical curiosity   
   - Newton's method exhibits convergence quadratic in the number of iterations     
   - Compared to the approximate linear convergence for gradient descent methods       

## Newton's method   

Newton's method in higher dimensions  

- Use the gradient, $\nabla l(\vec{\theta})$ for the first derivatives of the likelihood   

- Second derivative is represented as a matrix, $\nabla^2 l(\vec{\theta})$, known as the **Hessian**       

- The quadratic update is:  

$$
x_{n+1} = x_n + \gamma |\nabla_\theta^2 l(\vec{\theta})|^{-1} \nabla_\theta l(\vec{\theta})
$$

- Requires computing the **inverse Hessian** matrix with practical difficulties   
  - The inverse of the Hessian may not exist as this matrix may be singular    
  - With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive      
  - Computing the full gradient and Hessian requires summing over all observations   

- For large scale problems **quasi-Newton** methods are used      
   - Use an approximation to avoid computing the full inverse Hessian   
   - **Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)** algorithm the most widely used quasi-Newton method 


## Limitations of MLE   

The maximum likelihood estimator has a number of important limitations, including   

- Incorrect model for complex distributions     

- Parameter near limits      

- High dimensional problems     

- Correlated features     


## Incorrect model and complex distributions   

In many real-world problems the distribution are not simple    

- Problematic for maximum likelihood methods   

- Consider a likelihood function that is only approximately correct    
   - Population being modeled has a different distribution   
   - Outliers in the form of erroneous samples       

- Example; maximum likelihood estimator for population with a mixture of simple distributions    
   - Mixture has multiple modes     
   - One mode is the maximum    
   
- Mode found by any gradient-based algorithm is dependent on the starting point    
   - MLE algorithm will find the nearest **local maximum**     
   - No guarantee of finding the actual maximum likelihood point      

## Incorrect model and complex distributions  

Example; univariate Normal maximum likelihood estimator for mixture of 3 Normal distributions    

```{python, echo=FALSE}
import numpy as np    
import numpy.random as nr
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm, binom
import seaborn as sns

samples = np.concatenate((nr.normal(loc=0.0, scale=1.0, size=10000), 
                          nr.normal(loc=6.0, scale=2.0, size=10000), 
                          nr.normal(loc=-4.0, scale=1.5, size=5000)))
samples = pd.Series(samples) #, name='Multi_Modal_Samples') 

plt.rc('font', size=5)
fig, ax = plt.subplots(2, 1, figsize=(6, 8))      
#_=plt.hist(samples, bins=100)
sns.kdeplot(data=samples, ax=ax[0], linewidth=1)
ax[0].set_xlabel('X')
ax[0].set_ylabel('Probability Density')
ax[0].set_title('Multi-Modal Probability Density')

scale = np.var(samples)
logpdf = lambda x, y: norm.logpdf(x, loc=y, scale=scale)
mu_values = np.arange(-15.0, 15.0, 0.1)
log_likelihood = []
for mu in mu_values:
    log_likelihood.append(logpdf(samples,mu).sum())
max_LL = mu_values[np.argmax(log_likelihood)]    
ax[1].plot(mu_values,log_likelihood, linewidth=1)
ax[1].axvline(max_LL, linewidth=1, linestyle='--', color='black')
ax[1].set_title('Log-likelihood vs. location value')
ax[1].set_xlabel('X')
ax[1].set_ylabel('Log-likelihood')
```


## Parameter near limits

For many distributions, parameter values have limits    

- The log-likelihood function may have an extremely high gradient near the limit    
   - Results can be poorly determined parameter estimates and slow convergence
   - Fisher information drops rapidly near these limits, indicating poorly determined gradient   
   
- Examples:     
   - Variance estiamtes near 0; variables with low variance  
   - Binomial parameter estimates near $p=0$ and $p=1$; case with either very few successes or failure    

## Parameter near limits

Example; Binomial likelihood and Fisher information with $p=0.5$     
Extreme gradients and low information near limits, $p=0$ and $p=1$

```{python, echo=FALSE}
from scipy.stats import norm, binom

def plot_likelihood_1(sample_dist, df, num_samples, start, stop, p, linestyle):
    '''
    Function to plot log-likelihood for distributions with a single parameter density functions
    - sample_dist is a function to sample from the known distribution
    - df is the log of the density function, pdf or pmf
    - num_samples are the number of samples drawn from the known distribution 
    - start, stop are the range on the x asis to be plotted
    - p is the known value of the parameter 
    - linestyle are the line style used to display the likelihood functions 
    '''
    ## Setup for plot
    plt.rc('font', size=10)
    fig, ax = plt.subplots(1,2, figsize=(12,4), ) 
    X = np.arange(start, stop, step=0.001)

    ## Loop over number of samples
    for i,samps in enumerate(num_samples): 
        ## Compute a sample from standard Normal
        sample = sample_dist(samps)
        ## Loop over the x values and compute the likelihood
        y=[]
        for mu in X:
            y.append(df(sample, mu).sum())
        ## Plot the likelihood    
        _=ax[0].plot(X, y, linewidth=1, label= str(samps) + ' samples', linestyle=linestyle[i])

    ## Add annotions to plot
    ax[0].vlines(p, ymin=min(y), ymax=0.0, linewidth=1, linestyles='dotted');
    ax[0].set_ylabel('Log-likelihood');
    ax[0].set_xlabel('x');
    ax[0].set_title('Log-likelihood for different numbers of samples');
    ax[0].legend();
    
    n=100
    pi = np.arange(0.01, 1.0, step=0.01) 
    binom_information = np.apply_along_axis(lambda x: -100/(n/(x*(1-x))), 0, pi)
    ## Now make the plot
    _=ax[1].plot(pi, binom_information, lw=3);
    ax[1].set_title('Information for MLE for Binomial distribution with n=100');
    ax[1].set_ylabel('Information');
    ax[1].set_xlabel('Pi');
    plt.show();
    
    plt.show()         

p=0.5
sample_dist = lambda x: nr.binomial(n=1, p=p, size=1000)
def pmf(x, p):
    ## FIrst compute 
    n = len(x)
    k = np.sum(x)
    ## Return the log likelihood
    return binom.logpmf(k, n, p)
num_samples = [1000]
#start = 0.05
#stop = 0.95
start = 0.0005
stop = 0.999999
linestyle = ['solid']
plot_likelihood_1(sample_dist, pmf, num_samples, start, stop, p, linestyle)
```



## High dimensional problems   

The MLE method often finds poor solutions to problems in high dimensions    

- High dimensions means a large numbers of parameters    
   - The likelihood function has corresponding high dimensionality; one dimension for each parameter 

- For high dimensional problems, it is often the case that the gradient and Hessian are not well determined    
   - Uncertainty in the variables can lead to considerable uncertainty in determining the gradient in high dimensions     
   - MLE algorithms may not converge or converge to results with a large uncertainty 

- Even with few parameters, MLE methods can have convergence problems    
   - Example; difficulties fitting the variance parameter while fitting the location parameter for a univariate Normal distribution  

- We will discuss this problem later in the course


## Correlated features  

Theory  assumes that the variables used for MLE are independent    

- In reality, never truly the case    

- Some variables have a high correlation to each other    

- MLE algorithm can breakdown since gradients will not be well determined

- We will discuss this problem later in the course


## Summary      


- Likelihood is a measure of how well a **parametric model** fits a data sample

$$\mathcal{L}(\mathbf{X} |\ \mathbf{\theta}) = \prod_{i=1}^n f(x_i | \mathbf{\theta})$$

- In most practical cases, we work with the **log likelibood**      

$$l(\vec{\theta}\ |\ \mathbf{X}) = log\big( \mathcal{L}(\vec{\theta}\ |\ \mathbf{X}) \big) = \sum_{j} log \Big( f(x_j\ |\ \vec{\theta}) \Big)$$


## Summary   


- Matrix is the **observed information matrix** of the model, $\mathcal{J}(\vec{\theta})$.   

- The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood  

- Fisher information or **expected information** is the expectation over the second derivative of the observed information            

$$\mathcal{J}(\vec{\theta}) = -\mathbf{E}\big\{ \mathcal{I}(\vec{\theta}) \big\} = -\mathbf{E}\big\{ \frac{\partial^2\ l(\mathbf{X} | \theta)}{\partial \theta^2} \big\}$$

- Fisher information relates to the score function as its variance   

$$
\frac{\partial\ l(\theta)}{ \partial \theta}\ \dot{\sim}\
\mathcal{N} \big(0, 1/ \mathcal{I}_\theta \big)
$$


- For **large sample**, $n \rightarrow \infty$, take the expectation over $\mathbf{X}$:    

$$\hat{\theta} \dot{\sim} \mathcal{N}\Big(\theta, \frac{1}{n\mathcal{I}(\theta)} \Big)$$


- The maximum likelihood estimate of model parameters, $\hat{\theta}$, is Normally distributed      

- The larger the Fisher information, the lower the variance of the parameter estimate    
   - Greater curvature of the log likelihood function gives more certain the parameter estimates       
   - The variance of the parameter estimate is inversely proportional to the number of samples, $n$   



## Summary  

- The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill' 

- The maximum likelihood for the model parameters is achieved when two conditions are met:  

$$
\frac{\partial\ l(\mathbf{X}\ |\ \mathbf{\theta)})}{\partial \theta} = 0 \\
\frac{\partial^2\ l(\mathbf{X}\ |\ \mathbf{\theta)})}{\partial \theta^2} < 0
$$   

- Gradient of the log-likelihood is known as the score function   

$$
grad(l(\vec{\theta})) =  \nabla_\theta\ l(\vec{\theta}) = \begin{pmatrix}
  \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_1} \\
   \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_2}\\
   \vdots \\
   \frac{\partial l(\mathbf{X}\ |\ \mathbf{\theta})}{\partial \theta_n} 
 \end{pmatrix}
 $$

## Summary  

- The gradient descent method finds the maximum of the log-likelihood function by following the gradient 'uphill'    

 - Given a current parameter estimate vector at step n , $\hat{\theta}_n$, the improved parameter estimate vector,  $\hat{\theta}_{n+1}$, is found:    

$$\hat{\theta}_{n+1} = \hat{\theta}_n + \gamma\ \nabla_\theta\ l(\hat{\theta})$$   

- The hyperparameter $\gamma$ is the **learning rate** or **step size**     

- Stochastic optimization uses a Bernoulli random sample of the data to estimate the **expected update** of the model weights     

$$W_{t+1} = W_t + \alpha\ E_{\hat{p}data}\Big[ \nabla_{W} J(W_t) \Big]$$ 

- Where, $E_{\hat{p}data} \big[ \big]$ is the expected value of the gradient given the Bernoulli sample of the data, $\hat{p}data$.

- Empirically, SGD has good convergence properties     


## Summary    

- **Newton's method**, and related methods, employ a **quadratic approximation** to optimization.    

- Newton's method exhibits convergence quadratic in the number of iterations     
   - Compared to the approximate linear convergence for gradient descent methods       

- The quadratic update is:  

$$
x_{n+1} = x_n + \gamma |\nabla_\theta^2 l(\vec{\theta})|^{-1} \nabla_\theta l(\vec{\theta})
$$

- $\gamma$ is a learning rate of step size

- Requires computing the **inverse Hessian** matrix with practical difficulties   
  - The inverse of the Hessian may not exist as this matrix may be singular    
  - With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive      

- For large scale problems **quasi-Newton** methods are used      
   - Use an approximation to avoid computing the full inverse Hessian   
   - **Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)** algorithm the most widely used quasi-Newton method 



## Summary   

The maximum likelihood estimator has a number of important limitations, including   

- Incorrect model and complex distributions     

- Parameter near limits      

- High dimensional problems     

- Correlated features   

