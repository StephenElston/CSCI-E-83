---
title: "LinearModelsPart2"
author: "Steve Elston"
date: "10/27/2022"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

 
## Dealing with Overfit Models

Example: We might suspect that the terms, ses and prog are significant in predicting social science test scores:  

```{python}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf 
import scipy.stats as ss  
from math import atan

linear_model = smf.ols("socst ~ C(ses)*C(prog)", data=test_scores_train).fit()
linear_model.summary()
```


## Dealing with Overfit Models

Example: After 3 or 4 rounds of *guess and cut* feature pruning, we arrive at a model with only significant coefficients:    

```{python}
linear_model = smf.ols("socst ~ - 1 + C(ses):C(prog)", data=test_scores_train).fit()
linear_model.summary()
```


## Dealing with Overfit Models   

Let's compare the results of the unpruned and pruned models     

- The metrics indicate the fit is exactly the same   
   - Why prefer the pruned model?    
   
- What are the consequences of the over-fit model?    
   - Several predictors (features) are included that are not needed     
   - Including non-significant predictors can only reduce generalization of the model    
   - Consider the effect of an unexpected value of a non-significant predictor on the response recalling the response is linear in the coefficients    
   
- But manually pruning a model with a great many features is a doomed task!   
   - What if we included all the interactions with type of school, race and sex?   
   - We now have up to 5th order interaction - 45 model coefficients with none significant!!    



## Dealing with Overfit Models   

**Idea!:** Try systematically pruning the model using some metric    

- But hard to find a good metric    

- Further, making multiple hypothesis tests is a fraught undertaking    
   - High probability of Type 1 or Type 2 error     
   - The multiple hypothesis problem    
   
- Leads to the the **step-wise regression algorithm**     
   - Forward step-wise regression adds most explanatory variable one at a time    
   - Backward step-wise regression removes lest explanatory variable one at a time    
   - Can go both directions - see the R documentation     


## Regularization - a Better Approach      

Regularization is a systematic approach to preventing over-fitting     

- To understand regularization need to understand the bias-variance trade-off     

- To better understand this trade-off decompose mean square error:    

$$\Delta y = E \big[ Y - \hat{f}(X) \big]$$
$y =$ the label vector     
$X =$ the feature matrix      
$\hat{f}(x) =$ the trained model      

- Expanding this relation gives us:    

$$\Delta y = \big( E[ \hat{f}(X)] - \hat{f}(X) \big)^2 + E \big[ ( \hat{f}(X) - E[ \hat{f}(X)])^2 \big] + \sigma^2\\
\Delta y = Bias^2 + Variance + Irreducible\ Error$$


## Regularization - a Better Approach

There is a trade-off between bias and variance      

- Need to find the trade-off point  

```{r BiasVariance, out.width = '50%', fig.cap='The trade-off between bias and variance', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BiasVariance.png"))
```


## Regularization - a Better Approach

Decompose the $p$ x $p$ covariance matrix $cov(A) = \frac{1}{p}A^TA$ into **eigenvalues** and **eigenvectors**: 

$$cov(A) = Q \Lambda Q^{-1}$$

where,   

$Q$ is the $p$ x $p$ matrix of $p$ **orthonormal eigenvectors** of length $p$  

And, the $p$ eigenvalues are represented as diagonal matrix: 

$$
\Lambda = 
\begin{bmatrix}
\lambda_1, 0, \ldots, 0\\
0, \lambda_2, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \lambda_p
\end{bmatrix}
$$

For real-valued $cov(A)$ the eigenvectors $Q$ are real valued so:   

$$Q^{-1} = Q^T$$


## Regularization - a Better Approach

The inverse of the covariance can be computed from its eigendecomposition   

$$cov(A)^{-1} = Q^T \Lambda^{-1} Q$$

Where $Q^{-1}$ is written:       

$$
\Lambda^{-1} = 
\begin{bmatrix}
\frac{1}{\lambda_1}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2}, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p}
\end{bmatrix}
$$


## Regularization - a Better Approach    

At first look eigen-decomposition seems a bit mysterious

- The eigenvalues are the **roots** of the covariance matrix    
   - Similar to the familiar roots of a polynomial  

- For square matrix, $A$, and some norm 1 vector, $x$, we can find a root, $\lambda$: 

\begin{align}
Ax &=  \lambda x \\
Ax - \lambda x &= 0 \\
(A - I \lambda) x &= 0
\end{align}

- For $n$ x $n$ dimensional matrix there are $n$ eigenvalues and orthogonal eigenvalue    

- But, there is no guarantee that the $n$ values of $\lambda$ are unique     
   - If columns of $A$ are colinear, there are $< n$ unique eigenvectors        

## Regularization - a Better Approach

If columns of $A$ are not linearly independent, the covariance matrix is **ill-posed** and the eigenvectors are under-determined       

$$
cov(A) = Q \Lambda Q^T\\
cov(A)^{-1} = Q^T \Lambda^{-1} Q
$$

- The eigenvalues are ordered, $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$

- For ill-posed covariance matrix    
   - The smallest $\lambda_i \rightarrow 0$   
   - So, $\frac{1}{\lambda_i} \rightarrow \infty$
   - The inverse covariance matrix does not exist!    



## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- Recall:   

$$x = A b + \epsilon$$

- The **normal equations provide a solution**:

$$b = (A^TA)^{-1}A^Tx$$

- This solution requires finding the inverse of the covariance matrix, $(A^TA)^{-1}$       
   - But this inverse may be unstable   
   - In mathematical terminology we say the problem is **ill-posed**     
   
   
## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- We can add a **bias term** to the diagonal of the covariance matrix     

the **L2 or Euclidean norm** minimization problem):

$$min [\parallel A \cdot x - b \parallel +\ \alpha \parallel b\parallel]\\  or \\
b = (A^TA + \alpha^2 I)^{-1}A^Tx$$

Where the L2 norm of the coefficient vector is:
$$||b|| = \big( \beta_1^2 + \beta_2^2 + \ldots + \beta_m^2 \big)^{\frac{1}{2}} = \Big( \sum_{i=1}^m \beta_i^2 \Big)^{\frac{1}{2}}$$


## L2 Regularization    

How can we understand this relationship?     

$$b = (A^TA + \alpha^2I)^{-1}A^Tx$$

- Adds values along the diagonal of the covariance matrix     
   - This creates a so called ridge in the covariance, $cov_{regularized} = A^TA + \alpha^2$     
   - Leads to the term **ridge regression**   

- Constrain the L2 norm values of the model coefficients using the **penalty term** $\alpha$     
   - Larger $\alpha$ is more bias but lover variance   
   - Larger $\alpha$ makes the inverse of the covariance more stable   
   
- L2 regularization is a **soft constraint** on the model coefficients    
   - Even smallest coefficients are not driven to 0   
   - All coefficients can grow in value, but under the constraint   


## L2 Regularization   

Eigen-decomposition of the regularized covariance matrix:    

$$A^TA + \alpha^2I = Q 
\begin{bmatrix}
\lambda_1 + \alpha^2, 0, \ldots, 0\\
0, \lambda_2 + \alpha^2, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \lambda_p + \alpha^2
\end{bmatrix}
Q^T$$

The inverse covariance matrix is then: 

$$\Big( A^TA + \alpha^2I \Big)^{-1} = Q^T 
\begin{bmatrix}
\frac{1}{\lambda_1 + \alpha^2}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2 + \alpha^2}, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p + \alpha^2}
\end{bmatrix}
Q$$

With any $\alpha > 0$, the inverse eigenvalues of the inverse covariance matrix are bounded     

Increasing $\alpha$ increases bias, but increases the stability of the inverse   

## L2 Regularization     

**Example:** compute the eigenvalues of a covariance matrix     

```{python, echo=FALSE}
test_scores_train['socst_zero_mean'] = test_scores_train['socst'] - np.mean(test_scores_train['socst'])
test_scores_test['socst_zero_mean'] = test_scores_test['socst'] - np.mean(test_scores_test['socst'])
```

```{python}
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
Y, X = dmatrices("socst_zero_mean ~ C(ses, levels=[1,2,3])*C(prog, levels=[1,2,3])", data=test_scores)
cov_X = np.matmul(np.transpose(X),X)
cov_X = np.divide(cov_X, float(cov_X.shape[0]))
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 100$

**Add regularization** with $\alpha=0.1$ and compute the eigenvalues

```{python}
alpha_sqr = 0.1 
alpha_sqr = np.diag([alpha_sqr] * cov_X.shape[0])
#alpha_sqr = np.diag([alpha] * cov_X.shape[0])
#cov_X = np.divide(np.matmul(np.transpose(X),X), float(cov_X.shape[0]))
cov_X = np.add(cov_X, alpha_sqr)
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 10$


## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$    

- The norm of the coefficient vector, $\vec{b}$, is constrained


```{r L2, out.width = '60%', fig.cap='L2 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L2.jpg"))
```   

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python}
def regularized_coefs(df_train, df_test, alphas, L1_wt=0.0, n_coefs=8,
                      formula = 'socst_zero_mean ~ C(ses)*C(prog)', 
                      label='socst_zero_mean'):
    '''Function that computes a linear model for each value of the regularization 
    parameter alpha and returns an array of the coefficient values. The L1_wt 
    determines the trade-off between L1 and L2 regularization'''
    coefs = np.zeros((len(alphas),n_coefs + 1))
    MSE_train = []
    MSE_test = []
    for i,alpha in enumerate(alphas):
        ## First compute the training MSE
        temp_mod = smf.ols(formula, data=df_train).fit_regularized(alpha=alpha,L1_wt=L1_wt)
        coefs[i,:] = temp_mod.params
        MSE_train.append(np.mean(np.square(df_train[label] - temp_mod.predict(df_train))))
        ## Then compute the test MSE
        MSE_test.append(np.mean(np.square(df_test[label] - temp_mod.predict(df_test))))
        
    return coefs, MSE_train, MSE_test

alphas = np.arange(0.0, 0.3, step = 0.003)    
Betas, MSE_test, MSE_train = regularized_coefs(test_scores_train, test_scores_test, alphas)
#Betas[:5]
```

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python, echo=FALSE}
def plot_coefs(coefs, alphas, MSE_train, MSE_test, ylim=None):
    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis
    for i in range(coefs.shape[1]): # Iterate over coefficients
        ax[0].plot(alphas, coefs[:,i])
    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)
    ax[0].set_ylabel('Partial slope values')
    ax[0].set_xlabel('alpha')
    ax[0].set_title('Parial slope values vs. regularization parameter number')
    if ylim is not None: ax[0].set_ylim(ylim)
    
    ax[1].plot(alphas, MSE_train, label='Training error')
    ax[1].plot(alphas, MSE_test, label='Test error')
    ax[1].set_ylabel('Mean squared error')
    ax[1].set_xlabel('alpha')
    ax[1].set_title('MSE vs. regularization parameter number')
    plt.legend()
    plt.show()
    
    
plot_coefs(Betas, alphas, MSE_test, MSE_train)#, ylim=(-5.0,5.0))
```

## L1 Regularization    

The L1 norm provides regularization with different properties     

- Constrains the model parameters using the L1 norm: 

$$min [\parallel A \cdot x - b \parallel +\ \alpha \parallel b\parallel^1]$$

- This form looks a lot like the L2 regularization formulation   

- $\parallel b\parallel^1$ is the L1 norm       

- Compute the l1 norm of the $m$ model parameters:

$$||b||^1 = \big( |b_1| + |b_2| + \ldots + |b_m| \big) = \Big( \sum_{i=1}^m |b_i| \Big)^1$$

$|b_i|$ is the absolute value of $b_i$. 


## L1 Regularization 

What are the properties of the L1 regularization   

- L1 norm is a **hard constraint**     

- L1 regularization drives coefficients to zero   

- The hard constraint property leads to the term **lasso regularization**    

- Lasso regression is a method of **feature selection**   


## L1 Regularization   

The Lasso regularization is a strong constraint on coefficient values   

- Some coefficients are forced to zero    

- The constraint curve is like a *lasso*


```{r L1, out.width = '60%', fig.cap='L1 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L1.jpg"))
```   


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.0, 0.5, step = 0.005)
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=1.0)
Betas[:5]
```


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE_train, MSE_test) #, ylim=(-10.0,10.0))
```



## Elastic Net Regularization    

Do we always have to choose between the soft constraint of L2 and the hard constraint of L1?   

- L2 regularization works well for **colinear features**    
   - Down-weights colinear features   
   - But soft constraint so poor model selection 

- L1 regularization provides **good model selection** by hard constraint    
   - But poor selection for colinear features     
   
- The **Elastic Net** weights L1 and L2 regularization    
   - Hyperparameter $\lambda$ weights L1 vs. L2 regularization   
   - Hyperparameter $\alpha$ sets strength of regularization  

$$min \Big[ \parallel A \cdot x - b \parallel +\ \lambda\ \alpha \parallel b\parallel^1 +\ (1- \lambda)\ \alpha \parallel b\parallel^2 \Big]$$

## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.0, 0.5, step = 0.005)
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=0.5)
Betas[:5]
```


## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE_train, MSE_test) #, ylim=(-10.0,8.0))
```


## Elastic Net Regularization 

Check the model summary for $\lambda=0.5$, $\alpha=0.06$

```{python}
lm_elastic = smf.ols("socst_zero_mean ~ C(ses)*C(prog)", data=test_scores_train).fit_regularized(alpha=0.06, L1_wt=0.5)
lm_elastic.params
```


```{python, echo=FALSE}
test_scores_train['predicted'] = lm_elastic.predict(test_scores_train)
test_scores_train['resids'] = np.subtract(test_scores_train.predicted, test_scores_train.socst_zero_mean)

def residual_plot(df):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(figsize=(8, 3), ) 
    RMSE = np.std(df.resids)
    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);
    plt.axhline(0.0, color='red', linewidth=1.0);
    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.title('PLot of residuals vs. predicted');
    plt.xlabel('Predicted values');
    plt.ylabel('Residuals');
    plt.show()
    
residual_plot(test_scores_train)  
```



## Summary  

- Working with categorical variables    
   - One-hot encoding creates binary numeric variables 
   - Only one case = 1, or hot, at a time  

- Over-fit models and regularization    
   - Bias variance trade-off between fit to training data and generalization 
   - L2 regularization is a soft constraint      
   - L1 regularization is a hard constraint  
   - Elastic Net trade-off between L1 and L2  

- Models with nonlinear response   
   - The generalized linear model   
   - Link function transforms to linear model  

